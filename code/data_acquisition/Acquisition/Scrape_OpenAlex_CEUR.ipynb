{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8fffb-3ca3-4b92-9a88-10540d20292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyalex import Works, Authors, Sources, Institutions, Concepts, Publishers, Funders\n",
    "from thefuzz import fuzz\n",
    "import json\n",
    "import re\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c0729d-3fcc-4566-a86b-9fea88557e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_ceur = pd.read_parquet(\"../../../data/metadata_CEUR.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443da64f-1573-4be4-a44b-92d20f87edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/SemanticScholar_CEUR.json\", encoding=\"utf-8\") as f:\n",
    "    SemanticScholar_FINAL = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008b4c0-c804-4cc5-b159-84a1846cf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "merge_list = [] # Will contain pairs of Semantic Scholar IDs and corresponding OpenAlex IDs\n",
    "ids_not_scraped = [] # Will contain IDs from SemanticScholar_FINAL that could not be processed\n",
    "\n",
    "# Iterate through each entry in the SemanticScholar_FINAL dictionary\n",
    "for i in SemanticScholar_FINAL:\n",
    "    try:\n",
    "        # Attempt to extract the OpenAlex ID for each Semantic Scholar entry\n",
    "        merge_list.append([i, \"W\" + SemanticScholar_FINAL[i][0][\"externalIds\"][\"MAG\"]]) # Append the MAG ID by Semantic Scholar and OpenAlex ID to merge_list\n",
    "    except:\n",
    "        # If there's a KeyError, it means the expected data is missing\n",
    "        ids_not_scraped.append(i) # Add the problematic ID to ids_not_scraped\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e7526-158a-4919-a08d-6b3754c70f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_has_mag = pd.DataFrame(merge_list, columns=[\"ID\",\"MAG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c09873-379e-4faa-a7d4-09ddda3c605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_has_mag_merged = pd.merge(df_has_mag, df_metadata_ceur, how=\"inner\", left_on=\"ID\", right_on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628f247-bc65-4a38-a19d-fa6bb3e5aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(title):\n",
    "    return re.findall(r'\\b\\d{4}\\b', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633ebeb-bbf7-41fb-ab11-0c13283b0612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionaries and lists to store data\n",
    "\n",
    "OpenAlex_by_MAG = {}      # Will store matched OpenAlex works with Semantic Scholar IDs as keys\n",
    "ids_found = []            # Will keep track of IDs that were successfully matched\n",
    "ids_not_scraped = []      # Will keep track of IDs that could not be processed\n",
    "\n",
    "# Iterate through each row in the DataFrame df_has_mag_merged (Already found documents in Semantic Scholar that provide MAG ID)\n",
    "for i,j in df_has_mag_merged.iterrows():\n",
    "  \n",
    "    try:\n",
    "        # Fetch the OpenAlex work using the MAG ID from the current row\n",
    "        work = Works()[j[\"MAG\"]]\n",
    "\n",
    "        # Extract years from the work title and the DataFrame title\n",
    "        years_semantic = extract_years(work[\"title\"])\n",
    "        years_df = extract_years(j[\"Title\"])\n",
    "\n",
    "        # Check if the titles match closely and if the years are the same\n",
    "        if fuzz.ratio(j[\"Title\"], work[\"title\"]) > 95 and years_df == years_semantic: \n",
    "            OpenAlex_by_MAG[j[\"ID\"]] = work  # Store the matched work in the dictionary with Semantic Scholar ID as key\n",
    "            ids_found.append(j[\"ID\"])        # Add the ID to the list of found IDs\n",
    "    except:\n",
    "        # Handle cases where the OpenAlex work could not be retrieved or other errors occur\n",
    "        ids_not_scraped.append(j[\"ID\"])\n",
    "        print(\"ID not given or found\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d21d42-b206-4cc5-8b42-b9b613723b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to categorize documents based on DOI and CEUR link availability\n",
    "has_MAG_has_DOI_has_no_ceur = [] # List for documents with DOI but without CEUR links\n",
    "has_doi_has_ceur = [] # List for documents with both DOI and CEUR links\n",
    "\n",
    "# Iterate through each document in the OpenAlex_by_MAG dictionary\n",
    "for i in OpenAlex_by_MAG:\n",
    "    # Check if the document has a DOI\n",
    "    if OpenAlex_by_MAG[i][\"doi\"]:\n",
    "        # Retrieve possible open access locations for the document\n",
    "        best_oa_location = OpenAlex_by_MAG[i].get(\"best_oa_location\", {})\n",
    "        oa_access = OpenAlex_by_MAG[i].get(\"open_access\", {})\n",
    "        locations = OpenAlex_by_MAG[i].get(\"locations\", [])\n",
    "\n",
    "        # Collect all unique PDF URLs from the open access locations\n",
    "        collected_urls = []\n",
    "        if isinstance(best_oa_location, dict):\n",
    "            pdf_url = best_oa_location.get(\"pdf_url\")\n",
    "            if pdf_url is not None:\n",
    "                collected_urls.append(pdf_url)\n",
    "        if isinstance(oa_access, dict):\n",
    "            pdf_url = oa_access.get(\"oa_url\")\n",
    "            if pdf_url is not None:\n",
    "                collected_urls.append(pdf_url)\n",
    "        if isinstance(locations, list):\n",
    "            for j in OpenAlex_by_MAG[i][\"locations\"]:\n",
    "                if j[\"is_oa\"] == True:\n",
    "                    if j[\"pdf_url\"] is not None:\n",
    "                        collected_urls.append(j[\"pdf_url\"])\n",
    "\n",
    "        # Remove duplicate URLs\n",
    "        collected_urls = list(set(liste_urls))\n",
    "\n",
    "        # Get the CEUR URL for the current document from the merged DataFrame\n",
    "        ceur_url = df_has_mag_merged.loc[df_has_mag_merged['ID'] == i, 'url'].values[0]\n",
    "\n",
    "        # Check if the CEUR URL is among the collected PDF URLs\n",
    "        found = any(ceur_url in url for url in collected_urls)\n",
    "\n",
    "        if found:\n",
    "            # If the CEUR URL is found among the URLs, categorize the document as having DOI and CEUR\n",
    "            if i not in has_doi_has_ceur:\n",
    "                has_doi_has_ceur.append(i)\n",
    "            continue   \n",
    "        else:\n",
    "            # If the CEUR URL is not found, categorize the document as having DOI but no CEUR\n",
    "            if i not in has_MAG_has_DOI_has_no_ceur:\n",
    "                has_MAG_has_DOI_has_no_ceur.append(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9e635-51fc-4388-af90-8dc1657eeafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter DataFrame to include only rows where the ID is in the list of documents with DOI but no CEUR link\n",
    "df_has_mag_has_doi =  df_has_mag_merged[df_has_mag_merged[\"ID\"].isin(has_MAG_has_DOI_has_no_ceur)]\n",
    "\n",
    "# Initialize a list to store tuples of IDs and corrected OpenAlex IDs\n",
    "mag_and_id = []\n",
    "\n",
    "# Iterate over each row in the filtered DataFrame\n",
    "for i, j in df_has_mag_has_doi.iterrows():\n",
    "    \n",
    "    # Clean up the title by removing commas\n",
    "    title = j[\"Title\"].replace(\",\", \"\")\n",
    "    title_encoded = urllib.parse.quote(title) # URL encode the cleaned title for the search query\n",
    "\n",
    "    # If no results are found, skip to the next entry\n",
    "    work = Works().search_filter(title=title_encoded).get()\n",
    "    if len(work) == 0:\n",
    "        print(\"Skip - no results found\")\n",
    "        continue\n",
    "    \n",
    "    # If exactly one result is found and the MAG ID matches, skip the entry    \n",
    "    if len(work) == 1 and j[\"MAG\"][1:] == work[0][\"ids\"][\"mag\"]:\n",
    "        print(\"Skip - result matches existing MAG ID\")\n",
    "        continue\n",
    "    \n",
    "    # If exactly one result is found but the MAG ID does not match, prompt user for correction\n",
    "    if len(work) == 1 and j[\"MAG\"][1:] != work[0][\"ids\"][\"mag\"]:\n",
    "        print(\"Results differ\")\n",
    "        print(\"Current ID:\", \"https://openalex.org/\" + j[\"MAG\"])\n",
    "        print(\"New ID:\", work[0][\"id\"])\n",
    "        user_input = input(\"Enter the correct OpenAlex ID: \")\n",
    "        user_input = user_input.replace(\"https://openalex.org/works/\", \"\")\n",
    "        mag_and_id.append([j[\"ID\"], user_input])\n",
    "\n",
    "    # If multiple results are found, prompt user to choose the correct one\n",
    "    if len(work) > 1:\n",
    "        print(\"Multiple results found:\")\n",
    "        for k in work:\n",
    "            print(k[\"id\"])\n",
    "        print(j[\"Authors\"])\n",
    "        print(j[\"Title\"])\n",
    "        \n",
    "        print(\"Current ID:\", \"https://openalex.org/\" + j[\"MAG\"])\n",
    "        user_input = input(\"Enter the correct OpenAlex ID: \")\n",
    "        user_input = user_input.replace(\"https://openalex.org/works/\", \"\")\n",
    "        # Add the chosen ID to the list if it is different from the current MAG ID\n",
    "        if user_input  != j[\"MAG\"]:\n",
    "            mag_and_id.append([j[\"ID\"], user_input])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e47e43-e49b-4c74-b8c5-6ab5cd249965",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_and_id = [['ceur_7', 'W3097562850'],\n",
    " ['ceur_14', 'W3095609828'],\n",
    " ['ceur_41', '-'],\n",
    " ['ceur_153', '-'],\n",
    " ['ceur_2923', 'W2292665776'],\n",
    " ['ceur_1681', 'W216921825'],\n",
    " ['ceur_2570', 'W2186486156'],\n",
    " ['ceur_743', '-'],\n",
    " ['ceur_1524', '-'],\n",
    " ['ceur_899', 'W2734855474'],\n",
    " ['ceur_2686', 'W2964258545'],\n",
    " ['ceur_864', 'W2951384152']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de8b72-d25c-4a11-add6-ea894103835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of MAG and CEUR ID pairs\n",
    "for i in mag_and_id:\n",
    "    # If the second item (OpenAlex ID) is \"-\", remove the corresponding entry from OpenAlex_by_MAG\n",
    "    if i[1] == \"-\":\n",
    "        del OpenAlex_by_MAG[i[0]]\n",
    "    else:\n",
    "        # If the CEUR ID is already in OpenAlex_by_MAG, remove the existing entry\n",
    "        if i[0] in OpenAlex_by_MAG:\n",
    "            del OpenAlex_by_MAG[i[0]]\n",
    "        # Add or update the OpenAlex entry for the given CEUR ID\n",
    "        OpenAlex_by_MAG[i[0]] = Works()[i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a770fc5-d2b2-42e3-86b1-2fc86e701bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter OpenAlex_by_MAG to keep only those entries not in has_MAG_has_DOI_has_no_ceur\n",
    "filtered_dict = {key: OpenAlex_by_MAG[key] for key in OpenAlex_by_MAG if key not in has_MAG_has_DOI_has_no_ceur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcaa8c-202f-46ae-b89e-98bde255e1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lists to store IDs of documents with DOI and CEUR URLs, and those with DOI but without CEUR URLs\n",
    "\n",
    "documents_with_doi_no_ceur = []\n",
    "doi_without_ceur_documents = []\n",
    "\n",
    "# Iterate over the filtered OpenAlex entries\n",
    "for i in OpenAlex_by_MAG:\n",
    "    # Extract open access URLs from various sources in the OpenAlex entry\n",
    "    best_oa_location = OpenAlex_by_MAG[i].get(\"best_oa_location\", {})\n",
    "    oa_access = OpenAlex_by_MAG[i].get(\"open_access\", {})\n",
    "    locations = OpenAlex_by_MAG[i].get(\"locations\", [])\n",
    "    list_urls = []\n",
    "    if isinstance(best_oa_location, dict):\n",
    "        pdf_url = best_oa_location.get(\"pdf_url\")\n",
    "        if pdf_url is not None:\n",
    "            list_urls.append(pdf_url)\n",
    "    if isinstance(oa_access, dict):\n",
    "        pdf_url = oa_access.get(\"oa_url\")\n",
    "        if pdf_url is not None:\n",
    "            list_urls.append(pdf_url)\n",
    "    if isinstance(locations, list):\n",
    "        for j in OpenAlex_by_MAG[i][\"locations\"]:\n",
    "            if j[\"is_oa\"] == True:\n",
    "                if j[\"pdf_url\"] is not None:\n",
    "                    list_urls.append(j[\"pdf_url\"])\n",
    "                    \n",
    "    # Remove duplicate URLs\n",
    "    list_urls = list(set(list_urls))\n",
    "    \n",
    "    # Get the URL from the DataFrame for the current CEUR ID\n",
    "    url_value  = df_has_mag_merged.loc[df_has_mag_merged['ID'] == i, 'url'].values[0]\n",
    "    url_value  = url_wert.replace(\"https\", \"http\") # Standardize the URL scheme\n",
    "\n",
    "    # Print the collected URLs and the corresponding CEUR URL for debugging\n",
    "    if len(list_urls) > 0:\n",
    "        print(list_urls)\n",
    "        print(url_value )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Check if any of the collected URLs match the CEUR URL\n",
    "    found = any(url_value  in url for url in list_urls)\n",
    "\n",
    "    if found:\n",
    "        if i not in has_doi_has_ceur:\n",
    "            documents_with_doi_no_ceur.append(i) # Document has DOI and CEUR URL\n",
    "        continue   \n",
    "    else:\n",
    "        if i not in has_MAG_has_DOI_has_no_ceur:\n",
    "            doi_without_ceur_documents.append(i) # Document has DOI and CEUR URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617b53b-ed1f-4461-a00c-90e6382bbc65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IDs from OpenAlex that we have already found and processed by MAG ID\n",
    "ids_found_with_MAG = list(OpenAlex_by_MAG.keys())\n",
    "\n",
    "# Filter out documents from the main DataFrame that have already been processed (i.e., have MAG IDs)\n",
    "df_no_mag = df_metadata_ceur[~df_metadata_ceur[\"ID\"].isin(ids_found_with_MAG)]\n",
    "\n",
    "# Dictionary to store documents that need to be matched by title\n",
    "documents_without_MAG  = {}\n",
    "\n",
    "# Iterate through the filtered DataFrame\n",
    "for i, j in df_no_mag.iterrows():\n",
    "    \n",
    "    # Clean and encode the title for the search query\n",
    "    title = j[\"Title\"].replace(\",\", \"\")\n",
    "    title_encoded = urllib.parse.quote(title)\n",
    "    try:\n",
    "        # Search for works in OpenAlex by the cleaned and encoded title\n",
    "        work = Works().search_filter(title=title_encoded).get()    \n",
    "        \n",
    "        if len(work) == 0:\n",
    "            # If no results are found, print a message and continue to the next document\n",
    "            print(\"no matches\")\n",
    "            continue\n",
    "            \n",
    "        elif len(work) == 1:\n",
    "            # If exactly one result is found, check if it matches well with the title and year\n",
    "            print(\"One option\")\n",
    "            years_semantic = extract_years(work[0][\"title\"])\n",
    "            years_df = extract_years(j[\"Title\"])\n",
    "            if fuzz.ratio(j[\"Title\"], work[0][\"title\"]) > 95 and years_df == years_semantic:\n",
    "                # Add the result to the dictionary if the title and year match\n",
    "                documents_without_MAG [j[\"ID\"]] = work\n",
    "                \n",
    "        elif len(work) > 1:\n",
    "            # If multiple results are found, display them for user selection\n",
    "            for k in work:\n",
    "                print(k[\"id\"])\n",
    "            print(j[\"Title\"])\n",
    "            print(j[\"Authors\"])\n",
    "            \n",
    "            # Prompt the user to select the correct result\n",
    "            mag_id_to_take = int(input())\n",
    "            if mag_id_to_take != \"-\":\n",
    "                documents_without_MAG [j[\"ID\"]] = work[mag_id_to_take]\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1d020-cc3c-47c5-b80d-225ea14ed404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store documents without MAG ID, converted for uniformity\n",
    "uniform_documents_without_MAG = {}\n",
    "\n",
    "# Iterate through each item in the original dictionary\n",
    "for i in documents_without_MAG :\n",
    "    # Check if the item is a list\n",
    "    if type(documents_without_MAG [i]) == list:\n",
    "        uniform_documents_without_MAG[i] = documents_without_MAG [i][0]\n",
    "    \n",
    "    # Check if the item is a dictionary\n",
    "    elif isinstance(documents_without_MAG [i] , dict):\n",
    "        uniform_documents_without_MAG[i] = documents_without_MAG [i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96daa05e-48b6-4d13-90a4-9ebca8adcc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for the types of items in the uniform dictionary\n",
    "count_lists = 0\n",
    "count_dicts = 0\n",
    "\n",
    "for i in uniform_documents_without_MAG:\n",
    "    if type(uniform_documents_without_MAG[i]) == list:\n",
    "        print(i)\n",
    "        count_lists +=1\n",
    "    elif isinstance(uniform_documents_without_MAG[i] , dict):\n",
    "        #print(documents_without_MAG [i])\n",
    "        count_dicts += 1\n",
    "    else:\n",
    "        print(type(uniform_documents_without_MAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9559f14-3653-4e6f-a84a-6662fbf2978d",
   "metadata": {},
   "source": [
    "## Manually add still missing OpenAlex documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc25e4a-ad06-43e8-9f7d-0228474716cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine IDs from OpenAlex and documents without MAG IDs\n",
    "all_found_ids  = list(OpenAlex_by_MAG.keys()) + list(uniform_documents_without_MAG.keys()) \n",
    "\n",
    "# Filter the original dataframe to find records that are missing from the combined list\n",
    "df_missing_records  = df_metadata_ceur[~df_metadata_ceur[\"ID\"].isin(all_found_ids)]\n",
    "\n",
    "# Dictionary to store records that are still missing after initial searches\n",
    "still_missing_records = {}\n",
    "\n",
    "for i, j in df_missing_records.iterrows():\n",
    "    # Print the title and authors of the missing record\n",
    "    print(j[\"Title\"])\n",
    "    print(j[\"Authors\"])\n",
    "    \n",
    "    # Prompt for a potential OpenAlex ID\n",
    "    potential_id = input(\"Enter the OpenAlex ID (or '-' to skip): \")\n",
    "    \n",
    "    if potential_id == \"-\":\n",
    "        # Skip to the next record if input is \"-\"\n",
    "        continue\n",
    "    else:\n",
    "        # Add the record to still_missing_records using the provided OpenAlex ID\n",
    "        still_missing_records[j[\"ID\"]] = Works()[\"W\" + potential_id]\n",
    "\n",
    "# Get the list of IDs found in the last step\n",
    "last_found_ids  = list(still_missing_records.keys())\n",
    "\n",
    "# Filter the dataframe again to find records that are still missing\n",
    "final_openalex_ceur = df_missing_records[~df_missing_records[\"ID\"].isin(last_found_ids)]\n",
    "\n",
    "# Merge the dictionaries to create a final JSON output\n",
    "final_json_openalex_ceur = still_missing_records  | OpenAlex_by_MAG | uniform_documents_without_MAG\n",
    "\n",
    "# Save the final combined dictionary to a JSON file\n",
    "with open('../../../data/OpenAlex_CEUR.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(final_json_openalex_ceur, f , ensure_ascii = False, indent=4)\n",
    "\n",
    "df_finally_missing.to_parquet(\"../../../data/OpenAlex_CEUR_not_found.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MasterThesisEnv)",
   "language": "python",
   "name": "masterthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
