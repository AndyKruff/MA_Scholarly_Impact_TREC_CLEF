{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b501ba8-8ff4-4b0e-bb24-e9c229ed88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import urllib.parse\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329399cf-1d56-4fb9-8f74-cc86378be73a",
   "metadata": {},
   "source": [
    "# Extract CEUR metadata from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfc4184-bca7-4324-b8b0-f3df130c27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceur = pd.read_parquet(\"../../../data/metadata_CEUR.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a7b16-ba9a-4fd7-9ee3-2fb126dc5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tresholds and variables for storage\n",
    "\n",
    "data_CEUR = {}\n",
    "non_matching_rows = []\n",
    "threshhold = 75\n",
    "threshold_avg = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f8f28-ff9c-4c12-a03d-8d7cd9933a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract years or 4-digit numbers from titles\n",
    "\n",
    "def extract_years(title):\n",
    "    return re.findall(r'\\b\\d{4}\\b', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cb6ef-51b8-480a-839c-c53dbfec035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fuzzy match the titles scraped from CEUR website and the once from Semantic Scholar\n",
    "\n",
    "def fuzzy_match_lists(list1, list2, threshold_single, threshhold_avg):\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "        \n",
    "    total_similarity = 0\n",
    "    for item1, item2 in zip(list1, list2):\n",
    "        similarity_score = fuzz.ratio(item1, item2)\n",
    "        total_similarity += similarity_score\n",
    "        if fuzz.ratio(item1, item2) < threshold_single:\n",
    "            return False\n",
    "    average_similarity = total_similarity / len(list1)\n",
    "\n",
    "    if average_similarity > threshhold_avg:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959fcf0-872f-489c-b0c8-de10ebe8f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row in the merged DataFrame\n",
    "\n",
    "for i, j in df_ceur.iterrows():\n",
    "    # Define parameters for the API request\n",
    "    params = {\n",
    "        \"query\": f'title:(\"{j[\"Title\"]}\")', # Search query using the title from the DataFrame\n",
    "        \"limit\": 1,                         # Limit the results to 1\n",
    "        \"fields\": \"corpusId,externalIds,citationCount,influentialCitationCount,authors,journal,title\"   # Fields to retrieve\n",
    "    }\n",
    "    \n",
    "    # Make the GET request to the API and parse the JSON response\n",
    "    resp = requests.get(url, params=params, headers=headers).json()\n",
    "    try:\n",
    "        # Extract the title from the API response\n",
    "        title_semantic = resp[\"data\"][0][\"title\"]\n",
    "        title_df = j[\"Title\"]\n",
    "\n",
    "        # Extract years from both the semantic title and the title from the DataFrame\n",
    "        years_semantic = extract_years(title_semantic)\n",
    "        years_df = extract_years(title_df)\n",
    "\n",
    "        # Create sorted lists of author names from both the API response and the DataFrame\n",
    "        sorted_author_list1 = sorted([i[\"name\"].split()[-1] for i in resp[\"data\"][0][\"authors\"]])\n",
    "        sorted_author_list2 = sorted([i.split()[-1] for i in j[\"Authors\"]])\n",
    "\n",
    "        # Print the sorted author lists for debugging\n",
    "        print(sorted_author_list1)\n",
    "        print(sorted_author_list2)\n",
    "        \n",
    "        # Check if the title matches, years are the same, and authors match\n",
    "        if fuzz.ratio(title_df, title_semantic) > 90 and years_df == years_semantic and fuzzy_match_lists(sorted_author_list1, sorted_author_list2, threshhold, threshold_avg) == True:\n",
    "            \n",
    "            # If all conditions are met, add the data to the dictionary\n",
    "            data_CEUR[j[\"ID\"]] = resp[\"data\"]\n",
    "        else:\n",
    "            # If any condition is not met, add the row to the non-matching list and print a failure message\n",
    "            non_matching_rows.append(j)\n",
    "    except:\n",
    "        # If an exception occurs, add the row to the non-matching list\n",
    "        non_matching_rows.append(j)\n",
    "\n",
    "    # Pause for 3.5 seconds between requests to avoid overwhelming the API\n",
    "    time.sleep(3.5)\n",
    "\n",
    "# Create a DataFrame from the non-matching rows and save it as a parquet file\n",
    "non_matching_df = pd.DataFrame(non_matching_rows, columns=merged_df.columns)\n",
    "non_matching_df.to_parquet(\"../../../data/metadata_CEUR_not_found.parquet\")\n",
    "\n",
    "# Save the successfully matched data to a JSON file\n",
    "with open('../../../data/SemanticScholar_CEUR_found.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_CEUR, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f9b8b-678e-4e26-b435-56a8de0941de",
   "metadata": {},
   "source": [
    "## Retrieve still missing CEUR data from Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f359940-2967-483e-9fad-fbea9ea3c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CEUR_missing = pd.read_parquet(\"../../../data/metadata_CEUR_not_found.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f94699-9aa8-4d85-a9a1-208388f4f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceur_proceedings_df_with_ID_missing = pd.merge(df_CEUR_missing, df_ceur, on=['PubYear', 'Title', 'CEUR Title', 'Volume', 'filename', 'Section'], how='left', suffixes=('', '_df2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef336ba-398c-4efc-a51e-cb24d60c68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceur_proceedings_df_with_ID_missing.drop(columns=['Authors_df2', \"url_df2\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6dbba-8ff6-4e35-a1a4-c3631bd1e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/SemanticScholar_CEUR_found.json\", encoding=\"utf-8\") as f:\n",
    "    Semantic_Scholar_CEUR_found = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59d402-0216-4b19-9b4a-5948834e79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_ids = list(Semantic_Scholar_CEUR_found.keys())\n",
    "not_found_ids = ceur_proceedings_df_with_ID_missing[\"ID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22fe54a-95ad-42d5-a483-898dc85c40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CEUR = {}\n",
    "non_matching_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a433fa-edb8-4787-abac-bdfd0bbc0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API parameters for Semantic Scholar\n",
    "\n",
    "url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "headers = {\n",
    "    \"x-api-key\": api_key  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4f8a3-166d-4104-8e54-122bd3c8bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in ceur_proceedings_df_with_ID_missing.iterrows():\n",
    "\n",
    "    # Define parameters for the API request\n",
    "    params = {\n",
    "        \"query\": f'title:(\"{j[\"Title\"]}\")', # Search query using the title from the DataFrame\n",
    "        \"limit\": 1,# Limit the results to 1\n",
    "        \"fields\": \"corpusId,externalIds,citationCount,influentialCitationCount,authors,journal,title\" # Fields to retrieve\n",
    "    }\n",
    "    \n",
    "    # Make the GET request to the API and parse the JSON response\n",
    "    resp = requests.get(url, params=params, headers=headers).json()\n",
    "    try:\n",
    "        # Extract the title and other details from the API response\n",
    "        title_semantic = resp[\"data\"][0][\"title\"]\n",
    "        title_df = j[\"Title\"]\n",
    "        years_semantic = extract_years(title_semantic)\n",
    "        years_df = extract_years(title_df)\n",
    "        sorted_author_list1 = sorted([i[\"name\"].split()[-1] for i in resp[\"data\"][0][\"authors\"]])\n",
    "        sorted_author_list2 = sorted([i.split()[-1] for i in j[\"Authors\"]])\n",
    "\n",
    "        # Print the extracted information manual control\n",
    "        print(\"Title SemanticScholar:\", title_semantic, \"\\t\", \"Years:\" , years_semantic)\n",
    "        print(sorted_author_list1)\n",
    "        print(\"Title Given:          \", title_df , \"\\t\", \"Years:\" , years_df)\n",
    "        print(sorted_author_list2)\n",
    "        print(\"Fuzz Ratio:\", fuzz.ratio(title_df, title_semantic))\n",
    "        print(resp[\"data\"][0][\"paperId\"])\n",
    "        \n",
    "        # Prompt the user for input to decide whether to include the data or not\n",
    "        input_frame = input()\n",
    "        if input_frame == \"+\":\n",
    "            # If user inputs \"+\", add the paperId to the data_CEUR dictionary\n",
    "            data_CEUR[j[\"ID\"]] = resp[\"data\"][0][\"paperId\"]\n",
    "        elif input_frame == \"-\":\n",
    "            # If user inputs \"-\", skip to the next record\n",
    "            continue\n",
    "        else:\n",
    "            # If user inputs anything else, add the custom input to the data_CEUR dictionary\n",
    "            data_CEUR[j[\"ID\"]] = input_frame\n",
    "            \n",
    "    except:\n",
    "        # Handle exceptions and prompt the user for input\n",
    "        print(\"Except:\")\n",
    "        print(j[\"Title\"])\n",
    "        print(j[\"PubYear\"])\n",
    "        print(j[\"Authors\"])\n",
    "        input_frame_2 = input()\n",
    "        if input_frame_2 == \"-\":\n",
    "            # If user inputs \"-\", skip to the next record\n",
    "            continue\n",
    "        else:\n",
    "            # If user inputs anything else, add the custom input to the data_CEUR dictionary\n",
    "            data_CEUR[j[\"ID\"]] = input_frame_2\n",
    "    # Pause for 2.5 seconds between requests to avoid overwhelming the API\n",
    "    time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2925b5c-f654-4bf4-8977-d9a86a2e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_CEUR['ceur_3368']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158a6e5-4cf6-440a-a613-d6df8d889cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"limit\": 1,\n",
    "    \"fields\": \"corpusId,externalIds,citationCount,influentialCitationCount,authors,journal,title\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631fd99-e8bc-4d0d-a748-f1bc188c4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ceur_Semantic = {}\n",
    "for key, value in data_CEUR.items():\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{value}\"\n",
    "    resp = requests.get(url, params=params, headers=headers).json()\n",
    "\n",
    "    data_ceur_Semantic[key] = [resp]\n",
    "    time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2357962-02e4-4730-8def-333ce52ff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/SemanticScholar_CEUR_found.json\", encoding=\"utf-8\") as f:\n",
    "    SemanticScholar_found_by_title = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b1f63-7b63-4ebd-931f-adce9cfa3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SemanticScholar_found_by_title['ceur_1652']\n",
    "del SemanticScholar_found_by_title['ceur_2258']\n",
    "del SemanticScholar_found_by_title['ceur_2324']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3d199-7c03-4a2a-ae6f-a1966708b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemanticScholar_FINAL_json = SemanticScholar_found_by_title | data_ceur_Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934e920-bd08-4cc1-9f38-b4da08eb94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refers to QALD-4 instead of QALD-5\n",
    "SemanticScholar_FINAL_json.pop(\"ceur_2974\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754988e-3e03-4ac4-a918-32b0d532ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data/SemanticScholar_CEUR.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(SemanticScholar_FINAL_json, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MasterThesisEnv)",
   "language": "python",
   "name": "masterthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
