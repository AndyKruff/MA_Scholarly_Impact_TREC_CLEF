{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19288ae5-044b-4ec5-9709-b88613b78f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf30cd2-e253-4bc3-a2f4-a52986297a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea27a37-8a28-402e-a864-5c14f2c9f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_text_from_abstract(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'ns0': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Find the <abstract> element\n",
    "    abstract = root.find(\".//ns0:abstract\", ns)\n",
    "    \n",
    "    if abstract is not None:\n",
    "        # Extract all <p> tags within <div> inside <abstract>\n",
    "        paragraphs = abstract.findall(\".//ns0:div//ns0:p\", ns)\n",
    "\n",
    "        # Extract and concatenate text content from all <p> tags\n",
    "        extracted_text = \"\\n\".join([p.text.strip() for p in paragraphs if p.text])\n",
    "\n",
    "        return extracted_text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b15ad6-635a-452b-bba8-f936b2cfb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for file in glob.glob(\"../../resources/XML_CEUR/*/*.tei.xml\"):\n",
    "    files.append(file)\n",
    "for file in glob.glob(\"../../resources/XML_CEUR/*/*/*.tei.xml\"):\n",
    "    files.append(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2446aff6-2650-495f-a593-b1a537cb918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728ac5d0-f2f1-4bd2-a540-f2392c8a65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in files:\n",
    "    string = extract_text_from_abstract(i)\n",
    "    corpus_text.append(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13cbd079-457a-428a-ba90-337799787200",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71473de4-13db-4986-8ef4-5fa02c1ea9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    stopwords_labs_tracks = [ \"Adhoc\", \"AToMiC\", \"Blog\", \"CENTRE\",\"Chemical\", \"Chinese\",\n",
    "                            \"Clinical Decision Support\", \"Clinical Trials\", \"Common Core\", \"Complex Answer Retrieval\", \"Contextual Suggestion\", \"Conversational Assistance\",\n",
    "                            \"CrisisFACTs\", \"Cross-Language\", \"Crowdsourcing\", \"Decision\", \"Deep Learning\", \"Dynamic Domain\", \"Enterprise\", \"Fair Ranking\", \"Entity Track\",\n",
    "                            \"Federated Web Search\",\"Filtering\", \"Genomics\", \"HARD\", \"Health Misinformation\", \"Incident Streams\", \"Interactive\", \"Knowledge Base Acceleration\",\n",
    "                            \"Legal\", \"LiveQA\", \"Medical\", \"Microblog\", \"Million Query\", \"NeuCLIR\", \"News\", \"Novelty\", \"OpenSearch\", \"Podcast\", \"Precision Medicine\",\n",
    "                            \"Query Track\", \"Question Answering\", \"Real-time Summarization\", \"Relevance Feedback\", \"Robust\", \"Session\", \"Spam\", \"Spoken Document Retrieval\",\n",
    "                            \"Tasks Track\", \"Temporal Summarization\", \"Terabyte\", \"Total Recall\", \"Video Track\", \"Web Track\",\n",
    "                            \"German Indexing and Retrieval Testdatabase\", \"GIRT\", \"Cross-Language Text Retrieval\", \"CLTR\", \"Cross-Language Information Retrieval\", \"CLIR\",\n",
    "                            \"Multilingual Text Retrieval\", \"Ad-hoc\", \"Domain Specific Cross-Language IR\", \"Interactive Cross-Language IR\", \"iCLEF\", \"Spoken Document/Speech Retrieval\",\n",
    "                            \"CLEF SR\", \"Question Answering\", \"QA@CLEF\", \"Multimedia Retrieval\", \"ImageCLEF\", \"Multilingual Web Search\", \"WebCLEF\", \"Geographical Retrieval\", \n",
    "                            \"GeoCLEF\", \"CLEF@MorphoChallenge\", \"MorphoChallenge\", \"Morpho Challenge\", \"CLEF@SemEval\", \"SemEval\", \"Cross-Language Video Retrieval\", \"VideoCLEF\",\n",
    "                            \"Multilingual Information Filtering\", \"INFILE\", \"Log File Analysis\", \"LogCLEF\", \"Intellectual Property in the Patent Domain\", \"CLEF-IP\",\n",
    "                            \"Component-based Evaluation\", \"Grid@CLEF\", \"Web People Search\", \"WEPS\", \"Cross-lingual Expert Search\", \"CriES\", \"Digital Text Forensics and Stylometry\",\n",
    "                            \"PAN\", \"Music Information Retrieval\", \"MusiCLEF\", \"Cultural Heritage in CLEF\", \"CHiC\", \"Retrieval on Structured Datasets\", \"INEX\", \"Online Reputation Management\",\n",
    "                            \"RepLab\", \"eHealth\", \"CLEF-ER\", \"Biomedical Semantic Indexing and Question Answering\", \"BioASQ\", \"LifeCLEF\", \"Biodiversity Identification and Prediction\", \n",
    "                            \"News Recommendation Evaluation\", \"NewsREEL\", \"Living Labs\", \"LL4IR\", \"Social Book Search\", \"SBS\", \"Microblog Cultural Contextualization\", \"MC2\",\n",
    "                            \"Dynamic Search for Complex Tasks\", \"DynSE\", \"Multimodal Spatial Role Labeling\", \"MSRL\", \"Early Risk Prediction on the Internet\", \"eRisk\", \"Personalised Information Retrieval\",\n",
    "                            \"PIR-CLEF\", \"Reproducibility\", \"CENTRE@CLEF\", \"Identification and Verification of Political Claims\", \"CheckThat!\", \"Extracting Protests from News\",\n",
    "                            \"ProtestNews\", \"Answer Retrieval for Questions on Math\", \"ARQMath\", \"ChEMU\", \"Cheminformatics Elsevier Melbourne University lab\", \"Identifying Historical People, Places and other Entities\",\n",
    "                            \"HIPE\", \"Living Labs for Academic Search\", \"LiLAS\", \"Argument Retrieval\", \"Touch√©\", \"Automatic Simplification of Scientific Texts\",\n",
    "                            \"SimpleText\", \"Intelligent Disease Progression Prediction\", \"iDPP@CLEF\", \"Automatic Workplay and Humour Translation\", \"JokeR\", \"Learning to Quantify\",\n",
    "                            \"LeQua\"]\n",
    "                            \n",
    "    additional_stopwords = [\"TREC\", \"Track\", \"CLEF\", \"Working Notes\", \"Workshop\", \"Participation\", \"Lab\"]                       \n",
    "    def remove_stopwords(text, stopword_list):\n",
    "    # Create a regular expression pattern to remove all stopwords\n",
    "        pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(word) for word in stopword_list) + r')\\b', re.IGNORECASE)\n",
    "        cleaned_text = pattern.sub('', text)  # Ersetze Stopwords durch leeren String\n",
    "        return cleaned_text\n",
    "\n",
    "    text = remove_stopwords(text, stopwords_labs_tracks)\n",
    "    text = remove_stopwords(text, additional_stopwords)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Stem words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    stopwords = stopwords.words('english')\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in stemmed_words if word not in stopwords]\n",
    "    \n",
    "    # Create bi-grams\n",
    "    bi_grams = [' '.join(bi) for bi in zip(filtered_words, filtered_words[1:])]\n",
    "    tri_grams = [' '.join(bi) for bi in zip(filtered_words, filtered_words[1:], filtered_words[2:])]\n",
    "\n",
    "    return filtered_words + bi_grams + tri_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa279d6e-17f2-4adb-99ed-fd00dc98a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb44ffe-49c9-4f66-9922-dc6e4a142ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(corpus_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23dd4f09-3927-42a4-aa2d-5d0a1f6b2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "document_idx = 1000\n",
    "tfidf_values = tfidf_matrix[document_idx].toarray()[0]\n",
    "\n",
    "# Indizes der TF-IDF-Werte in absteigender Reihenfolge sortieren\n",
    "sorted_indices = np.argsort(tfidf_values)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24d43fb5-eae6-4ec3-b8b0-dfaaabfe21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a65aab9e-dbc6-4a5a-bdd1-fc6a516e47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wort: expert, TF-IDF-Wert: 0.19245729944626527\n",
      "Wort: use challeng, TF-IDF-Wert: 0.13720030051956036\n",
      "Wort: crosslingu expert, TF-IDF-Wert: 0.13062548620234943\n",
      "Wort: crosslingu expert search, TF-IDF-Wert: 0.13062548620234943\n",
      "Wort: expert search, TF-IDF-Wert: 0.12596058120574907\n",
      "Wort: crosslingu, TF-IDF-Wert: 0.11932289869199968\n",
      "Wort: yahoo, TF-IDF-Wert: 0.11688613849519762\n",
      "Wort: challeng, TF-IDF-Wert: 0.08396209798404346\n",
      "Wort: topic, TF-IDF-Wert: 0.07591588010006188\n",
      "Wort: submit run, TF-IDF-Wert: 0.07297378481078887\n"
     ]
    }
   ],
   "source": [
    "top_n = 10\n",
    "for i in range(top_n):\n",
    "    idx = sorted_indices[i]\n",
    "    word = feature_names[idx]\n",
    "    tfidf_value = tfidf_values[idx]\n",
    "    print(f\"Wort: {word}, TF-IDF-Wert: {tfidf_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd999af1-7333-4f63-a040-f0dd14b4e75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speed and scale at which information can be created and propagated has increased markedly over the last few decades and has far exceeded the scope that human fact-checkers can handle, enabling the rise in harmful and compelling disinformation campaigns. As a result, increasing attention is focusing on the \"Claim Matching\" problem, where a portion of the fact checking process is automated via AI, by matching content with a human verified claims database. In this report, we discuss a novel neural pipeline for claim matching. Specifically, we demonstrate the efficacy of generative re-rankers to aid the claim matching process, and introduce a new training objective that targets maximizing mutual information. In the CLEF CheckThat! 2022 Competition sub-task 2a, our claim matching approach placed first, beating the second place team by over 3.4 percentage points (evaluated on MAP@5).'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_text[3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d71af-12cf-4d47-8b87-bd689be58d91",
   "metadata": {},
   "source": [
    "## Try extraction with Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b018b3a-8d72-4147-83d4-fa1a49ffef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_text_from_abstract(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'ns0': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Find the <abstract> element\n",
    "    abstract = root.find(\".//ns0:abstract\", ns)\n",
    "    \n",
    "    if abstract is not None:\n",
    "        # Extract all <p> tags within <div> inside <abstract>\n",
    "        paragraphs = abstract.findall(\".//ns0:div//ns0:p\", ns)\n",
    "\n",
    "        # Extract and concatenate text content from all <p> tags\n",
    "        extracted_text = \"\\n\".join([p.text.strip() for p in paragraphs if p.text])\n",
    "\n",
    "        return [path, extracted_text]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7f309d-1d92-4ed7-8d04-dc0cc09d16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for file in glob.glob(\"../../resources/XML_CEUR/*/*.tei.xml\"):\n",
    "    files.append(file)\n",
    "for file in glob.glob(\"../../resources/XML_CEUR/*/*/*.tei.xml\"):\n",
    "    files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56657bc-8520-400a-a26b-9a51ed29591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = []\n",
    "\n",
    "for i in files:\n",
    "    string = extract_text_from_abstract(i)\n",
    "    corpus_text.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe9f459-fc04-4e71-93c6-e9ef0b12a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cac1a12-2420-4b02-9930-0095fc9db8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\" # Current Version 0.3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "473cbd41-1c32-4da7-9f30-bbf843477a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_terms = []\n",
    "\n",
    "# Classify the sentence with Llama3 and count how many of the citations are referring to the usage of the underlying dataset \n",
    "\n",
    "for i in corpus_text[10:20]:\n",
    "    # Define the prompt for the classification with Llama3\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Can you identify the underlying technical methods used by the authors in the provided abstract? Focus specifically on techniques such as machine learning models (e.g., Deep Learning, Large Language Models, Support Vector Machines), data processing methods, or other technical approaches. If such methods are mentioned, list them separated by commas. If no relevant technical methods are mentioned, respond with \"None\". Provide only the methods or \"None\" without any additional commentary or explanation.\n",
    "    \"\"\"\n",
    "    prompt = prompt + \"\\nAbstract:\\n\" + i[1] \n",
    "    data = {\"model\":\"llama3\", \"prompt\" : f\"{prompt}\"}\n",
    "    response = httpx.post(url, data=json.dumps(data), headers={\"Content-Type\": \"application/json\"}, timeout=15)\n",
    "    response_lines = [line for line in response.text.strip().split(\"\\n\") if line]\n",
    "    response_dicts = [json.loads(line) for line in response_lines]\n",
    "    answer = \"\".join(response_dict.get(\"response\", \"\") for response_dict in response_dicts)\n",
    "    if answer == \"None\":\n",
    "        terms = []\n",
    "    else:\n",
    "        if \",\" in answer:\n",
    "            terms = [method.strip() for method in answer.split(\",\")] #  Add multiple found methods to list\n",
    "        else:\n",
    "            terms = [answer.strip()]  # Add single method to list\n",
    "    relevant_terms.append([i[0], i[1], terms])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c87675a-495f-44eb-a252-46866a6ccf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We present an approach to multilingual information retrieval that does not depend on the existence of specific linguistic resources such as stemmers or thesaurii. Using the HAIRCUT system we participated in the monolingual, bilingual, and multilingual tasks of the CLEF-2000 evaluation. Our method, based on combining the benefits of words and character n-grams, was effective for both language-independent monolingual retrieval as well as for cross-language retrieval with translated queries. After describing our monolingual retrieval approach we compare a translation method using aligned parallel corpora to commercial machine translation software.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_terms[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4245678-212e-4819-a5ed-3282111fbd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine Learning models (None)',\n",
       " 'Deep Learning (None)',\n",
       " 'Large Language Models (None)',\n",
       " 'Support Vector Machines (None)',\n",
       " 'data processing methods: words',\n",
       " 'character n-grams.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_terms[5][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b087c92-ed99-40d1-b8cd-6a43922958e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MasterThesisEnv)",
   "language": "python",
   "name": "masterthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
