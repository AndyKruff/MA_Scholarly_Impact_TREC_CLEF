{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead2fb8d-f52d-425c-b839-9b17fb88a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from lxml import etree\n",
    "import re\n",
    "from thefuzz import fuzz\n",
    "from pathlib import Path\n",
    "import random\n",
    "import httpx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7941858-3da4-4a3e-b076-e84a1d8c6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clef_overview = pd.read_parquet(\"../../../data/CLEF_overview_paper_with_ids_of_citing_papers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978c9459-6a42-4450-9ddd-ad87bff9c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to CLEF XML-tranformed documents\n",
    "\n",
    "directory = '../../../data/citing_overview_paper_paper_pdfs/XML_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6033356f-69b4-4227-b650-a1147d4090fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all paths to the corresponding documents\n",
    "files = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fd43f23-ab15-475c-9d4d-c306edbb6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def list_all_paths(directory):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files:\n",
    "            paths.append(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            paths.append(os.path.join(root, name))\n",
    "    return paths\n",
    "\n",
    "directory = '../../../data/citing_overview_paper_paper_pdfs/XML_files_new'\n",
    "\n",
    "all_paths = list_all_paths(directory)\n",
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43541d33-16af-4f70-a9cd-86ae085a567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_closest_head_tag(element):\n",
    "    \"\"\"\n",
    "    Finds the closest <head> tag before the given element and extracts its text content.\n",
    "\n",
    "    Args:\n",
    "        element (etree.Element): The XML element from which to start searching for the closest <head> tag.\n",
    "\n",
    "    Returns:\n",
    "        str: The text content of the closest <head> tag without HTML tags. \n",
    "             Returns \"No text found in the <head> tag.\" if the <head> tag is empty.\n",
    "             Returns \"No <head> tag found.\" if no <head> tag is found before the element.\n",
    "    \"\"\"\n",
    "\n",
    "    # Search for the closest <head> before the current element\n",
    "    for sibling in element.itersiblings(preceding=True):\n",
    "        if sibling.tag == '{http://www.tei-c.org/ns/1.0}head':\n",
    "            # Extract the text content of the <head> tag without HTML tags\n",
    "            return sibling.text.strip() if sibling.text else \"No text found in the <head> tag.\"\n",
    "\n",
    "    return \"No <head> tag found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e419e5cc-f4df-4f38-8cac-dae2640d0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_references(xml_file, search_string):\n",
    "    \"\"\"\n",
    "    Finds references within an XML file that match a given search string, and retrieves the surrounding text.\n",
    "\n",
    "    Args:\n",
    "        xml_file (str): Path to the XML file to search within.\n",
    "        search_string (str): The string to search for within <title> elements.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentences containing the found references and their corresponding <head> tags. \n",
    "              Each item in the list is a tuple containing the sentence and the closest <head> tag text. \n",
    "              Returns an empty list if no references are found.\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    listbibl_elements = root.findall('.//tei:listBibl', namespaces=ns)\n",
    "    results = []\n",
    "\n",
    "    # Iterate through all <listBibl> elements\n",
    "    for listbibl in listbibl_elements:\n",
    "        # Search for <title> elements within <listBibl>\n",
    "        titles = listbibl.findall('.//tei:title', namespaces=ns)\n",
    "        for title in titles:\n",
    "            # Check if the title contains the search string\n",
    "            if title is not None and title.text is not None:\n",
    "                if fuzz.ratio(search_string.lower(), title.text.lower()) > 90:\n",
    "                    bibl_struct = title.getparent().getparent()\n",
    "                    xml_id = bibl_struct.get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "                    results.append((title.text, xml_id))  \n",
    "    if len(results) == 0:\n",
    "        return []\n",
    "\n",
    "    else:\n",
    "        ref_elements = root.findall('.//tei:ref', namespaces=ns)\n",
    "        sentences = []\n",
    "    \n",
    "        for ref in ref_elements:\n",
    "            # Retrieve the target attribute\n",
    "            target = ref.get('target')\n",
    "            \n",
    "            if target:\n",
    "                # Clean the target attribute by removing the \"#\" symbol\n",
    "                target_id = target.lstrip('#')\n",
    "                \n",
    "                # Compare the target ID with the searched ID\n",
    "                if target_id == xml_id:\n",
    "                    # Find the parent <p> tag\n",
    "                    parent = ref.getparent()\n",
    "                    \n",
    "                    # Search for the nearest <p> element\n",
    "                    while parent is not None and parent.tag != '{http://www.tei-c.org/ns/1.0}p':\n",
    "                        parent = parent.getparent()\n",
    "    \n",
    "                    if parent is not None:\n",
    "                        # Retrieve the entire text of the parent <p> tag\n",
    "                        text = parent.text if parent.text is not None else ''\n",
    "                        tail = parent.tail if parent.tail is not None else ''\n",
    "                        full_text = text + ''.join(parent.itertext()) + tail\n",
    "                        \n",
    "                        # Find the text around the <ref> tag\n",
    "                        ref_text = ref.text.strip() if ref.text is not None else ''\n",
    "                        \n",
    "                        # Search for the sentence containing the <ref> tag\n",
    "                        pattern = rf'[^.!?]*{re.escape(ref_text)}[^.!?]*[.!?]'\n",
    "                        match = re.search(pattern, full_text)\n",
    "                        head_tag = find_closest_head_tag(parent)\n",
    "                        if match:\n",
    "                            sentence = match.group().strip()\n",
    "                            sentences.append([sentence])\n",
    "                       \n",
    "                       \n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb528343-28ed-42cf-a58c-f2bf026ca92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../data/citing_overview_paper_paper_pdfs/XML_files_new\"\n",
    "columns = []\n",
    "\n",
    "# Parse through the XML-transformed citing documents and extract the context text for the underlying reference\n",
    "\n",
    "for i, j in df_clef_overview.iterrows():\n",
    "    snippet_extraction_list = []\n",
    "\n",
    "    for k in j[\"filtered_citing_paper_id_lists\"]:\n",
    "        filename = k.replace(\"https://openalex.org/\", \"\") + \".tei.xml\"\n",
    "        file_path = Path(path + \"/\" + filename)\n",
    "\n",
    "        if file_path.is_file():\n",
    "            extractions = find_references(file_path, j[\"Title\"])\n",
    "            if len(extractions) > 0:\n",
    "                snippet_extraction_list.append(extractions)\n",
    "\n",
    "    columns.append(snippet_extraction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6034deca-0827-482f-be27-a18decaa5703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['One of the proposed tasks asked to the participants on this data was to automatically pre-populate handover forms with relevant text-snippets (slot filling) [16].']],\n",
       "  [['The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers [51].'],\n",
       "   ['The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers [51].'],\n",
       "   ['Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 [51].']]],\n",
       " [[['Further development of search technologies for consumer health search considers self-diagnosis information needs and needs related to treatment and management of health conditions [28].']],\n",
       "  [['Evaluation campaigns and resources in this domain are presented, including TREC Medical Records Track [43,45,46], TREC Clinical Decision Support Track [36][37][38], CLEF eHealth (consumer health search [13,14,35,55] and as of 2017 search systems for the compilation of systematic reviews), i2b2 Shared Task Challenges¬≥, ALTA Shared Task ¬≥https://www.']],\n",
       "  [['Further development of search technologies for consumer health search considers self-diagnosis information needs and needs related to treatment and management of health conditions (Zuccon et al. 2016).']],\n",
       "  [['To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth [18].']],\n",
       "  [[', [3,27].']],\n",
       "  [['These queries are part of the Conference and Labs of the Evaluation Forum (CLEF) 2016 electronic health (eHealth) collection [14], which is extensively used in this paper.'],\n",
       "   ['The CLEF 2016 collection contains 300 queries and 3298 relevant documents that also have been assessed with respect to understandability [14].'],\n",
       "   ['We used the thresholds U=2 for CLEF 2015 and U=40 for CLEF 2016, based on the distribution of understandability assessments and the semantic of understandability labels [44,14].'],\n",
       "   ['Relevance assessments on the CLEF 2015 and 2016 collections are incomplete Relevance assessments on the CLEF 2015 and 2016 collections are incomplete [44,14], that is, not all top ranked Web pages retrieved by the investigated methods have an explicit relevance assessment.']],\n",
       "  [['To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth (Zuccon et al. 2016).']],\n",
       "  [['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['In the 2017 CLEF eHealth CHS task, similarly to 2016, we used the ClueWeb 12 B13In the 2017 CLEF eHealth CHS task, similarly to 2016, we used the ClueWeb 12 B134 document collection [12,18].']]],\n",
       " [[['As stated in the overview of the corresponding CLEF eHealth 2016 task While some of the text lines were short and contained a term that could be directly linked to a single ICD10 code, other lines could be run-on [10].'],\n",
       "   ['As summed up in As summed up in [10], the systems that took part in the rst edition of this shared task opted for dierent methods, namely: dictionary-based pattern matching, machine learning (including topic modeling) and information retrieval methods.']],\n",
       "  [['(2007) o er to work on radiology reports while N√©v√©ol et al. (2016) release a large dataset for ICD-10 coding of death certi cates.'],\n",
       "   ['(2007) proposent de travailler sur des rapports de radiologie tandis que N√©v√©ol et al. (2016) ont distribu√© un ensemble de certi cats de d√©c√®s annot√©s avec des codes ICD-10.']],\n",
       "  [['Recently [3,4,5], the C√©piDC task consisted in extracting ICD-10 codes from death reports in several languages (French in 2016, French and English in 2017, French, Hungarian and Italian in 2018).']]],\n",
       " [[['Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].'],\n",
       "   ['Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].'],\n",
       "   ['Figure 2 shows that hierarchy of images classes that was used [10,11] to classify all subfigures into types.'],\n",
       "   ['These figures were distributed for the Im-ageCLEFmed 2016 multi-label and subfigure classification tasks7  [11] together with the figure captions.'],\n",
       "   ['More information can be found in the working notes of CLEF 2016 [11].']],\n",
       "  [['We briefly summarize the metrics here; please see [10], [31] for full details.']],\n",
       "  [['The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].'],\n",
       "   ['The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].'],\n",
       "   ['Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].'],\n",
       "   ['7% in ImageCLEF2016 [6].'],\n",
       "   ['The best results are obtained by a combination of cross-media predictions [5,6].'],\n",
       "   ['Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].'],\n",
       "   ['7% in ImageCLEF2016 [6].'],\n",
       "   ['For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset [5,6] using a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF 2016 In ImageCLEF 2016 [6], they expand their training set to 20,997 figures and reduce the size of the test set to 3456.']],\n",
       "  [['Take ImageCLEF medical [5,33,34] as an example; it provides thousands of labeled medical images for modality classification, which is a much smaller amount than the ImageNet dataset [20], which contains 1.'],\n",
       "   ['The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].'],\n",
       "   ['The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].'],\n",
       "   ['For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF2016 [34], they expand the training set to 6776 figures and the test set to 4166 figures.'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['We obtained good performance We obtained good performance [36] using CNN-6 in the Compound Figure Detection Task [33,34].']],\n",
       "  [['Identifying image types has been done many times [6,7] but so far it has not allowed to leverage machine learning on a very large scale to our knowledge.']],\n",
       "  [['A similar problem was later reintroduced as the ImageCLEF subfigure classification sub-task in 2015 and 2016 which focused on modality classification of individual subfigures of compound figures, in effect removing the \"COMP\" or compound figure modality [14,15].']],\n",
       "  [['In addition, the feature extraction process is computationally expensive and demands expertise in developing algorithms, requiring extensive labeling and accounting for the limited visibility and variability in morphology and position of the region of interest (ROI) for modality detection [8].'],\n",
       "   ['National Library of Medicine (NLM), the ImageCLEF2013 modality classification challenge [8], and the World Wide Web.']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.'],\n",
       "   ['The evaluation required to have a minimum overlap for the subfigure division between the ground truth and the data supplied by the groups in their runs (Garc√≠a Seco de Herrera et al, 2013).']],\n",
       "  [['Text has been used in most retrieval applications [10] but has also obtained very good results in modality classification [17,2], as it is complementary to visual information.']],\n",
       "  [['They also implemented training on artificially-constructed datasets and reported superior performances on ImageCLEF data sets [11].'],\n",
       "   ['For the imageCLEF2016 dataset [11], we trained 50 epochs using a smaller batch size of 8.'],\n",
       "   ['[27] using the ImageCLEF2016 dataset [11].']],\n",
       "  [['They also implemented training on artificially constructed datasets and reported superior performances on ImageCLEF dataset (Garc√≠a Seco de Herrera et al., 2016).'],\n",
       "   ['For the imageCLEF2016 dataset (Garc√≠a Seco de Herrera et al., 2016), we trained 50 epochs using a smaller batch size of 8.'],\n",
       "   ['(2020) using the ImageCLEF2016 dataset (Garc√≠a Seco de Herrera et al., 2016).']]],\n",
       " [[['On the information retrieval and keyword spotting front, there are a plethora of works dealing with handwritten document indexing and retrieval On the information retrieval and keyword spotting front, there are a plethora of works dealing with handwritten document indexing and retrieval [26,22,8,2,57].'],\n",
       "   ['One relevant example is the ImageCLEF 2016 Handwritten Scanned Document Retrieval challenge [57], aimed at developing retrieval systems for handwritten documents.'],\n",
       "   ['We use the ImageCLEF 2016 Bentham Handwritten Retrieval dataset [57], which has images from the Bentham Transcriptorium project [10].']],\n",
       "  [['In the field of information retrieval and keyword spotting, there have been numerous efforts on handwritten document indexing and retrieval [15]  [16].'],\n",
       "   ['In the field of information retrieval and keyword spotting, there have been numerous efforts on handwritten document indexing and retrieval [15]  [16].']]],\n",
       " [[['Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.']],\n",
       "  [['First, we briefly explain the ImageCLEF collection [22] for the scalable image annotation task.']]],\n",
       " [[[\"Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge [31,33] on species identification, there is no doubt that they are able to capture discriminant visual patterns of the plants in a much more effective way than previously engineered visual features.\"]],\n",
       "  [['To this end, we fine-tune our network on leaf training images from the 2016 PlantCLEF challenge [18] which is a large but unrelated plant dataset with images taken in a different setting than either of our testing datasets.']],\n",
       "  [['Some famous networks such as AlexNet [20], GoogLeNet [25], VGG [24] have also been applied for plant identification, especially in PlantClef competition from 2014 to 2017 and have obtained higher results compared to traditional methods based on hand-designed features [6], [14], [17], [26], [27].']],\n",
       "  [['Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10].'],\n",
       "   ['GoogLeNet, VGGNet, CaffeNet, AlexNet, ResNet, Inception v4 and Inception-ResNet are used by most teams in the PlantCLEF 2016/2017 competition [9,10], including the winning team.'],\n",
       "   ['Concerning plant identification from image of single organ, we apply CNN as it has been proved to be effective in previous studies [9].']],\n",
       "  [['[25].']],\n",
       "  [['Some participants of PlantCLEF2016 competition [12] tried using geolocation information.'],\n",
       "   ['To the best of our knowledge, there are only two fine-grained datasets that have been used in geolocation related research in this field [4,12].'],\n",
       "   ['The dataset for one of the Image-CLEF/LifeCLEF competitions [2], PlantCLEF2016 [12], contains partial geolocation information (less than half of the data) and is restricted to only plants from France.']],\n",
       "  [['(1) plants (1) plants [10], (2) car brands and models [11], and (3) insects (the Formicidae ants genera) [5].'],\n",
       "   ['The PlantCLEF 2016 dataset The PlantCLEF 2016 dataset [10] consists of observations of plant specimen and provides annotations in terms of organ, species, genus, and family.']],\n",
       "  [['To obtain an accurate flower identity descriptor ùêπ ùëì ùëñùëë , we adopt the EfficientNet [57] and pretrain it on LifeCLEF2021 Plant Identification [5,[23][24][25].']],\n",
       "  [['A captura em ambiente natural, tamb√©m √© realizada em alguns trabalhos, como em [Go√´au et al. 2016, Sun et al.']],\n",
       "  [['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']]],\n",
       " [[['In [60], the technical program of LIFEClef 2016 describes the future of birdsong recognition centered in DNN, as, for instance, the one described in [61].']],\n",
       "  [['The datasets of BAD task includes Warblr [3], Chernoby1 Exculusion Zone (CEZ) [3], Freefield [16], HJA [12], Bird-CLEF [17].']],\n",
       "  [['Statistical methods with great potential for automated identification are continuously appearing in the scientific literature Statistical methods with great potential for automated identification are continuously appearing in the scientific literature [35], and as discussed above, PROTAX-Sound provides a statistically rigorous method to combine the strengths of the different techniques.']],\n",
       "  [[', [23], [13], [24], [25].']],\n",
       "  [['A number of studies also apply deep learning technologies in bird voice classifica-tion [42,43,44], however, they only use conventional deep learning approaches such as CNN and do not make any novel improvement.']],\n",
       "  [['This category involves several species, from the smallest such as insects or birds emitting trains of voiced pulses [2,3] to the largest mammals.']],\n",
       "  [['Deep neural networks (DNNs) have consistently outperformed traditional methods for bird detection since 2016 Deep neural networks (DNNs) have consistently outperformed traditional methods for bird detection since 2016 [4,5].']],\n",
       "  [['Publications related to these competitions have shown successful approaches for that task, allowing progress in the study of biodiversity (Go√´au et al., 2016), particularly since convolutional neural networks started to be used as models trained from images representing the temporal evolution of acoustic features (Piczak, 2016;Sprengel et al.']]],\n",
       " [],\n",
       " [[['2016;Stamatatos et al. 2016).']]],\n",
       " [[['The most effective methods can mislead the identification systems in almost half of the cases [199].']],\n",
       "  [[', author obfuscation [3,4,12,15]) used primarily term frequencies [5,9] and features from stylometry [8,9,18].']],\n",
       "  [['[29] presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 [19,37,38].'],\n",
       "   ['[29] presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 [19,37,38].']],\n",
       "  [['[Potthast et al. 2016] propuseram um experimento para comparar a performance de diversos ofuscadores de texto.']],\n",
       "  [['According to According to [7], author obfuscation performance is measured based on three parameters, namely safety (the ability to disguise the original author from a given text), soundness (the level of similarity between the text modified from the obfuscation process with the original text), and sensibleness (the quality of grammar and legibility of the resulting text).']],\n",
       "  [['Al√©m deste trabalho, [Potthast et al. 2016] avaliaram a efici√™ncia de tr√™s m√©todos de ofuscac ¬∏√£o [Keswani et al.']],\n",
       "  [['Evaluating content preservation in text is important even if we value safety Evaluating content preservation in text is important even if we value safety (Potthast et al., 2016).']],\n",
       "  [['Indeed, it has been shown that many AId systems can be easily deceived in adversarial contexts [11,44].']]],\n",
       " [[['In the task of author profiling, such characteristics can vary along dimensions of age, gender, and personality type, though we will focus on age, which has conventionally been the most difficult [15].'],\n",
       "   ['Age was determined either by a provided birth date in the profile, or via an estimation with the degree starting date in the education section [15].']],\n",
       "  [[', gender) [18]; on the other hand, other works aim at preventing disclosure by masking the data that may disclose that authorship [19,20].']],\n",
       "  [['Numerous works on the topic have been published based on the results of the shared Author Profiling Tasks at digital text forensics events by PAN initiative [2,5,7,[27][28][29][30].']],\n",
       "  [['One way to predict age and other demographic information uses differences in linguistics to infer age groupings One way to predict age and other demographic information uses differences in linguistics to infer age groupings [4][5][6][7][8][9].']],\n",
       "  [['\\uf0b7 In 2016 (Rangel et al., 2016b), the focus was on the cross-genre evaluation, that is, training in one genre (Twitter) and evaluating in another one (blogs, social media and reviews).']],\n",
       "  [[', [41] for the latest edition), or the affective text [49], sentiment analysis [35], and other tasks in SemEval.']],\n",
       "  [['In 2016 PAN competition [13] the goal was to test the robustness of methods from the cross-genre perspective and SVMs were the dominant paradigm.']],\n",
       "  [[\"For example, in author profiling, the task is to recognize author's characteristics, such as age or gender [10], based on a collection of author's text samples, where the effect of data size is known to be an important factor influencing classification performance [11].\"]],\n",
       "  [['Our main dataset, PAN16 TWIT (Rangel et al., 2016), is split into train and development, following Elazar and Goldberg (2018).']],\n",
       "  [['[38], we plan further studies in this field.']],\n",
       "  [['Age classes included a gap in between: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27), 30s (33-48).'],\n",
       "   ['The best results were obtained on blogs for English with an accuracy above 75% for gender and below 60% for age identification [25].']],\n",
       "  [['Possible differences between female and male writing style have been widely investigated in the literature also with respect to the interference of textual genre (Rangel et al., 2016) Comparing the performance of the classification models, it turned out that lexical information is the most predictive one 5 .']],\n",
       "  [['The use of supervised learning algorithms for AP is shown in The use of supervised learning algorithms for AP is shown in [15].'],\n",
       "   ['For training and evaluating our AP approach we used the corpus of PAN2017 competition For training and evaluating our AP approach we used the corpus of PAN2017 competition [15], which was compiled from Twitter in Spanish.']],\n",
       "  [['However, in the context of the 2016 PAN evaluation campaign, a cross-genre setting was introduced for gender prediction on English, Spanish, and Dutch, and best scores were recorded at an average accuracy of less than 60% However, in the context of the 2016 PAN evaluation campaign, a cross-genre setting was introduced for gender prediction on English, Spanish, and Dutch, and best scores were recorded at an average accuracy of less than 60% (Rangel et al., 2016).']],\n",
       "  [['Unlike Facebook Unlike Facebook [10] and Twitter [11][12][13][14], there is only a handful of studies dissecting age demographics across cQA services [2,15].'],\n",
       "   ['html (accessed on 1 February 2021)) in Twitter, blogs and social media [11][12][13][14].']],\n",
       "  [['97%) in this competition using stylistic and second-order features into SVM [20].']],\n",
       "  [['Twitter dataset (Rangel et al., 2016).'],\n",
       "   ['Elazar and Goldberg use the Twitter dataset Rangel et al. (2016), set the sensitive attribute to be age, and try to produce representations that would perform well on the main task of \"conversation detection\" (mention detection) on Tweets.']],\n",
       "  [[', 2014;Rangel et al., 2016;Verhoeven et al.']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   [', 119 , while three 34,49,66 used data sets that were created for the PAN-CLEF author profiling shared tasks [103][104][105] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [['We therefore opt for a subset of the PAN16 dataset (Pardo et al., 2016).']],\n",
       "  [[', 2018a), and mention prediction with two attributes: gender and age of authors (Rangel et al., 2016) totaling 12 experiment setups.']]],\n",
       " [[['The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF [8], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.'],\n",
       "   ['This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( [5,6]) and is closely related to the Interactive Track of the CLEF Social Book Search Lab of 2015 and 2016 [7,8].']],\n",
       "  [['The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab [4,5,6], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.']]],\n",
       " [],\n",
       " [],\n",
       " [[['They cover a period ranging from May 2015 to November 2016 and are composed by 134 different languages [5,4].']],\n",
       "  [['This resulted in an impressive amount of data and a pilot task, which are described in detail in (Ermakova et al. 2016).']]],\n",
       " [[['‚Ä¢ The Wikipedia collections WIKI10 and WIKI11 were used in the Wikipedia image retrieval task of ImageCLEF 2010 and 2011 [Popescu et al. 2010].']],\n",
       "  [['We start with a summary of multimedia databases because they represent a mature domain with more than 25 years of experience in close range digital photography; we have to mention image data collections such as ImageCLEF Wikipedia We start with a summary of multimedia databases because they represent a mature domain with more than 25 years of experience in close range digital photography; we have to mention image data collections such as ImageCLEF Wikipedia [5], PASCAL [6], ImageNet [7], and LabelMe [8].'],\n",
       "   ['The ImageCLEF Wikipedia The ImageCLEF Wikipedia [5] collection consists of 237 434 images for which 137 users provided annotations.']],\n",
       "  [['The Image-CLEF 2011 Wikipedia collection uses the ImageCLEF 2010 Wikipedia Collection [12], which is based on the September 2009 Wikipedia dumps.']],\n",
       "  [[', TerraSAR-X) the number of categories that can be identified in each image is very limited [1] and these categories are not semantically annotated contrary with multimedia where this stage is very well established and the identified categories are already annotated [2] √∑ [4].']]],\n",
       " [],\n",
       " [[['The ImageCLEF 2010 Database The image sequences used for the Robot Vision Task at the ImageCLEF 2010 challenge evaluation were taken from the previously unreleased COLD-Stockholm database The ImageCLEF 2010 Database The image sequences used for the Robot Vision Task at the ImageCLEF 2010 challenge evaluation were taken from the previously unreleased COLD-Stockholm database [34].']],\n",
       "  [['The number of participant groups increased to 8 for the 2010@ICPR edition [17], and the number of submitted runs also rose(29).']],\n",
       "  [['The third edition of the Robot Vision challenge The third edition of the Robot Vision challenge [32] was attended by seven groups, with three of them participating to both tasks: mandatory and optional.']]],\n",
       " [[[\", 2010), together with the relevant class name and class description from a set of 1,000 different classes, and (bottom-row) ImageClef VCDT'10 data set (Nowak and Huiskes, 2010) together with some of the relevant concepts from a set of 93 labels.\"],\n",
       "   ['In this section we compare SVM and TagProp on the data set of the ImageClef 2010 Visual Concept Detection and Annotation Task In this section we compare SVM and TagProp on the data set of the ImageClef 2010 Visual Concept Detection and Annotation Task (Nowak and Huiskes, 2010).'],\n",
       "   [\"The SVM and TagProp methods using the combination of Text, SIFT and LCS features, are the two best performing submissions to the ImageClef'10 challenge (Nowak and Huiskes, 2010), for more details see (Mensink et al.\"],\n",
       "   [\"ImageCLEF'10 data set We use ImageClef'10 to refer to the subset of the MIR-Flickr data set ImageCLEF'10 data set We use ImageClef'10 to refer to the subset of the MIR-Flickr data set (Huiskes and Lew, 2008) that was used as training set in the ImageClef'10 Photo Annotation Challenge2  (Nowak and Huiskes, 2010).\"],\n",
       "   ['Our baseline method is the winning system of the challenge (Nowak and Huiskes, 2010;Mensink et al.'],\n",
       "   ['For an overview of the challenge, including the participants, different methods and results, see (Nowak and Huiskes, 2010).']],\n",
       "  [[', diversifying visual contents, we tested a broad category of visual descriptors which are known to perform well in image retrieval tasks [14]: global color naming histogram (CN, 11 values) -maps colors to 11 universal color names: \"black\", \"blue\", \"brown\", \"grey\", \"green\", \"orange\", \"pink\", \"purple\", \"red\", \"white\", and \"yellow\" [15]; global Histogram of Oriented Gradients (HoG, 81 values) -represents the HoG feature computed on 3 by 3 image regions [16]; global color moments computed on the HSV Color Space (CM, 9 values) -represent the first three central moments of an image color distribution: mean, standard deviation and skewness [17]; global Locally Binary Patterns computed on gray scale representation of the image (LBP, 16 values) [18]; global Color Structure Descriptor (CSD, 64 values) -represents the MPEG-7 Color Structure Descriptor computed on the HMMD color space [19]; global statistics on gray level Run Length Matrix (GLRLM, 44 dimensions) -represents 11 statistics computed on gray level run-length matrices for 4 directions: Short Run Emphasis, Long Run Emphasis, Gray-Level Non-uniformity, Run Length Non-uniformity, Run Percentage, Low Gray-Level Run Emphasis, High Gray-Level Run Emphasis, Short Run Low Gray-Level Emphasis, Short Run High Gray-Level Emphasis, Long Run Low Gray-Level Emphasis, Long Run High Gray-Level Emphasis [20]; global descriptor which is obtained by the concatenation of all values.']],\n",
       "  [['The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task [17].'],\n",
       "   ['The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task [17].']],\n",
       "  [['2011;Nowak and Huiskes 2010;Stanek et al.']],\n",
       "  [['Previous work Previous work [16] have shown that annotation quality significantly affects the performance; 2) the MR datasets are picked up from photos uploaded by thousands of individuals, and represent the image retrieval area much more effectively; 3) there are several action concepts in the concept set of TV, however, in this work we only use static features which cannot capture the space-time aspects that are more effective to detect the action; 4) another important reason is that concepts for TV are much more infrequent than those of MR; 5) the most important but not the last reason is the two evaluation modes are different.']],\n",
       "  [[\"We conduct experiments using three public benchmark data sets: the Scene Understanding data set We conduct experiments using three public benchmark data sets: the Scene Understanding data set [4] (SUN'09), the data set of the ImageCLEF'10 Photo Annotation Task [5] (ImageCLEF), and the Animals with Attributes data set [6] (AwA).\"],\n",
       "   [\"ImageCLEF'10 data set: We use ImageCLEF'10 to refer to the subset of the MIR-Flickr data set ImageCLEF'10 data set: We use ImageCLEF'10 to refer to the subset of the MIR-Flickr data set [26] that was used as training set in the ImageCLEF 2010 Photo Annotation Challenge [5].\"],\n",
       "   ['The same features have been used in our system that won the challenge [5]; for more details see [28].'],\n",
       "   ['However, to compare our methods to the ImageCLEF 2010 challenge results, we   4 shows both top performing results of the participants in the ImageCLEF 2010 challenge (see [5] for an overview of the participants, different methods and results) and the performances of our methods.']],\n",
       "  [[\"We conduct experiments using three public benchmark data sets: the Scene Understanding dataset We conduct experiments using three public benchmark data sets: the Scene Understanding dataset [4] (SUN'09), the dataset of the ImageCLEF'10 Photo Annotation Task [12] (ImageCLEF), and the Animals with Attributes dataset [11] (AwA).\"],\n",
       "   [\"The ImageCLEF'10 data set is a subset of the MIR-Flickr data set The ImageCLEF'10 data set is a subset of the MIR-Flickr data set [10] used in the ImageCLEF Photo Annotation task in 2010 [12] as training set.\"]],\n",
       "  [['For this we use the training set of the data set used in the ImageCLEF Photo Annotation task in 2010 [13].']],\n",
       "  [[\"We report on empirical results on image data sets from the PASCAL visual object classes (VOC) 2009 [27] and ImageCLEF2010 PhotoAnnotation [28] challenges, showing that non-sparse MKL significantly outperforms the uniform mixture and ' 1 -norm MKL.\"],\n",
       "   ['Both, the BoW-S and HoG kernels (Kernels 24-25,31-32) use gradients and therefore are moderately correlated; the same holds for the BoW-C and HoC kernel groups (Kernels [26][27][28][29][30].'],\n",
       "   ['ImageCLEF 2010 PhotoAnnotation: The Image-CLEF2010 PhotoAnnotation data set[28] consists of 8000 labeled training images taken from flickr and a test set with recently disclosed labels.']],\n",
       "  [['Many of these datasets are built from online photo sharing communities such as Flickr [1,2,3,4] and even collections built from image search engines [5] consist largely of Flickr images.'],\n",
       "   [\"-The ImageCLEF Annotation Task ('CLEF') uses a subset of 18,000 images from the MIR dataset, though the correspondence is provided only for 8,000 training images [3].\"],\n",
       "   ['Details about these datasets can be found in [1,2,3,4].'],\n",
       "   ['Comparing our method to the best text-only method reported in the ImageCLEF 2011 competition [3], we observe a 7% improvement in MAP.']],\n",
       "  [[\"In the ImageCLEF'10 dataset (Nowak and Huiskes, 2010) the images are labeled with 93 diverse concepts, see Figure 4.\"]],\n",
       "  [[', 2004, Nowak, 2010].']],\n",
       "  [['-CLEF -CLEF [53] is a subset of MIR dataset with newly added labels, which contains 18,000 images.']],\n",
       "  [[', 2004;Carneiro and Vasconcelos, 2005;Nowak and Huiskes, 2010;Semenovich and Sowmya, 2010;Moser and Serpico, 2013].'],\n",
       "   [', 2010;Nowak and Huiskes, 2010;Nowak et al.'],\n",
       "   [', 2010;Nowak and Huiskes, 2010;Nowak et al.'],\n",
       "   ['Experimental results on two challenging datasets [Nowak and Huiskes, 2010;Nowak et al.'],\n",
       "   [', 2004;Carneiro and Vasconcelos, 2005;Nowak and Huiskes, 2010;Semenovich and Sowmya, 2010;Zhang et al.'],\n",
       "   [\"‚Ä¢ ImageClef '10‚Ä¢ ImageClef '105 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2010 photo annotation challenge [Nowak and Huiskes, 2010].\"]]],\n",
       " [],\n",
       " [],\n",
       " [[['In particular, we used: an extended version of the c@1 introduced in 2009 in the ResPubliQA exercise of the CLEF 4 initiative [19], the standard accuracy measure, and the mean reciprocal rank (MRR) [25].']],\n",
       "  [['La principal contribuci√≥n es la adaptaci√≥n autom√°tica de sistemas de BR existentes (espec√≠ficamente de sus patrones de pregunta-respuesta y las taxonom√≠as de TRE) a diferentes dominios restringidos, sin requerir ning√∫n esfuerzo manual como el resto de propuestas previas existentes (Sekine;Sudo;Nobata, 2002;Hovy;Hermjakob;Ravichandran, 2002;Metzler;Croft, 2005;Li;Roth, 2006;Ferr√©s;Rodr√≠guez, 2006;Terol;Mart√≠nez-Barco;Palomar, 2006;Kosseim;Yousefi, 2008;Pe√±as et al., 2009).']],\n",
       "  [['For illustration purposes we present the three best performing systems that participated in the TREC QA 2007 [28], QA@CLEF 2009 (monolingual English) [108] and NTCIR QA Track 2008 (English-Simplified Chinese and English-Japanese) [81], see Table 1.']],\n",
       "  [['The idea was to adapt the system introduced in [62] and used as part of the participation in the Paragraph Selection (PS) Task and Answer Selection (AS) Task of QA@CLEF 2010 -ResPubliQA [64].']],\n",
       "  [['fbk4faq -In fbk4faq -In (Fonseca et al., 2016), the authors proposed a system based on vector representations for each query, question and answer.'],\n",
       "   ['A deep analysis of results is reported in A deep analysis of results is reported in (Fonseca et al., 2016), where the authors have built a custom development set by paraphrasing original questions or generating a new question (based on original FAQ answer), without considering the original FAQ question.']],\n",
       "  [[', 2008;Pe√±as et al., 2009;P√©rez et al.']],\n",
       "  [['The evaluation has been performed on the Re-sPubliQA 2010 Dataset adopted in the 2010 CLEF QA Competition The evaluation has been performed on the Re-sPubliQA 2010 Dataset adopted in the 2010 CLEF QA Competition (Penas et al. (2010)).']],\n",
       "  [['Advancements in this domain also include emanating evaluation forums producing large-scale research methodologies like the Text REtrieval Conference (TREC) (Voorhees 2004) and the Cross-Lingual Evaluation Forum (CLEF) (Pe√±as et al. 2010).']],\n",
       "  [['This evaluation task was proposed at the Cross-Language Evaluation Forum (CLEF) (Pe√±as et al., 2009(Pe√±as et al.']]],\n",
       " [[['Most of the approaches to this task use standard information retrieval (IR) processes [7], [8].'],\n",
       "   ['26 [7].']]],\n",
       " [[['The first project is the annual PAN International Competition on Plagiarism Detection (PAN-PC), initiated in 2009 (Potthast, Stein, Eiselt, Barr√≥n Cede√±o, & Rosso, 2009).'],\n",
       "   ['24 in 2009 (Potthast et al., 2009).']]],\n",
       " [[['Since 2010, academic competitions have been held to enhance and compare automatic vandalism detection techniques in Wikipedia, using an annotated corpus of changes as the ground truth (Potthast and Holfeld, 2011).']],\n",
       "  [['Due to its use in the 1st International Competition on Wikipedia Vandalism DetectionDue to its use in the 1st International Competition on Wikipedia Vandalism Detection17  (Potthast, Stein, and Holfeld 2010) it is one of the most widely used corpus in the scientific literature.'],\n",
       "   ['A more exhaustive compilation can be found in (Potthast, Stein, and Holfeld 2010).'],\n",
       "   ['The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in (Mola-Velasco 2010;Potthast, Stein, and Holfeld 2010).']],\n",
       "  [['Detailed analyses of both tasks have been published as CLEF Notebook Papers [3,6], which can be downloaded at www.']],\n",
       "  [['By performing 1 At PAN 2010 and 2011, a task on Wikipedia vandalism detection was organised (Potthast and Holfeld, 2011;Potthast et al., 2010c).'],\n",
       "   ['This software has been successfully used in the PAN International Competition on Wikipedia Vandalism Detection (Adler, de Alfaro, Mola-Velasco, Rosso, and West, 2010;Potthast et al., 2010c).'],\n",
       "   ['(Potthast and Holfeld, 2011;Potthast, Stein, and Holfeld, 2010c) and authorship identification in 2011(Argamon and Juola, 2011) have been organised in PAN as well.'],\n",
       "   ['Potthast and Holfeld (2011);Potthast et al. (2010c) for an overview on automatic detection of this kind of \"contribution\".']],\n",
       "  [['[24] as part of the vandalism detection competition associated with PAN 2010.'],\n",
       "   ['See [24] for additional details.'],\n",
       "   ['We chose to use the features of We chose to use the features of [6] as being representative of a solution focused on Language (L) features due to its top-place performance in the PAN 2010 competition [24].'],\n",
       "   ['Note that performance numbers reported forNote that performance numbers reported for[6] and[7] differ from those reported in[24] due to our use of 10-fold cross validation over the entire PAN2010 corpus and differences in ML models (e.']],\n",
       "  [['On the PAN-WVC-10 corpus, which has become fairly popular since, more recent work [20], [19] reported an accuracy of 96% (thus comparable to ours).']],\n",
       "  [['The interest in this aspect has been reflected in the holding of two International Competitions on Wikipedia Vandalism Detection in 2010 and 2011, in the context of the Cross Language Evaluation Forum Conferences, CLEFs [5,6].']],\n",
       "  [['In fact, a shared task organized by Potthast, Stein, and Holfeld (2010) resulted in plenty of approaches.'],\n",
       "   ['Otherwise, most papers on Wikipedia vandalism propose automatic vandalism detection tools: Potthast, Stein, and Gerling (2008) first developed machine learning technology for this purpose, and many of the approaches in existence today have been developed or derived from the results of two shared tasks at PAN 2010and PAN 2011(Potthast, Stein, and Holfeld 2010;Potthast and Holfeld 2011).']],\n",
       "  [['ROC-AUC has been a standard machine learning metric [16].'],\n",
       "   ['ROC-AUC has been a standard machine learning metric [16].']],\n",
       "  [['The interest in this aspect has been reflected in the holding of two International Competitions on Wikipedia Vandalism Detection in 2010 and 2011, in the context of the Cross Language Evaluation Forum Conferences, CLEFs [5,6].']],\n",
       "  [['Thus, here we are using data provided by Wikipedia, gathered using Mechanical Turk [13].'],\n",
       "   ['To help address this issue, Wikipedia has compiled a set of rules, the Wikipedia norms [13], to maintain and organize its content.']],\n",
       "  [['This use case is relevant because Wikipedia is an open and collaborative community with norms to maintain and organize its content [76], including the requirement to use proper writing style, refrain from removing content, avoid editing wars, and not engage in hate speech.']]],\n",
       " [[['For system evaluation in English, our system (RGAI) participated in the third Web People Search Task challenge [1].'],\n",
       "   ['The third WePS shared task The third WePS shared task [1] introduced a novel subtask which sought to mine attributes for persons, i.'],\n",
       "   ['The previous value is maximal (1) when the number of shared categories is lower than or equal to the number of shared clusters, and it is minimal (0) when the two items do not share any category.']],\n",
       "  [['Web People Search (WePS) systems Web People Search (WePS) systems [3] are concerned with clustering the results of ambiguous name queries in order to distinguish between people with the same names.'],\n",
       "   ['Evaluation campaigns of the Web People Search task Evaluation campaigns of the Web People Search task [4,5,6,3] generally focus on ways of clustering documents into sets which characterize different persons that share the same name.']],\n",
       "  [['Our approach to processing Web data sets has been very much influenced by work done in the Web People Search (WePS) task Our approach to processing Web data sets has been very much influenced by work done in the Web People Search (WePS) task [4].'],\n",
       "   ['The second [9] and third [4] editions of the WePS campaign included an Attribute Extraction task [10], where personal information like date of birth, birthplace, occupation, and nationality had to be extracted from Web search results.'],\n",
       "   [\"The clustering measures used in the first WePS campaign don't apply in MTT, but the more traditional recall and precision measures used in the later WePS-3 Attribute Extraction Task [4] are useful for measuring the performance of the nominal filter.\"]],\n",
       "  [['The WePS (Web People Search) Evaluation cam- paign evaluated systems that extract personal attributes, including dates of birth, birth places, affiliations, occupations, and awards [1].']],\n",
       "  [['Our approach is based on the following processes: (1) creating curriculum vitae using related work Our approach is based on the following processes: (1) creating curriculum vitae using related work [1], (2) extracting the names of places where the person studied and worked from the vitae, (3) getting location information from the place names using Google Maps API, and (4) displaying a vitae that includes location information along with a map using Google Maps JavaScript API.'],\n",
       "   ['Recall = correct extracted place names correct place names to be extracted Recall = correct extracted place names correct place names to be extracted (2) Note that correct place names are duplicated.'],\n",
       "   ['WePS-3 conducted a competitive evaluation on person attribute extraction on web pages WePS-3 conducted a competitive evaluation on person attribute extraction on web pages [2].']],\n",
       "  [[\"There is also a workshop on web people searching, called WePS (standing for Web People Search) [1][2][3], that is concerned with organizing web results for a person's given name.\"]],\n",
       "  [['Most similarity measurements (Artiles et al., 2010;Miller, 1995) could be used in the proposed method.']],\n",
       "  [['The WePS campaign (Artiles et al., 2010) introduced a task which sought to mine attributes for persons, i.'],\n",
       "   ['The third Web People Search (WePS3) The third Web People Search (WePS3) (Artiles et al., 2010) datasets were used for testing our English name disambiguation approach described in Section 6.'],\n",
       "   ['The third WePS shared task (Artiles et al., 2010) introduced a novel subtask which sought to mine attributes for persons, i.'],\n",
       "   ['Our system participated in the third WePS challenge Our system participated in the third WePS challenge (Artiles et al., 2010) and achieved top results on the person attribute extraction subtask.'],\n",
       "   ['When we investigated the manually annotated attributes in the training and test sets of the second WePS Campaign attribute extraction task (Artiles et al., 2010), we also found that the majority of attributes consisted of multiword named entities.'],\n",
       "   ['The author participated in the third WePS challenge (Artiles et al., 2010) and achieved top results on the person attribute extraction subtask.']],\n",
       "  [['There is a sub-task of WePS (Web People Search Workshop) (Artiles et al. 2010;Artiles et al.']],\n",
       "  [['The WePS-3 workshop was an evaluation Task under the scope of TebleCLEF (Artiles et al., 2010).']],\n",
       "  [['5 Document annotation matrix from the TREC KBA track.'],\n",
       "   ['The slot filling task has been addressed at the Web People Search (WePS) evaluation campaign, targeting only persons The slot filling task has been addressed at the Web People Search (WePS) evaluation campaign, targeting only persons [5], and at the Knowledge Base Population track of the Text Analysis Conference (TAC KBP), targeting persons, organizations, and geo-political entities [37,45,74].'],\n",
       "   ['For details of the test collections, evaluation methodology, and evaluation metrics, we refer to the overview papers of the respective campaigns [5,74].']],\n",
       "  [['WePS-2/3 [11] conducted competitive evaluation on person attribute extraction on web pages.']],\n",
       "  [['WePS-2/3 conducted competitive evaluation on person attribute extraction on web pages [8].']]],\n",
       " [[['For the microblog category, the collection was obtained from the trial and training data sets of the Task 2 of the well recognised international competition known as WePS-3 evaluation campaign For the microblog category, the collection was obtained from the trial and training data sets of the Task 2 of the well recognised international competition known as WePS-3 evaluation campaign [1].']],\n",
       "  [['However, research works dealing with the problem of word ambiguity (in this case, to determine whether a word refers to a company or not) have only been studied in literature recently (Amigo, Artiles, Gonzalo, Spina & Liu, 2010).'],\n",
       "   ['The works which have attempted a solution on the tweet categorization task as part of Task 2 of the WePS-3 evaluation campaign 4 for the online reputation management are summarised in The works which have attempted a solution on the tweet categorization task as part of Task 2 of the WePS-3 evaluation campaign 4 for the online reputation management are summarised in (Amigo et al., 2010).']],\n",
       "  [['According to [46], the classifier described in [45] outperformed all other competing classifiers in the WePS-3 evaluation campaign.']],\n",
       "  [[\", understanding whether the mention of an entity like 'apple' refers to the fruit or to the company) [3] by exploiting the content generated by users on other social networks.\"],\n",
       "   ['As these company profiles were developed in the context of the WePS3 task [3], we assume that their accuracies are bounded by the values in the first row of Table III.'],\n",
       "   ['Several works Several works [3], [28], [29] have addressed the problem of tweet classification in various contexts.']],\n",
       "  [['The results of all the participant systems can be seen in (Amig√≥ et al. 2010) .'],\n",
       "   ['In that table we can also see other metrics like precision, recall, and F-measure for related tweets (+) and unrelated tweets(-), all of them, defined in (Amig√≥ et al. 2010).']],\n",
       "  [['We evaluated our methods on two different datasets: the \"WWW\" dataset We evaluated our methods on two different datasets: the \"WWW\" dataset [9], that was used as a benchmark in a number of entity resolution methods and the WePS dataset [3].'],\n",
       "   ['We evaluated our methods on two different datasets: the \"WWW\" dataset We evaluated our methods on two different datasets: the \"WWW\" dataset [9], that was used as a benchmark in a number of entity resolution methods and the WePS dataset [3].']]],\n",
       " [[['Most CLIR research has been focused on the cross-lingual matching function although the recent CLEF conferences have initiated some work on the second function: (partial) translation for document selection (Oard & Gonzalo, 2002).']],\n",
       "  [['The interactive CLEF experimental framework The interactive CLEF experimental framework (Gonzalo & Oard, 2002) was followed, but additional measurements (both objective and subjective) were recorded.']],\n",
       "  [[\"El objetivo del iCLEF'2002 El objetivo del iCLEF'2002 (Gonzalo and Oard, 2002) fue proporcionar un marco de referencia com√∫n para realizar experimentos comparando dos sistemas de recuperaci√≥n de informaci√≥n transling√ºe que permitan a un usuario que desconoce el idioma de los documentos realizar una expansi√≥n interactiva de la consulta, una selecci√≥n interactiva de documentos (al igual que el a√±o anterior), o ambas opciones a la vez.\"]],\n",
       "  [['(Oard & Gonzalo, 2002)).']],\n",
       "  [['A special task was designed this year for interactive user experiments A special task was designed this year for interactive user experiments (Oard and Gonzalo 2001).']]],\n",
       " [],\n",
       " [],\n",
       " [[['For the 67  Training Protocol We train a single encoder-only transformer model with a separate classification head for each of the 107 tasks.']],\n",
       "  [['The use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing The use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing (Gasco et al., 2021).']],\n",
       "  [['For the Spanish paper, we use paper abstracts open sourced by the Mesinesp (Gasco et al., 2021).']]],\n",
       " [[[', CheckThat 2021 Cross-Language Evaluation Forum (CLEF) [41,42].']],\n",
       "  [[', 2018;Shaar et al., 2021) largely ignore relevant attributes of the claim (e.']],\n",
       "  [['e, tweets containing verifiable claims that we think will be of general interest Shaar et al. (2021), and consider them as authority supporting tweets.']],\n",
       "  [['To further verify the effectiveness of our method, we conduct a comparison on the document-level CE dataset (CLEF-2021, subtask 1B To further verify the effectiveness of our method, we conduct a comparison on the document-level CE dataset (CLEF-2021, subtask 1B (Shaar et al., 2021)) using our method and Claimbuster.']],\n",
       "  [['More sophisticated representation techniques, such as LIWC [10] and ELMo [11], have also been investigated.']]],\n",
       " [[['initiative [5][6][7], that has advanced research in this field by producing benchmarks and baselines and organizing shared tasks.'],\n",
       "   ['Lab [6,7,25].'],\n",
       "   ['Lab [6,7,25].'],\n",
       "   ['Lab Task 2 (verified claim retrieval ) challenges of the CLEF2020 [25], CLEF2021 [6] and CLEF2022 [7] initiatives in English language.']]],\n",
       " [[[', CheckThat 2021 Cross-Language Evaluation Forum (CLEF) [41,42].']],\n",
       "  [['We examine three datasets: the widely-used LIAR (Wang, 2017), CT-FAN-22 We examine three datasets: the widely-used LIAR (Wang, 2017), CT-FAN-22 (K√∂hler et al., 2022) that contains both English and German corpora, and a new dataset LIAR-New.'],\n",
       "   ['Second, we use the CT-FAN-22 dataset (K√∂hler et al., 2022) for additional tests, including transfer and multilingual settings.'],\n",
       "   ['The state-of-the-art approaches are reported by The state-of-the-art approaches are reported by K√∂hler et al. (2022) from a competition run on this dataset.']],\n",
       "  [['Recent works Recent works [58,70,86,38,22] have prominently recognized evidence as a premier element in fake news detection apart from patterns.']]],\n",
       " [],\n",
       " [[['Finally, different challenges have been organized lately for named entity recognition and relation extraction in Spanish biomedical texts Finally, different challenges have been organized lately for named entity recognition and relation extraction in Spanish biomedical texts [19,20].']]],\n",
       " [[['However, only recently studies have been carried out in the health domain, also due to the growing impact of Consumer Health Search (CHS) [24,25].']]],\n",
       " [],\n",
       " [[['VQA-MED-2018 [9], VQA-RAD [14], VQAMED-2019 [4], RadVisDial [13], PathVQA [11], VQA-MED-2020 [3], SLAKE [18], and VQA-MED-2021 [5].']],\n",
       "  [['The VQA-MED-2020 dataset represented the third iteration of this influential initiative, presented as part of ImageCLEF 2020 to advance the answer to medical visual questions [278].']],\n",
       "  [[', 2020), VQA-Med-2021(Ben Abacha et al., 2021), and MIMIC-Diff-VQA (Hu et al.']]],\n",
       " [],\n",
       " [[['In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.']],\n",
       "  [['Motivated by this, we extend our proposed model Motivated by this, we extend our proposed model [9] for the ImageCLEFmedical 2021 [10], [11] by adding an explainability module.'],\n",
       "   ['We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset [10], [11], which includes three sets: the training set composed of 2756 medical images; the validation set and the test set consisting of 500 and 444 radiology images, respectively.']],\n",
       "  [[', 2022;Pelka et al., 2021;Hsu et al.'],\n",
       "   [\"To the best of our knowledge, no existing figure-caption datasets explicitly contain the figures' accompanying documents (Pelka et al., 2021;Hsu et al.\"]],\n",
       "  [['The ROCO dataset has been used in the medical caption tasks The ROCO dataset has been used in the medical caption tasks [3][4][5][6] at the Image Retrieval and Classification Lab of the Conference and Labs of the Evaluation Forum (ImageCLEF) 7 .']]],\n",
       " [],\n",
       " [],\n",
       " [[['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']],\n",
       "  [['[20] Natural Images 50,000 10,000 Tiny ImageNet [21] Natural Images (ImageNet subset) 100,000 10,000 Stanford dogs [22] Natural Images (Dog breeds) 12,000 8,580 Flowers-102 [23] Natural Images (Flower species) 2,040 6,149 CUB-200-2011 [24] Natural Images (Bird species) 5,994 5,794 Stanford Cars [25] Natural Images (Car models) 8,144 8,041 Food-101 [26] Natural Images (Food categories) 75,750 25,250 DTD [27] Texture Images 1,880 1,880 47 NEU Surface Defects [28] Surface Defect Images 1,440 360 6 UC Merced Land Use [29] Remote Sensing Images 1,680 420 21 EuroSAT [30] Remote Sensing Images 18,900 8,100 10 PlantVillage [31] Plant Images 44,343 11,105 39 PlantCLEF [32] Plant Images 10,455 1135 20 Galaxy10 DECals [33] Astronomy Images (Galaxy Morphology) Stanford Dogs [22]: Stanford Dogs dataset is a comprehensive dataset for fine-grained image classification, containing 20,580 images of 120 different dog breeds.'],\n",
       "   ['PlantCLEF PlantCLEF [32]: The PlantCLEF dataset is a large-scale dataset for plant identification, comprising millions of images covering thousands of plant species, including trees, flowers, fruits, and leaves.']]],\n",
       " [[['More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning [13,17].'],\n",
       "   ['Participants in recent iterations of BirdCLEF have relied heavily on DCNNs, making use of transfer learning and data augmentation [17].'],\n",
       "   ['Participants in recent iterations of BirdCLEF have relied heavily on DCNNs, making use of transfer learning and data augmentation [17].'],\n",
       "   ['Common to pipelines evaluated in BirdCLEF competition are specific post-processing techniques such as filtering by time and location of recording, boosting, and ensembling [17].']],\n",
       "  [[\"Dataset yang digunakan pada penelitian ini berasal dari dataset ESC-50 Dataset yang digunakan pada penelitian ini berasal dari dataset ESC-50 [8] untuk suara gergaji mesin, UrbanSound8K [9] untuk suara tembakan senjata, dan BirdClef-2021 [10] untuk 8 jenis suara burung (Wild Turkey, Black-bellied Plover, American Coot, Great Crested Flycatcher, Townsend's Solitaire, Ruddy Turnstone, Common Chlorospingus, dan Black-andwhite Warbler).\"]],\n",
       "  [['The SSL pretraining used the BirdCLEF2021 dataset from Kaggle challenge The SSL pretraining used the BirdCLEF2021 dataset from Kaggle challenge [15], containing ‚àº 63 k recordings of various lengths from the Xeno-Canto public repository of bird sound recordings [16].']],\n",
       "  [['However, the precondition for employing deep learning is the availability of adequate datasets 4 .']]],\n",
       " [[['Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification.']]],\n",
       " [[['Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].']],\n",
       "  [[', 2018), and snakes (Picek et al., 2021).']],\n",
       "  [['Therefore, we will try to handle all these drawbacks in our future works 27,28 .']],\n",
       "  [['Other paper used CNN and InceptionV3 to achieve an accuracy of 90% for 2 snake classes [13].']]],\n",
       " [],\n",
       " [[[', 2018(Kestemont et al., , 2019(Kestemont et al.'],\n",
       "   [', 2018(Kestemont et al., , 2019(Kestemont et al.']],\n",
       "  [['–û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ [7].'],\n",
       "   ['–ú–Ω–æ–≥–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç, —Å—Ç–∞–≤—à–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: —á–∞—Å—Ç–æ—Ç—ã —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å–ª–æ–≤, n-–≥—Ä–∞–º–º—ã —Å–ª–æ–≤, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ [7].']],\n",
       "  [['These include work on controlled corpora of blog, emails and interviews (Barlas and Stamatatos 2020), and fanfiction (Kestemont et al. 2020).']],\n",
       "  [['We consider such test (Kipnis, 2020) in the context of the authorship verification challenge of (Kestemont et al., 2020).']],\n",
       "  [['2020;2021) for AV that was shown to perform well at previous PAN authorship verification shared tasks (Kestemont et al. 2020).']],\n",
       "  [['‚Ä¢ In the 2020 PAN authorship verification task [15], methods employing neural networks made an appearance, particularly in the form of Siamese neural networks.']]],\n",
       " [],\n",
       " [[['At the 2021 Conference and Labs of the Evaluation Forum (CLEF), for instance, 66 academic teams participated in the task of directly detecting whether a Twitter user is likely to spread hate or not [51].'],\n",
       "   ['At the 2021 Conference and Labs of the Evaluation Forum (CLEF), for instance, 66 academic teams participated in the task of directly detecting whether a Twitter user is likely to spread hate or not [51].']],\n",
       "  [['Several works have been used in this category, such as the author profiling of the Hate Speech Spreader Detection shared task organized by PAN 2021 [21], char/word-IDF [22,23], and Vader/RoBERTa word embeddings [24], trigram features and POS tags [25][26][27], n-grams [28][29][30][31][32], and emotions [26,33].'],\n",
       "   ['Shallow lexical features [25], dictionaries [146], sentiment analysis [147], linguistic characteristics [148], knowledge-based features [149], and meta-information [21] were described in the literature as features connected to tweets.']],\n",
       "  [['0% for tweets in Spanish (Rangel et al., 2021).']]],\n",
       " [[['The text simplification task (Ermakova et al. 2021) is a promising research direction for providing simplified explanations.']]],\n",
       " [],\n",
       " [[[', 2020b(Bondarenko et al., , 2021) ) featured a related track.']],\n",
       "  [[', 2019;Bondarenko et al., 2021).']],\n",
       "  [[', underlying reasons -sometimes called perspectives [18,21], premises [13,26] or frames [3,47]).']],\n",
       "  [[', 2017b;Bondarenko et al., 2021), argument analysis (Feng and Hirst, 2011;Janier et al.']],\n",
       "  [[', 2019;Bondarenko et al., 2021) or a suitable counter-argument given an input argument (Wachsmuth et al.']],\n",
       "  [[', 2018), during argument retrieval, (Bondarenko et al., 2021) or in the legal context for case law retrieval (Locke and Zuccon, 2018).'],\n",
       "   [', 2018), and Webis-Touch√© (Bondarenko et al., 2021).'],\n",
       "   ['Webis-Touch√© 2020 Webis-Touch√© 2020 (Bondarenko et al., 2021) is an argument retrieval dataset based on the args.']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']]],\n",
       " [[[\"Enfin, les usagers √©taient dispos√©s √† soumettre de nouveau leur requ√™te ou √† la reformuler et √† examiner un bon nombre d'images, afin de trouver ce qu'ils cherchaient (Clough, Sanderson et M√ªller, 2004).\"]],\n",
       "  [['As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2004 track As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2004 track [17] that promotes cross language image retrieval has initiated a new medical retrieval task in 2004.'],\n",
       "   ['3 √ó 1 and 1 √ó 3), we set the similarity to zero if the difference in a grid dimension between two image indexes   3 compares the MAPs of the automatic VisMed run with those of the top 5 automatic runs as reported in ImageCLEF 2004 [17] where the percentages of improvement are shown in brackets, \"RF\" stands for the use of pseudo relevance feedback and \"Text\" means the case notes were also utilized.'],\n",
       "   ['Using the ImageCLEF 2004 CasImage medical database and retrieval task, we have demonstrated the effectiveness of our framework that is very promising when compared to the current automatic runs [17].']],\n",
       "  [['Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.']],\n",
       "  [['ImageCLEF 6 [5,6] has started within CLEF 7 (Cross Language Evaluation Forum [38]) in 2003 with the goal to benchmark image retrieval in multilingual document collections.']],\n",
       "  [['It is worth noting that image classification is a well studied area with several standard measures developed for the evaluation of automated classification It is worth noting that image classification is a well studied area with several standard measures developed for the evaluation of automated classification [26][27][28].']],\n",
       "  [['It was created for evaluation on the image track of the Cross Language Evaluation Forum [5].']],\n",
       "  [['As a benchmark project, ImageCLEF is more and more wellknown with its open data platform As a benchmark project, ImageCLEF is more and more wellknown with its open data platform [12].']],\n",
       "  [['One of the first medical multimedia retrieval challenges was ImageCLEF that started with a medical task in 2004 [60].']],\n",
       "  [['[9] for instance already shared a number of issues with KVQAE, such as multimodal information fusion.']]],\n",
       " [[['Research on SCR initially investigated IR for planned speech content such as news broadcasts and documentaries [1], [2].']],\n",
       "  [['Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].']]],\n",
       " [],\n",
       " [[[\"Pour le moment, les syst√®mes de QR bilingues sont beaucoup moins performants que les syst√®mes de QR monolingues, mais les chercheurs se disent encourag√©s par les r√©sultats obtenus jusqu'√† maintenant (Vallin et al, 2005).\"]],\n",
       "  [['Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 [6].'],\n",
       "   ['Besides, section 5 presents and discusses the results obtained using all official English questions of QA CLEF 2004 [8] and 2006 [6].'],\n",
       "   ['This fact has been confirmed in the last edition of CLEF 2006 [6].'],\n",
       "   ['Nowadays, at CLEF 2006 [6], three different approaches are used by CL-QA systems in order to solve the bilingual task.'],\n",
       "   ['Furthermore, this affirmation is corroborated checking the official results on the last edition of CLEF 2006 [6] where our method [3] has being ranked first at the bilingual English-Spanish QA task.'],\n",
       "   ['07% at CLEF 2006) and than other current bilingual QA systems [6].']],\n",
       "  [['BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006).'],\n",
       "   ['For this purpose we have used the CLEF 2006 set of questions, the EFE corpora, the evaluation measures34 proposed by the CLEF organization (Magnini et al, 2006) and our official results in this competition.']],\n",
       "  [['The Cross-Language Evaluation Forum or CLEFThe Cross-Language Evaluation Forum or CLEF3 , established in 2000, promotes multilingual question answering, where the question is posed in a different language than the language of the documents in the repository [73].']],\n",
       "  [['We compared the results of the best systems of both EQueR and CLEF QA task in 2004 (Valin et al., 2004) and found them to be consistent.']],\n",
       "  [['We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [],\n",
       " [[[\"Enfin, les r√©sultats de 2005 soulignent le besoin d'offrir une interface donnant plus de contr√¥le √† l'individu pour la formulation et la reformulation des requ√™tes (Clough et al, 2005).\"]],\n",
       "  [['The test collection ImageCLEFMed 2005 contains Casimage (9000 images), MIR (2000 images), PEIR (33000 images), and PathoPic (9000 images) (Clough et al., 2005).']],\n",
       "  [[\"Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval (Clough, 2003), confirming Flank's conclusion that image CLIR can be made to work.\"],\n",
       "   ['As part of preparations for the formation of the imageCLEF collection As part of preparations for the formation of the imageCLEF collection (Clough and Sanderson, 2003), a preliminary evaluation of image CLIR was conducted on the St.']],\n",
       "  [['The IRMA 10000 databaseThe IRMA 10000 database1 was used in the automatic annotation task of the 2005 ImageCLEF evaluation [17].'],\n",
       "   ['Table 1 gives an overview of the best results obtained for the IRMA tasks from the ImageCLEF 2005 evaluation [17] along with the results we obtained using sparse patch histograms with and without position information.']],\n",
       "  [['The St Andrews collection has been used for the past three years at ImageCLEFThe St Andrews collection has been used for the past three years at ImageCLEF5 , the cross-language image retrieval task (Clough, M√ºller, Hersh, Deselaers, Lehmann, Grubinger, 2005;Clough, M√ºller, Sanderson, 2005;Clough and Sanderson, 2003).']],\n",
       "  [['Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.']],\n",
       "  [['ImageCLEF is another image benchmarking competition that is part of the Cross Language Evaluation Framework (CLEF) competition (Clough et al. 2004).']],\n",
       "  [['Another recent example is where users complement their traditional keyword query with additional information, such as example documents [24], tags [73], images [76,91], categories [338], or their search history [20].']],\n",
       "  [['Accordingly, this technique is less time-consuming compared to the technique that depends on texts for the purposes of indexing and retrieving [5].']],\n",
       "  [['The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['Spreading from textual search evaluations [25], evaluation campaigns have been established for image retrieval [9], as well as for video content search [69].']]],\n",
       " [[['At other times, corpora is automatically obtained (for instance, English and Czech interview recordings of Survivors of the Shoah Visual History Foundation using in CL-SDR 2006 [11] were transcribed using a ASR system with the consequent increase of transcription errors).']],\n",
       "  [['2004), oft-recorded in noisy environments, mean WER for the 2006 ASR transcripts is reported as 25% (Pecina et al. 2008).']],\n",
       "  [['These results are confirmed for a very different retrieval task of unstructured oral testimonies in the speech retrieval task introduced at CLEF 2005 [6].'],\n",
       "   ['We are currently exploring the use of our document correction method for the CLEF speech retrieval task based on oral testimonies [6].']],\n",
       "  [['While best retrieval accuracy was achieved using a monolingual evaluation task where the queries were English, our results for the cross language task were the best among those making formal submissions to the CLEF 2007 CL-SR task, showing the lowest decrease relative to monolingual performance when queries were translated from their source language to English [12].']],\n",
       "  [['The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) [11,13].']],\n",
       "  [['For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) [22] and mean average segment precision (MASP) [10].']],\n",
       "  [['The data is drawn from the Cross-Language Speech Retrieval Track of the Crosslanguage Evaluation Forum (CLEF CL-SR) (Pecina et al., 2007) collection.']],\n",
       "  [['The transcripts produced with Automatic Speech Recognition (ASR) systems tend to contain many recognition errors, leading to low Information Retrieval (IR) performance [1] unlike the retrieval from broadcast speech, where the lower word error rate did not harm the retrieval [2].']],\n",
       "  [['Since the RSR 2011 task was a known-item search, one useful evaluation metric is the Mean Reciprocal Rank (MRR); additionally we apply a metric that evaluates the ranking and takes account of the distance between the predicted and actual jump-in point (mean Generalized Average Precision (mGAP)) [15].']],\n",
       "  [['mGAP [24] awards runs that not only find the relevant items earlier in the ranked output list, but also are closer to the jump-in point of the relevant content.']],\n",
       "  [['More recently in 2005 the Cross-Language Evaluation Forum (CLEF) has started a speech retrieval track on spoken interviews [33].']],\n",
       "  [['Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.']],\n",
       "  [[', 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007;Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b).']],\n",
       "  [['Moreover, user studies have repeatedly revealed that in interactive applications real users would often willingly trade some potential of retrieval effectiveness if doing so would lead to more understandable and predictable system behavior [Zhang et al. 2007].'],\n",
       "   ['Evaluating search systems using predefined passages is at best an imperfect model of the real task faced by a speech retrieval system, but work on alternative evaluation designs has started only recently [Pecina et al. 2007].']],\n",
       "  [['In this paper we report on work carried out for the Cross-Language Evaluation Forum (CLEF) 2005 Cross-Language Speech Retrieval (CL-SR) track (White et al, 2005).'],\n",
       "   ['See (Oard et al, 2004) and (White et al, 2005) for details.']],\n",
       "  [['In all experiments, we used the training dataset from the CLEF CL-SR 2007 task [18].']],\n",
       "  [['The focus then shifted towards spoken content that is produced spontaneously such as interviews, lectures and TV shows [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['We used the retrieval model PL2 and QE model BO1 described in the previous section (see Equations 1,3) to calculate the results shown in Table III, which shows performance in terms of Mean Average Precision (MAP), Recall and Precision for top 10 documents (P@10).']],\n",
       "  [['Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].']],\n",
       "  [['(2) spoken document retrieval: podcasts can be represented by their transcripts of their spoken content -in this way podcast search is related to spoken document retrieval [28], [3], [62].']],\n",
       "  [['Retrieval from an archive of oral history has also been well-studied [15], but lacks the multispeaker, multi-genre aspect of podcasts.']]],\n",
       " [[['To deal with these issues, a lot of research has been done in recent years, and the output has been presented in different styles, including research papers [7], books [8,9], doctoral dissertations [10,11], test collections [12], retrieval evaluation events [13], etc.'],\n",
       "   ['Procedure-1 computes the BM25 score of all the papers against the initial search query, which is combined with the citation analysis score to compute the final base weight of the candidate papers and ranks the initial search results in steps [6][7][8][9][10][11][12][13][14].']]],\n",
       " [],\n",
       " [[['For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].'],\n",
       "   ['As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.'],\n",
       "   ['The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].']],\n",
       "  [['It was then used for producing reports and overview graphs about the submitted experiments [1,3].']],\n",
       "  [['To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are [12,13].'],\n",
       "   ['This is the case, for example, of the CLEF 2005 multilingual merging track [12], which provided participants with some of the CLEF 2003 multilingual experiments as list of results to be used as input to their merging algorithms.']],\n",
       "  [[', 2005;Nunzio et al., 2005).']]],\n",
       " [],\n",
       " [[['Several QA reports [6,14] indicate that the translation errors cause an important drop in accuracy for cross-language tasks with respect to the monolingual exercises.']],\n",
       "  [['Recently there has been intensive research in this area, fostered by evaluation-based conferences such as the Text REtrieval Conference (TREC) (Voorhees, 2001b), the Cross-Lingual Evaluation Forum (CLEF) (Vallin et al., 2005), and the NII-NACSIS Test Collection for Information Retrieval Systems workshops (NTCIR) (Kando, 2005).']],\n",
       "  [['QA can be open-domain QA can be open-domain [7,8] or closed-domain [9].']],\n",
       "  [['Voorhees, 2005;Vallin, 2005).']],\n",
       "  [['We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [],\n",
       " [[['A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in [1].']],\n",
       "  [['(GBIF) Train and test occurrences datasets from the previous year edition [5] were merged to feed the current challenge.']]],\n",
       " [[['Many recent works, in particular those involved in recent bird classification Many recent works, in particular those involved in recent bird classification [39] and bird audio detection evaluations [40], have focused on the use of CNNs.']],\n",
       "  [['It was shown in previous editions of BirdCLEF [35,36] that systems for identifying birds from mono-directional recordings are now performing very well and several mobile applications implementing this are emerging today.']]],\n",
       " [[['Convolutional Neural Networks performed well in other fine-grained species identification tasks, including plant species classification [11,12], dog classification [19], bird classification [35,36], or classification of species in general [33].'],\n",
       "   ['For the FGVCx Fungi Classification challenge, we trained an ensemble of Inception-v4 and Inception-ResNet-v2 networks [28], inspired by the winning submission in the ExpertLifeCLEF plant identification challenge 2018 [11].']],\n",
       "  [['Other recent research by [24,25] focused on investigating how automated methods for identifying species from imagery data compare to manual annotations performed by citizen scientists.'],\n",
       "   ['Further, research on performing large-scale experiments on automatic approaches for identifying species from imagery data collected from citizen science portals revealed that automated species classification performs similarly and sometimes even better than manual annotations performed by citizen scientists [24,25,29].'],\n",
       "   ['In particular the authors of [24] presented a study encompassing 10,000 plant species while the authors of [25] performed a study for more than 5,000 categories of plants, animals, and fungi.']],\n",
       "  [['Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].'],\n",
       "   ['For the FGVCx Fungi Classification challenge, we trained an ensemble of six models (listed in Table 2) based on Inception-v4 and Inception-ResNet-v2 architectures [65], and inspired by our winning submission in the ExpertLifeCLEF plant identification challenge 2018 [41].']],\n",
       "  [['To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data [7,8] and with learning to classify ImageNet [19] classes from a long-tailed noisy training dataset downloaded from the web, Webvision 1.']],\n",
       "  [['These advances allowed large-scale experiments and even the construction of models that surpassed human performance in some plant recognition tasks [Go√´au et al. 2018].']],\n",
       "  [['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']]],\n",
       " [[['In the arms race between authorship attribution and authorship obfuscation, it is important that both attribution and obfuscation consider the adversarial threat model In the arms race between authorship attribution and authorship obfuscation, it is important that both attribution and obfuscation consider the adversarial threat model (Potthast et al., 2018).']]],\n",
       " [[['The research of user profiling identification and detection for Arabic language is even more scarce [9], [10].']],\n",
       "  [['To address this situation, one potential benchmark setting is to require participating teams to submit or upload the used executable processing pipelines that generate automatic results [6].']],\n",
       "  [['Twitter verisi √ºzerinden cinsiyet tahminlemesi konusunda CLEF 2018 kapsamƒ±nda PAN 2018 c ¬∏alƒ±s ¬∏tayƒ±nda ortak bir c ¬∏alƒ±s ¬∏ma Twitter verisi √ºzerinden cinsiyet tahminlemesi konusunda CLEF 2018 kapsamƒ±nda PAN 2018 c ¬∏alƒ±s ¬∏tayƒ±nda ortak bir c ¬∏alƒ±s ¬∏ma [3] d√ºzenlenmis ¬∏tir.']],\n",
       "  [[', 2017;RANGEL et al., 2018).']],\n",
       "  [['Only for Arabic it was possible to improve accuracy (albeit less than 2%) [21].']],\n",
       "  [[\"For instance, more recently, during PAN 2018 [4], multiple techniques were explored to predict user's gender on twitter posts including images, the best result was reported by Takahashi et al.\"],\n",
       "   [\"For instance, more recently, during PAN 2018 [4], multiple techniques were explored to predict user's gender on twitter posts including images, the best result was reported by Takahashi et al.\"],\n",
       "   ['This traditional techniques try to analyze the semantics and relationship between words [4,7] using techniques like word2vec, bag of words or TF-IDF [8].'],\n",
       "   ['in [4], the CNN with 16 layers, VGG, performs well for age or gender prediction on social network images.'],\n",
       "   ['The use of the weights of a pre-trained network to initialize a new CNN is a technique that has reported good results for age-gender prediction The use of the weights of a pre-trained network to initialize a new CNN is a technique that has reported good results for age-gender prediction [4,5].']],\n",
       "  [['The latter can be confirmed by reviewing the results from the PAN1 competitions (Rangel et al., 2018), where the best-performing systems employed content-based features for representing documents regardless of their genre.']],\n",
       "  [['However, few approaches and little labeled data exists for this task for languages other than English, with some exceptions [16,32,62].']],\n",
       "  [['As part of the PAN at CLEF initiative [1] there have been multiple author profiling challenges in the past years, which generated a substantial body of research in this field, summarized in [18], [20] and [19].']],\n",
       "  [['Similar to our research, some studies have developed classifiers to identify the gender or age of users in text data [55] and in social media such as Twitter [56,57], Sina Weibo [58], Facebook [59], and Netlog [60].']],\n",
       "  [[\"Rangel and colleagues Rangel and colleagues [11] point out that due to the huge amount of information available on social networking platforms, it is possible to obtain information about different attributes such as gender, age, personality, native language, or political orientation from the analysis of an author's profile.\"],\n",
       "   ['Theoretical and empirical studies have demonstrated a strong relationship between social factors and linguistic attitudes, since language is perceived as a social activity that reflects and influences social reality Theoretical and empirical studies have demonstrated a strong relationship between social factors and linguistic attitudes, since language is perceived as a social activity that reflects and influences social reality [11,27].'],\n",
       "   ['However, at present time, researchers mainly focus on digital social networks, where language is more spontaneous and less formal [11].'],\n",
       "   ['Consequently, social activities represent a great challenge for the selection and identification of the user profile, which is caused mainly by the diversity of texts and complex social structures Consequently, social activities represent a great challenge for the selection and identification of the user profile, which is caused mainly by the diversity of texts and complex social structures [11,29,30].']],\n",
       "  [['PAN2013 [2,3] ten itibaren bu g√∂revler i√ßerisine giren cinsiyet tespiti bug√ºn bile √∂nemini korumaktadƒ±r.'],\n",
       "   ['Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.'],\n",
       "   ['Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.']],\n",
       "  [['This research line is very well summarized by Ikae and Savoy in [13] and has as corner stone the author profiling task at PAM-CLEF competitions [14,15,16,17,18,19,20].']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [['The PAN 2015, 2016, 2017 and 2018 datasets The PAN 2015, 2016, 2017 and 2018 datasets [19,20] consisting of tweets in different languages.']],\n",
       "  [['Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions [11,18,19], require manual vetting, and often need to be reconstructed from pointers to proprietary APIs (e.']]],\n",
       " [[['In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection [9], and Style Change Detection [7].'],\n",
       "   ['Concerning the style change detection sub-task, we chose as a baseline model, the method of Zlatkova Concerning the style change detection sub-task, we chose as a baseline model, the method of Zlatkova [2] that is state-of-theart on PAN 2018 Competition [7].']],\n",
       "  [[\"For the next edition, we shall continue working with 'fanfiction' For the next edition, we shall continue working with 'fanfiction' [10,11].\"],\n",
       "   ['However, this task was deemed as highly complex and hence, was relaxed in 2018, asking participants to predict whether a given document is single-or multi-authored [11].']],\n",
       "  [['2, n-grams models are still the most widely used state-of-art methods used in authorship attribution and provide overall good and stable results, for example, in the 2018 PAN-challenge [47].']],\n",
       "  [['In a 2018 authorship attribution competition [12], \"n-grams were the most popular type of features to represent texts in\" one of the primary tasks in the competition.']],\n",
       "  [['Here, we describe our approach, which powered the winning system Here, we describe our approach, which powered the winning system [20] for the PAN@CLEF 2018 task on Style Change Detection [7].'],\n",
       "   ['We used data provided by the organizers of the CLEF-2018 PAN task on Style Change DetectionWe used data provided by the organizers of the CLEF-2018 PAN task on Style Change Detection5  [7], which was based on user posts from StackExchange covering different topics with 300-1,000 tokens per document.']],\n",
       "  [['For NLP researchers, fanfiction provides a large source of literary text with metadata, and has already been used in applications such as authorship attribution (Kestemont et al., 2018) and character relationship classification (Kim and Klinger, 2019).'],\n",
       "   ['Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and G√≥mez-Rodr√≠guez, 2019), finegrained entity typing (Chu et al.']],\n",
       "  [['At the other extreme, the high performance of applying machine learning on authorship verification and attribution (Kestemont et al., 2018(Kestemont et al.'],\n",
       "   ['Despite the overwhelming success of deep learning, traditional features are still effective in authorship analysis (Kestemont et al., 2018(Kestemont et al.']],\n",
       "  [[', 2015;Kestemont et al., 2018).'],\n",
       "   ['The PAN18-FF dataset The PAN18-FF dataset (Kestemont et al., 2018) consists of fan fiction prose texts written by admirers of authors, novels, TV shows, movies, etc.']],\n",
       "  [['75 -the second-best score reported for this part of the challenge; see (Kestemont et al., 2018).']],\n",
       "  [['Up until the very recent PAN 2018 and PAN 2020 Authorship event [3,4], the most popular and effective approaches still largely relies on n-gram features and traditional machine learning classifiers, such as support vector machines (SVM) [5] and trees [6].']],\n",
       "  [['As described by Kestemont et al. (2018), the shared task of cross-fandom AA presents a collection of fan fiction writings from different domains and aims to detect authors for unseen documents.']],\n",
       "  [['‚Ä¢ In the 2018 PAN authorship attribution task [14], features continued to focus on characters and word n-grams, with various weighting and normalization methods.']],\n",
       "  [['2020;Kestemont et al. 2018).'],\n",
       "   ['Another is to build statistical models that can predict and compensate for the issues arising from the mismatches (Daum√© 2009;Daum√© and Marcu 2006;Kestemont et al. 2018).']],\n",
       "  [['Various methods have been applied to the task, ranging from SVM based approaches, such as (Kestemont et al., 2018), to transformer based models, like (Bauersfeld et al.'],\n",
       "   ['The third and final dataset, The third and final dataset, PAN-2018(Kestemont et al., 2018), contains medium-length texts of around 800 words each, centered on fan fiction.'],\n",
       "   [', 2022) and (Kestemont et al., 2018).'],\n",
       "   [', 2022) and (Kestemont et al., 2018).'],\n",
       "   [', 2022) and (Kestemont et al., 2018).']]],\n",
       " [[['It is shown that this automatically identifying the intended sense of ambiguous words improves the performance of clinical and biomedical applications such as medical coding and indexing, 12,13 detection of adverse drug event, 14 automatic medical reporting, 15,16 and other secondary uses of data such as information retrieval and extraction, 17 and question-answering systems.']],\n",
       "  [['The program performances were evaluated on a shared task for disease detection using death certificates [17], and the program was described in-detail at this occasion [18].']],\n",
       "  [[', 2017(N√©v√©ol et al., , 2018)).']],\n",
       "  [['Recently [3,4,5], the C√©piDC task consisted in extracting ICD-10 codes from death reports in several languages (French in 2016, French and English in 2017, French, Hungarian and Italian in 2018).'],\n",
       "   ['Best CLEF eHealth 2018 results on the raw and aligned French dataset [5] the coding decisions.']],\n",
       "  [[', 2017(N√©v√©ol et al., , 2018) ) are good candidates for such analysis.']]],\n",
       " [[['Several existing studies have shown that BM25 also provides solid baseline performance in screening prioritization [3,104] Seed-driven Document Ranking (SDR) is a ranking model specialized for screening prioritization.'],\n",
       "   ['For details of the creation of CLEF18 dataset, we refer to[3,104].']],\n",
       "  [['Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant [10][11][12][13].']],\n",
       "  [[\"Existing work on relevance assessment is mostly related to TREC and CLEF evaluation challenges through the means of constructing test collections and reporting annotators' experience [19][20][21].\"]],\n",
       "  [['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].'],\n",
       "   ['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].']],\n",
       "  [['-We validate the effectiveness of the proposed framework and provide a detailed analysis of its components on various datasets including the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the Text Retrieval Conference (TREC) Total Recall datasets [16], and the TREC Legal datasets [13].'],\n",
       "   ['Three datasets have been released-namely, EMED 2017 Three datasets have been released-namely, EMED 2017 [20], EMED 2018 [21,34], and EMED 2019 [22].'],\n",
       "   ['[9] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [20,21,22].'],\n",
       "   ['To examine the effectiveness of the proposed framework, we compared it against the Knee, Target, SCAL, SD-training, and SD-sampling methods and provided detailed analysis on various datasets including the CLEF Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the TREC Total Recall datasets [16], and the TREC Legal datasets [13].']],\n",
       "  [['[1,2,6].'],\n",
       "   ['This approach was demonstrated to be robust by evaluating it using rankings of varying effectiveness produced by participants from the CLEF eHealth task ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].'],\n",
       "   ['The approach was demonstrated to be robust on a set of rankings of varying effectiveness produced by participants in a shared task [6].'],\n",
       "   ['Following Following [13], we utilise the publicly available submissions to the CLEF 2017 e-Health Lab Task 2 ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].']],\n",
       "  [['HRR tasks include electronic discovery in the law (eDiscovery) [3], systematic review in medicine [22][23][24]47], document sensitivity review [34], online content moderation [55], and corpus annotation to support research and development [60].']],\n",
       "  [['loss er is introduced for the first time in [37] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [82,84,86].']],\n",
       "  [['Other applications include open government document requests Other applications include open government document requests [3] and systematic review in medicine [14][15][16]32].']],\n",
       "  [['Unfortunately, it is not clear these measures were actually used: neither track overview discusses results on them [24,25].'],\n",
       "   ['A similar asymmetry can occur in systematic review in medicine [24,25].']],\n",
       "  [['TAR evaluation conferences TAR evaluation conferences [17,[23][24][25]38] have emphasized interventional stopping rules, i.']],\n",
       "  [['We use topics from the CLEF TAR task from 2017, 2018, and 2019 We use topics from the CLEF TAR task from 2017, 2018, and 2019 [11][12][13].']],\n",
       "  [['In addition, several recent challenges, such as TREC [17,24] and CLEF eHealth task 2 [25][26][27], further promote the development of automatic document screening.'],\n",
       "   ['It has been shown that this active learning solution outperforms its counterparts in many real-world cases [17,[24][25][26][27].']],\n",
       "  [['We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].'],\n",
       "   ['We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].']],\n",
       "  [['Methods to automate such review processes were evaluated in the scope of the Conference and Labs of Evaluation Forum (CLEF) with the so-called eHealth challenges regarding Technology Assisted Reviews (TAR) for systematic reviews (SR) in Empirical Medicine Methods to automate such review processes were evaluated in the scope of the Conference and Labs of Evaluation Forum (CLEF) with the so-called eHealth challenges regarding Technology Assisted Reviews (TAR) for systematic reviews (SR) in Empirical Medicine [10], in which relevant documents must be automatically retrieved for a given topic.'],\n",
       "   ['For the external validation of the two implemented search methods to retrieve publications relevant to a given medical topic, the established CLEF 2018 eHealth TAR dataset for the \"Subtask 1: No Boolean Search\" was used [10].']],\n",
       "  [['We explore initial experiments on the CLEF TAR 2019 dataset [16].'],\n",
       "   ['The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings [16,27,34], often sourced at the title and abstract screening level, rather than at the full-text level.'],\n",
       "   ['Cost-based and economic-based metrics are also used, especially in the context of the query formulation task in the CLEF TAR shared task [14][15][16], e.'],\n",
       "   ['Traditionally, retrieval was conducted at the level of publications [14][15][16].'],\n",
       "   ['We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets [16].'],\n",
       "   ['However, there are several other types of systematic reviews, such as diagnostic test accuracy reviews, prognostic reviews, and qualitative research reviews, each of which presents unique challenges for automation and evaluation [16].']],\n",
       "  [['The TREC Legal [1,6,18,20], TREC Total Recall [10], and the CLEF eHealth Technology-Assisted Review Tasks [11,12] have provided researchers with access to datasets and standardised evaluation methods.']],\n",
       "  [['Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets [28][29][30] and systematic review collection with seed studies (Seed Collection) [79].']],\n",
       "  [['These methods are evaluated and compared against alternative approaches on a range of datasets used to evaluate TAR approaches: the CLEF Technology-assisted Review in Empirical Medicine [30][31][32], the TREC Total Recall tasks [24], and the TREC Legal Tasks [17].'],\n",
       "   ['Its development was informed by experience from TREC Total Recall tracks [24,49] and also adopted by the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32].'],\n",
       "   ['We follow the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32] and Li and Kanoulas [39] in choosing a = 1 and b = 100.']],\n",
       "  [['We also include in our experiments three corpora from the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets from the years 2017, 2018, and 2019 [24][25][26].']],\n",
       "  [['Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews [13,[16][17][18], satisfying legal disclosure requirements [2,12,23], social media content moderation [38] and test collection development [22].'],\n",
       "   ['CLEF 2017/2018/2019 [16][17][18]: Collections of systematic reviews produced for the Conference and Labs of the Evaluation Forum (CLEF) 2017, 2018, and 2019 e-Health lab Task 2: Technology-Assisted Reviews in Empirical Medicine.']],\n",
       "  [['We rely on the CLEF-TAR 2017, 2018 and 2019 Subtask 2 datasets [22][23][24] (abbreviated as .']]],\n",
       " [[['Search scenarios were selected from the CLEF 2018 collection Search scenarios were selected from the CLEF 2018 collection [12], a collection used for evaluating search engines tailored to consumer health search.']],\n",
       "  [['This collection consists of over 5 million medical webpages from selected domains acquired from the CommonCrawl [7].'],\n",
       "   ['These spoken queries are generated by 6 individuals using the information needs derived for the 2018 challenge [7].']],\n",
       "  [['Finally, we also report on a generalisation effort made to evaluate our predictive technology over new datasets [2,3].'],\n",
       "   ['A final section is also provided in which our experiments are extended and applied to two new datasets [2,3] for the sake of achieving generalisation.'],\n",
       "   ['As can be seen, there are several concepts intimately related such as reliabilty [1], trustworthiness [2], credibility [3], or veracity [22].'],\n",
       "   ['[3] and the CLEF eHealth consumer health search task 2018 [2] were used to further evaluate this classification tech-  nology.']],\n",
       "  [['eu, accessed on 15 February 2021) has promoted the eHealth track since 2013 and, the CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of building search engines that are robust to query variations to support information needs of health consumers.'],\n",
       "   ['The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition.'],\n",
       "   ['The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition.']],\n",
       "  [[', 2015;Jimmy et al., 2018;Cross et al.']]],\n",
       " [[['This despite its heavy focus on gathering, organizing, and delivering a relevant social data related to events  Cross language/argumentative mining Cossu et al. (2018) generating a large number of micro-blog posts and web documents (such as, and in particular, cultural festivals).'],\n",
       "   ['The cross language task The cross language task (Cossu et al. 2018) was specific to movies.'],\n",
       "   ['The argumentation mining task The argumentation mining task (Cossu et al. 2018) aimed to automatically identify reason-conclusion structures from text, which can model the position, stance or attitude (as expressed via Twitter microblogs) of a social web user about a cultural event.']]],\n",
       " [[['While we note significant efforts being made through various vehicles, such as NTCIR [10] and ImageCLEF [4], to support off-line adhoc search tasks, by the release of a first generation of lifelog test collection, until now, there was no dedicated benchmarking effort for interactive lifelog search, nor is there a test collection designed to support such benchmarking.'],\n",
       "   ['Based on the collections from NTCIR-12 and NTCIR-13, rigorous comparative benchmarking initiatives have been organised: the NTCIR 12 -Lifelog [9], and ImageCLEFlifelog2017 [3] exploited the NTCIR-12 collection and NTCIR-13 Lifelog 2 [10], ImageCLEFlifelog2018 [4] were proposed based on the NTCIR-13 collection.']],\n",
       "  [['This sub-task follows the success of the LMRT sub-task in ImageCLEFlifelog 2018 [7] with some minor adjustments.']],\n",
       "  [['People usually want to keep footage of the events that happen around them for many purposes such as reminiscence People usually want to keep footage of the events that happen around them for many purposes such as reminiscence [1], retrieval [2] or verification [3].']],\n",
       "  [['The increasing uptake of lifelogging as a personal and practitioner technology have also lead to related activities in ImageCLEF [2], the Lifelog Search Challenge [11] and a related task at MediaEval 2019.']],\n",
       "  [['While there have been some initial efforts, such as the lifelogging challenge at NT-CIR [16], or the lifelogging tasks at ImageCLEF [7] and MediaEval, even these can be considered to be pseudo-personal data challenges, since the test collections involved (lifelogs) are typically small in nature (from a few individuals).'],\n",
       "   ['7 https://every-sense.']],\n",
       "  [['2017aNguyen et al. , 2018) ) which focused on a series of image-retrieval and summarisation focused benchmarking initiatives since 2017, and the Lifelog Search Challenge (LSC) (Gurrin et al.']],\n",
       "  [['In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].']],\n",
       "  [['Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges [82][83][84]; Lifelog Search Challenge [85][86][87], which aims to encourage the development of efficient interactive lifelog retrieval systems; and NTCIR Lifelog Tasks [77].']],\n",
       "  [['Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].']],\n",
       "  [['The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [23] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [24][25][26]55] and the ImageCLEF Lifelog tasks [12][13][14]46].']],\n",
       "  [['To benchmark for different search engines in indexing and retrieving multimodal lifelog data, there have been a number of interactive lifelog retrieval challenges such as NTCIR Lifelog [7,8], ImageCLEF Lifelog [4,5,20], and Lifelog Search Challenge (LSC) [9,10].']],\n",
       "  [['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['In case the user opens the moment-detailed box (3) for further browsing, the user can use the temporal browsing panel (5) to view the previous/after moments of the selected one by horizontal scrolling the panel as well as adjusting the time delta to view the temporal nearby or further apart moments.']]],\n",
       " [[['The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].'],\n",
       "   ['The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].']],\n",
       "  [[', 2017) of IMAGE-CLEF (de Herrera et al., 2018).'],\n",
       "   ['8M images based on their type and to extract the non-compound clinical ones, leading to 232,305 images along with their respective captions (de Herrera et al., 2018).'],\n",
       "   [', 2017;de Herrera et al., 2018) and stopped in 2019.']],\n",
       "  [['Out of the four publicly available ones, PEIR Gross1 and ImageCLEF [20] suffer from severe shortcomings [55], which are also discussed briefly below.'],\n",
       "   ['In previous work [55], we reported that three publicly available datasets can be used for DC research, namely PEIR Gross, ICLEFcaption [20], and IU X-Ray [21].'],\n",
       "   ['The average of the four variants was used as the official measure in ICLEFcaption [20,24].'],\n",
       "   ['Retrieval-based systems were also the top performing submissions of the ImageCLEF Caption Prediction subtask, a task that ran for two consecutive years [20,24].']],\n",
       "  [['It is promising in various applications such as human-computer interaction [8], [9], information retrieval [10], and medical image understanding [11].'],\n",
       "   ['Our DCR method reweights the RL reward for each ground truth captions as (10), and also directly uses CIDErBtw as a part of the final reward as (11).']],\n",
       "  [['In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.']]],\n",
       " [[['For the computer assisted analysis, it was noted that no solution has sufficient prediction accuracy (10)(11)(12)(13)(14).']],\n",
       "  [['ImageCLEFtuberculosis task (Cid et al., 2018) is one such example which was published to build models for detection, classification, and severity measurement of TB from the provided chest-CT.']],\n",
       "  [['The usual classification of TB is infiltrative, focal, tuberculoma, miliary, and fibrocavitory [3].']],\n",
       "  [['Traditional computer-aided diagnosis technology is usually aimed at one disease; for example, for judging the probability of lung cancer based on the presence of pulmonary nodules in Computed Tomography(CT) images of the chest [10], for detecting tuberculosis and classifying its severity [11], or for detecting breast cancer based on chest radiographs [12].']]],\n",
       " [[['The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].']],\n",
       "  [['The first attempt for building the dataset for medical VQA is by ImageCLEF-Med [6].'],\n",
       "   ['The first attempt for building the dataset for medical VQA is by ImageCLEF-Med [6].'],\n",
       "   ['The 2018 ImageCLEF-Med challenge [6] provides a good overview about the approaches and their results.']],\n",
       "  [['‚Ä¢ Word-based Semantic Similarity (WBBS) ‚Ä¢ Word-based Semantic Similarity (WBBS) (Hasan et al., 2018) We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al.'],\n",
       "   ['‚Ä¢ Word-based Semantic Similarity (WBBS) ‚Ä¢ Word-based Semantic Similarity (WBBS) (Hasan et al., 2018) We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al.'],\n",
       "   ['We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al., 2018) to evaluate the performance of the system.']],\n",
       "  [['When the ImageCLEF2018 competition [7] proposed a VQA-Med task in 2018, VQA was applied to the medical field for the first time.'],\n",
       "   ['between CBSS and WBSS is that CBSS uses Meta-Map between CBSS and WBSS is that CBSS uses Meta-Map [48] to extract biomedical concepts from answers through the pymetamap wrapper [7], the dictionaries of the concepts and predicted answers are established, and the semantic similarity between them is calculated by cosine similarity, which is similar to the principle of similarity calculation mentioned in Eq.']],\n",
       "  [['Due to these advantages, VQA models for medical applications have also been proposed [6,8,12,13,24,28], whereby allowing clinicians to probe the model with subtle differentiating questions and contributing to build trust in predictions.']],\n",
       "  [[', 2015;Hasan et al., 2018;Lau et al.']],\n",
       "  [['The mainstream VQA datasets in the medical domain include VQA-MED-2018 [40], VQA-MED-2019 [6], and VQA-MED-2020 [7], which were proposed by the challenge tasks.']],\n",
       "  [['VQA-MED-2018 [9], VQA-RAD [14], VQAMED-2019 [4], RadVisDial [13], PathVQA [11], VQA-MED-2020 [3], SLAKE [18], and VQA-MED-2021 [5].']],\n",
       "  [['The VQA-MED-2018 dataset was the first of its kind created specifically for visual QA (VQA) using medical images The VQA-MED-2018 dataset was the first of its kind created specifically for visual QA (VQA) using medical images [276].']]],\n",
       " [[['Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models [7][8] [9].']]],\n",
       " [[['In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.']],\n",
       "  [['For social media datasets, we use two public datasets, the eRisk-2018 For social media datasets, we use two public datasets, the eRisk-2018 (20) and Clpsych-2015 datasets (21).'],\n",
       "   ['We divide data into a, b, c, and d four age groups, indicating early adolescence (10)(11)(12)(13)(14)(15), late adolescence (16)(17)(18)(19)(20)(21)(22), adolescence (22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35), early middle-aged (22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35), and middleaged (35-).']]],\n",
       " [[[', check-worthiness estimation has been severely understudied as a problem [1,5,7].']],\n",
       "  [['Task 1: Check-Worthiness [ Barr√≥n-Cede√±o et al., 2018] asks to predict if a given claim from a political debate should be prioritized for fact checking.']],\n",
       "  [[', 2018;Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al.']],\n",
       "  [['lab featured a shared task on fact-checking a claim in the context of a political debate (Barr√≥n-Cede√±o et al., 2018;Nakov et al.']],\n",
       "  [['Recently, a great number of the studies proposing approaches on fake news detection are based on deep learning methods (Barr on-Cedeno et al., 2018;Popat et al.']],\n",
       "  [[', , 2019) ) and verification (Barr√≥n-Cede√±o et al., 2018;Hasanain et al.']],\n",
       "  [['Till now, most researchers have tried to handle this issue by checking if a post is worth-checking [6] to reduce the number of claims.'],\n",
       "   ['[6] has hosted since 2018 an open detection task of checkworthy claims.']],\n",
       "  [['Earlier works in identifying check-worthy claims were at the granularity of entire sentences Earlier works in identifying check-worthy claims were at the granularity of entire sentences [5,26,54,81].'],\n",
       "   ['[5].'],\n",
       "   ['[5]), sentences in medical newswire are long and complex, often positioning the primary claim(s) within a larger context of other information [96], we obtain 6,000 news articles from the \"Health\" category of Google News during April 2018 and augment this with the top 25 RSS feeds in the \"Health and Healthy Living\" category5 from November 2018 through April 2019 to get over 34,000 news articles.'],\n",
       "   ['Predicting the score of a claim-sentence pair is formulated as regression learning with target set Predicting the score of a claim-sentence pair is formulated as regression learning with target set [1,5] aligned with the Likert-type scale.']],\n",
       "  [[', 2017;Atanasova et al., 2018) are equipped.']],\n",
       "  [['Recently, Several works are carried out to detection of offensive language (Kalaivani and Thenmozhi, 2020a), hate speech (Kalaivani and Thenmozhi, 2020b), fake news detection, trustworthiness (Atanasova et al., 2018) and fact-checking (Elsayed et al.']],\n",
       "  [[', 2015), and trust-worthiness prediction (Barr√≥n-Cede√±o et al., 2018).']],\n",
       "  [[', 2018;Atanasova et al., 2019;Barr√≥n-Cedeno et al.']],\n",
       "  [[', 2018;Barr√≥n-Cede√±o et al., 2018;Atanasova et al.']],\n",
       "  [['2019 were obtained using Long Short-Term Memory (LSTM) networks [7].'],\n",
       "   ['Feature representation methods, including word embeddings, Bag of Words, Named Entity Recognition, and Part of Speech tagging [6,7,9] have been widely used to enhance model understanding of the task.']]],\n",
       " [[['In the recent CLEF 2018 competition on check-worthiness detection [17], Zou et al.'],\n",
       "   ['Out of these 7 speeches, 4 are by Donald Trump and are made available by the CLEF 2018 lab on automatic identification and verification of political claims [17].'],\n",
       "   ['2016 presidential election (following the same setting as in the official CLEF 2018 competition on check-worthiness detection [17] but using even more data).']],\n",
       "  [['20194 is a continuation of the evaluation lab at CLEF-2018 [12].']],\n",
       "  [['(Barr√≥n-Cede√±o et al., 2018) FactCheck.']],\n",
       "  [['Lab Task 2 [2,32] contains 94 claims from three debates as a training set, and 192 claims from seven debates and speeches as a test set.']],\n",
       "  [[\"As outlined in Atanasova et al. (2018), of a total of seven models compared, the most successful approaches used by the participants relied on recurrent and multi-layer neural networks, as well as combinations of distributional representations, matching claims' vocabulary against lexicons, and measures of syntactic dependency.\"]],\n",
       "  [['lab on identification and verification of claims (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [[\"labs' shared tasks (Nakov et al., 2018;Elsayed et al.\"]],\n",
       "  [[', 2015); trust-worthiness prediction and fact-checking (Atanasova et al., 2018;Atanasova et al.']],\n",
       "  [['These datasets are similar to ours in order of magnitude, however, use a different definition of claim, as is the case with others tackling the determination of check-worthiness of claims [4,17,31,47].']],\n",
       "  [['lab featured a shared task on fact-checking a claim in the context of a political debate (Barr√≥n-Cede√±o et al., 2018;Nakov et al.']],\n",
       "  [['Recently, a great number of the studies proposing approaches on fake news detection are based on deep learning methods (Barr on-Cedeno et al., 2018;Popat et al.']],\n",
       "  [['lab (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [[', 2015), and trust-worthiness prediction (Barr√≥n-Cede√±o et al., 2018).']],\n",
       "  [[', 2020;Atanasova et al., 2018;Barron-Cedeno et al.']],\n",
       "  [[', 2018;Barr√≥n-Cede√±o et al., 2018;Atanasova et al.']]],\n",
       " [],\n",
       " [[['In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations [48].']],\n",
       "  [['868 [36].']],\n",
       "  [['Since understanding temporality regarding clinical events conveyed in the narrative text is a crucial prerequisite for the utilization of the narratives, automatic means to identify temporal information from clinical narratives have gained much attention from the community [1][2][3][4][5][6][7][8][9][10].'],\n",
       "   [', the 2012 Informatics for Integrating Biology and the Bedside (i2b2) challenge [14], the 2013/ 2014 CLEF/ShARe challenges [4], and the 2015/2016/ 2017 Clinical TempEval challenges [5][6][7]).']],\n",
       "  [['The 2015 Analysis of Clinical Text (ACT) shared task The 2015 Analysis of Clinical Text (ACT) shared task [39] utilized the ShaRe dataset [40] consisting of 531 manually annotated discharge summaries, electrocardiograms, echo, and radiology reports from MIMIC-II.']],\n",
       "  [['The * SEM 2012 Shared Task (Morante and Blanco 2012) was devoted to processing the scope and focus of negation, and in the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al. 2014) participants had to detect whether a disorder was negated.']],\n",
       "  [['In this University of Pennsylvania Institute Review Board (IRB)-approved pilot study, we leveraged the pre-annotated 2014 ShARe/CLEF eHealth Challenge corpus In this University of Pennsylvania Institute Review Board (IRB)-approved pilot study, we leveraged the pre-annotated 2014 ShARe/CLEF eHealth Challenge corpus [21], a subset of the Medical Information Mart for Intensive Care (MIMIC)-II database [22] collected from the intensive care units of Beth Israel Deaconess Medical Center.'],\n",
       "   ['To align our annotated classes with current and well-adopted annotation efforts in the NLP community, we added new and expanded existing annotation classes to the ShARe [21], TimeML [14], and CALEX [17] schemas.']],\n",
       "  [['ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 [43] focused on facilitating understanding of information in clinical reports by extracting several attributes such as negation, uncertainty, subjects, severity, etc.']],\n",
       "  [['Finally, we used an open-source data set for part of our validation from the ShARe/CLEF 2014 challenge [22,23] We used two datasets for validation.'],\n",
       "   ['To evaluate the performance of our tool, we have used the ShAReCLEF dataset from task 2 of the 2014 challenge To evaluate the performance of our tool, we have used the ShAReCLEF dataset from task 2 of the 2014 challenge [22,23].']],\n",
       "  [['Content Generation Content Generation [124] generating new medical descriptions or knowledge based on a given input  [127] 2012 ‚àº4K English RE Link ShARe13 [128] 2013 ‚àº29K English NER Link GENIA13 [129] 2013 ‚àº5K English EE Link NCBI [21] 2014 ‚àº7K English NER Link ShARe14 [130] 2014 ‚àº35K English NER Link CADEC [20] 2015 ‚àº7.']],\n",
       "  [[', 2020) and CLEF 2014 (Mowery et al., 2014) datasets in this paper.']]],\n",
       " [[['The ShARe/CLEF eHealth 2014 shared task [8] focused on facilitating understanding of information in narrative clinical reports, such as discharge summaries, by visualizing and interactively searching previous eHealth data (Task 1) [9], identifying and normalizing disorder attributes (Task 2), and retrieving documents from the health and medicine websites for addressing questions monoand multi-lingual patients may have about the disease/disorders in the clinical notes (Task 3) [10].']],\n",
       "  [['boosting easy to read documents for laypeople [49,40,11], or search aids, such as query suggestions to match the searcher expertise.']],\n",
       "  [[\"In the framework of CLEF eHealth 2014 [12], one of the proposed shared tasks (task 3) addresses this challenge [8]: queries have been defined from real patient cases issued from the clinical documents provided by the CLEF eHealth 2014's task 2.\"]],\n",
       "  [[', 2013 ;Goeuriot et al., 2014), ce qui garantit que la recherche est faite dans un espace de fiabilit√© et fournit des informations de confiance.'],\n",
       "   [', 2013 ;Goeuriot et al., 2014) semblent √™tre assez uniques pour ce domaine de recherche.']],\n",
       "  [[\"Our experimental framework is the CLEF eHealth 2014's task 2 [10], for which queries are defined from real patient cases issued from clinical documents within the KRESMOI project [11].\"],\n",
       "   ['0560 [10].'],\n",
       "   ['This fact is also acknowledged for the average results of all the participants [10].']],\n",
       "  [[\"The patient uses 'layman' query terms, while the discharge summary contains an expert description of the diagnosis (Goeuriot et al. 2014;Kelly et al.\"]],\n",
       "  [['The use of the Web as source of health-related information is a wide-spread practice among health consumers The use of the Web as source of health-related information is a wide-spread practice among health consumers [19] and search engines are commonly used as a means to access health information available online [7].']],\n",
       "  [['Evaluation campaigns and resources in this domain are presented, including TREC Medical Records Track [43,45,46], TREC Clinical Decision Support Track [36][37][38], CLEF eHealth (consumer health search [13,14,35,55] and as of 2017 search systems for the compilation of systematic reviews), i2b2 Shared Task Challenges¬≥, ALTA Shared Task ¬≥https://www.']],\n",
       "  [['While organizers published overview papers in 2013 [14] and 2014 [18], no deeper analysis of these results has so far been reported.'],\n",
       "   ['More detailed descriptions are available in the 2013 and 2014 task overview papers in the CLEF proceedings [14,18].'],\n",
       "   ['11 Further details on baselines used are provided in the Task overview papers [14,18].']],\n",
       "  [['The CLEF 2015 collection contains 50 queries and 1437 documents that have been assessed as relevant by clinical experts and have an assessment for understandability The CLEF 2015 collection contains 50 queries and 1437 documents that have been assessed as relevant by clinical experts and have an assessment for understandability [44].'],\n",
       "   ['We used the thresholds U=2 for CLEF 2015 and U=40 for CLEF 2016, based on the distribution of understandability assessments and the semantic of understandability labels [44,14].'],\n",
       "   ['Relevance assessments on the CLEF 2015 and 2016 collections are incomplete Relevance assessments on the CLEF 2015 and 2016 collections are incomplete [44,14], that is, not all top ranked Web pages retrieved by the investigated methods have an explicit relevance assessment.']],\n",
       "  [[', 2013;Goeuriot et al., 2014) but may also be provided from works of researchers, such as POS-tag (Tsuruoka et al.']],\n",
       "  [['With the spreading awareness on the exploration of information extracted from medical discharge documents and clinical reports by laymen patients, searching online health related web-forums and other sources for health advice has become a common habit [17,18,19].'],\n",
       "   ['In addition, the present information retrieval systems pays more attention on a specific group of people with expert health knowledge [18,20,22].'],\n",
       "   ['However, while addressing users diverse information needs, such as searching information on a specific disease, preceding researches targeted only a specific group of users with expert health knowledge [17,18,20].'],\n",
       "   ['Previous researches on medical information retrieval also disclosed how desperate patients are in apprehending the content of their medical discharge reports and clinical reports [14,17,18].'],\n",
       "   ['In the medical domain, querying of the Internet for useful information has become increasingly important, owing to the huge amount of information available [17,18].']],\n",
       "  [['In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 [6], 2010 [7], 2012 [8] and 2014 [9], the NLP challenges organized by SemEval in 2014 [10], 2015 [11] and 2016 [12], and the NLP challenges organized by ShARe/CLEF in 2013 [13] and 2014 [14].']],\n",
       "  [['These are often built for the purposes of shared tasks [26,27].']],\n",
       "  [['We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30].']]],\n",
       " [],\n",
       " [[['The first set is a set of 250K images [13], collected by querying web image search engines.']],\n",
       "  [['In spite of sustained efforts and important progress over the last decade In spite of sustained efforts and important progress over the last decade [341,342], a number of important challenges still have to be addressed before including automatic annotation in Web retrieval pipelines.']],\n",
       "  [['Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.']]],\n",
       " [[['[28] used Case Retrieval in Radiology (CaReRa) project, which includes Clinical Experience Sharing (CES) concepts, to treat liver cancer.']]],\n",
       " [[[', Office-10+Caltech-10 [14], Office31 [31], ImageCLEF [5] and Digit Recognition.'],\n",
       "   ['ImageCLEF ImageCLEF [5] is developed for the ImageCLEF domain adaptation task 1 .']]],\n",
       " [[['We study this in the context of the INEX Social Book Search Track2  [12,14,15].'],\n",
       "   ['The INEX Social Book Search Track The INEX Social Book Search Track [12,14,15] investigates book search in collections with both professional metadata and social media content.'],\n",
       "   ['For the retrieval system, we use the Amazon/LibraryThing collection [2] that is also used in the INEX Social Book Search Track [15].']],\n",
       "  [['INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time [43].']],\n",
       "  [[') [14].']]],\n",
       " [[[\"[3] experiment with traditional query expansion techniques, exploiting Wikipedia articles as rich sources of information that can augment the user's query.\"]],\n",
       "  [['A concrete example is the user-selectable interface panels as evaluated in G√§de et al. (2016), which include a Browse view, a Search view and a Book-bag view, aiming to support pre-focus, focus formulation and post-focus stages.']],\n",
       "  [['Efforts have been undertaken to transfer this paradigm to less systems-focused research, including the TREC Interactive and Session tracks [12], the INEX Interactive track [14,16], the Interactive Social Book Search track [6,7,11], and the RePAST archive [5].']],\n",
       "  [['The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab [4,5,6], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.']]],\n",
       " [],\n",
       " [],\n",
       " [[['Details of the participants and the methods used in the runs are synthesised in the overview working note of the task [19].'],\n",
       "   ['A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task [19] and further developed in the individual working notes of the participants who submitted one (BME TMIT [22], FINKI [28], I3S [31], IBM AU [26], IV-Processing [30], MIRACL [23], PlantNet [24], QUT [25], Sabanki-Okan [27], SZTE [29]).'],\n",
       "   ['A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task [19] and further developed in the individual working notes of the participants who submitted one (BME TMIT [22], FINKI [28], I3S [31], IBM AU [26], IV-Processing [30], MIRACL [23], PlantNet [24], QUT [25], Sabanki-Okan [27], SZTE [29]).']],\n",
       "  [['The three datasets are the Caltech-UCSD-2011 (CUB200-2011) [22], Birdsnap [3], and PlantCLEF 2015 [14].']],\n",
       "  [['In recent years there has been an increasing interest in the problem of plant species classification in images In recent years there has been an increasing interest in the problem of plant species classification in images [3,7,12,18].']],\n",
       "  [['This paper presents the participation of Inria ZENITH team to the 2015-edition of this challenge [9,19].']],\n",
       "  [['Recently, structural features such as LBP variants have been used [9,10], [15,16].']],\n",
       "  [['We used the dataset released for the plant task of the LifeCLEF 2015 challenge [2].']],\n",
       "  [['Allowing the mass of citizens to produce accurate plant observations requires to equip them with much more accurate identification tools [50].']],\n",
       "  [['Theoretically, it is possible to combine multiple images from the same observation in the recognition process, which could further improve the accuracy, as shown for a different dataset by [33].']],\n",
       "  [['Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014.']],\n",
       "  [['Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10].']],\n",
       "  [['They use Transfer learning to fine-tune the pre-trained models, using LifeCLEF plant dataset (Go√´au et al., 2014) and applied classic data augmentation techniques based on image transforms such as rotation, translation, reflection, and scaling to decrease the chance of overfitting.']],\n",
       "  [['The meta-data [7], [8]consists of ObservationID: the plant observation ID from which several pictures, associated with the organ, File Name, MediaID, ClassID, Species, Family Name etc.']],\n",
       "  [['Recently, deep learning techniques have dominated the PlantCLEF challenge (Go√´au et al., 2014).']]],\n",
       " [[['In this work, 80% of the images were randomly selected as training data, whereas the rest (20%) were used for validation, as in previous studies of deep learning-based computer vision applications [27,28].'],\n",
       "   ['In this work, 80% of the image randomly selected as training data, whereas the rest (20%) were used for validatio previous studies of deep learning-based computer vision applications [27,28].']]],\n",
       " [[[\"An overview of the approaches of last year's participants of NEWSREEL 2014 is provided in [10].\"]]],\n",
       " [],\n",
       " [[['This non-parametric test is common in computational authorship studies, such as the PAN competition (Stamatatos et al., 2014), to compare the output of two authorship attribution systems, where we cannot make assumptions about the (potentially highly complex) underlying distributions.']],\n",
       "  [['in [11], which also incorporates data provided by the PAN evaluation lab [28,29,30].'],\n",
       "   ['The accessable dataset (see The accessable dataset (see [11,28,29,30]) contains about 9300 instances of the form Dknown, dunknown, l , where Dknown is a set of documents from a known author, dunknown defines a document in question and l ‚àà {0, 1} denotes the class label.']]],\n",
       " [[['Wright, Chin 13 trained support vector machine to classify the Five Factor personality.']],\n",
       "  [['We use the following datasets for these experiments: PAN14 (Rangel et al., 2014) 4 -we include the English data from PAN14 for the following domains: BLOGS, hotel REVIEWS, and Social Media (SOME).']],\n",
       "  [[', , 2014;;Rangel et al., 2015).']],\n",
       "  [['is currently receiving a growing interest in the Computational Linguistics community as it is also testified by the first shared task organized in 2013 on Author Profiling at PAN 2013 (Rangel et al., 2013).']]],\n",
       " [[['These expressions and 200 training examples from QALD-4 [11], used as input to the ZC05 algorithm, can be found at http://pub.']],\n",
       "  [[\"Today's artificially intelligent agents are good at answering factual questions about our world Today's artificially intelligent agents are good at answering factual questions about our world [9,15,41].\"],\n",
       "   ['There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].']],\n",
       "  [['Then, we apply our proposed system to Task 2 (Biomedical question answering over interlinked data) of QALD-4 [19] to compare its precision and recall to state-of-the-art question answering systems in the biomedical domain.'],\n",
       "   ['Biomedical question answering over interlinked data) of QALD-4 [19], using the FOL to SPARQL translation.']],\n",
       "  [['Datasets: We manually annotate superlative expressions from QALD-4 evaluation dataset (Unger et al., 2014) and TREC QA (2002QA ( , 2003) ) datasets (NIST, 2003), and guarantee that all the labeled superlative instances can be grounded to gradable Freebase predicates.']],\n",
       "  [['We evaluate HAWK against the QALD We evaluate HAWK against the QALD [21] benchmark.']],\n",
       "  [['The open challenge on QA over Linked Data [16] has many participants.']],\n",
       "  [['The QALD 2 proceedings are included in ILD 2012, QALD 3 [25] and QALD 4 [137]  SQA Surveys For each participant, problems and their solution strategies are given: Athenikos and Han [9] give an overview of domain specific QA systems for biomedicine.']],\n",
       "  [['In question answering systems such as QALD [1,2,3], answer to the queries is found from DBpedia which is updated on a regular basis.']],\n",
       "  [['85 RO FII (mentioned in [34]) semi-manual 0.']],\n",
       "  [['The QALD benchmarking initiative [12], now in its fifth year, puts emphasis on linked open data in RDF format.']],\n",
       "  [['One of the means to improve the access to the Linked Data Web lies in the development of natural-language interfaces that can transform human languages or even controlled languages into a representation suitable for querying large knowledge bases [10].'],\n",
       "   ['One of the means to improve the access to the Linked Data Web lies in the development of natural-language interfaces that can transform human languages or even controlled languages into a representation suitable for querying large knowledge bases [10].']],\n",
       "  [['DBpedia has been succesfully used as the primary knowledge source for Question answering Systems, especially within the Question Answering over Linked Data workshop [20].']],\n",
       "  [['The QALD-6 training set 25 , which is the recent successor of QALD-5 [19], contains 350 questions, including the questions from previous QALD challenges.']],\n",
       "  [['QALD is a series of international competitions on mapping natural language questions to knowledge base queries QALD is a series of international competitions on mapping natural language questions to knowledge base queries [9].'],\n",
       "   ['Multiple successful question answering systems were presented in the previous QALD competitions (see for example the overview in Multiple successful question answering systems were presented in the previous QALD competitions (see for example the overview in [9]).'],\n",
       "   ['In Table In Table 2, we report preliminary evaluation results on the training data set for Task 4 of the QALD-7 Shared Task using the metrics from [9].']],\n",
       "  [['With the increasing number of high quality datasets, the Semantic Web is seeing a renewed interest in estion Answering (QA) over graph data [18].']],\n",
       "  [['[30] and Section 5.'],\n",
       "   ['Throughout our experiment, we employed the Large-Scale Complex Question Answering DatasetThroughout our experiment, we employed the Large-Scale Complex Question Answering Dataset12 (LC-QuAD) [28] as well as the 5 th edition of Question Answering over Linked Data Challenge13 (QALD-5) dataset [30].']],\n",
       "  [['Multiple datasets have been proposed for KB-QA, which differ in the underlying KB (DBpedia or Freebase), size (a couple of hundreds to a few thousands), and question phenomena they involve (simple, compositional, and/or questions with conditions, among others) Multiple datasets have been proposed for KB-QA, which differ in the underlying KB (DBpedia or Freebase), size (a couple of hundreds to a few thousands), and question phenomena they involve (simple, compositional, and/or questions with conditions, among others) [1,5,7,9,10,29,31].'],\n",
       "   ['QALD [29,31] is a series of evaluation campaigns on QA over linked data, and releases datasets every year to evaluate KB-QA systems.']],\n",
       "  [['QALD-5 questions were compiled from the QALD-4 training and test questions, slightly modified in order to account for changes in the DBpedia dataset [107].']],\n",
       "  [['The most popular benchmarks for QA over Knowledge Bases are WebQuestions The most popular benchmarks for QA over Knowledge Bases are WebQuestions [1], SimpleQuestions [3] and QALD 3 [8][5] [10][11] [12].']],\n",
       "  [['The use of CQA for ontology matching opens new perspectives such as ontology matching with natural language to ontology mapping techniques [24] over multiple ontologies.']],\n",
       "  [['In the area of Question Answering using LD, challenges such as BioASQ In the area of Question Answering using LD, challenges such as BioASQ [43], and the Question Answering over Linked Data (QALD) [45] have aimed to provide benchmarks for retrieving answers to human-generated questions.']],\n",
       "  [['In this work, we use the QALD version 2 (QALD-2) data set benchmark from The Test Collection for Entity Search (DBpedia-Entity) [1], and; QALD version 4 (QALD-4) [34].'],\n",
       "   ['QALD-4 has ten queries that follow this criteria, Queries 12,13,21,26,30,32,34,41,42,and 44.']],\n",
       "  [['The field of QA is vast and can be classified along many different dimensions, including (i) knowledge based (Unger et al. 2014) vs.']],\n",
       "  [['[11] refer to nonneural KGQA approaches as \"traditional\" [7,30,38].']],\n",
       "  [['As far as quantities are concerned, lookups are supported by many methods, over both knowledge graphs and text documents, and are part of major benchmarks, such as QALD [36], NaturalQuestions [22], ComplexWebQuestions [35], LC-QuAD [12] and others.']],\n",
       "  [['Bulk of the recent literature on automated QA is focused on either (i) proposing a new kind of dataset Bulk of the recent literature on automated QA is focused on either (i) proposing a new kind of dataset (Unger et al., 2014;Rajpurkar et al.']],\n",
       "  [['Marginean (2014) proposed a different approach using manually generated rules and showed that it performs better than POMELO for biomedical data (QALD-4) (Unger et al., 2014); however, this approach is brittle and does not work for newer or more complex queries.']],\n",
       "  [['In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series [20][21][22][23][24][25][26][27][28], which is a series of benchmarks for evaluating KGQA systems.'],\n",
       "   ['For instance, a question on QALD-5 [24] \"Which anti-apartheid activist was born in Mvezo?']],\n",
       "  [['We used the following benchmarks in particular: QALD-5 We used the following benchmarks in particular: QALD-5 [46]: This benchmark consists of over 340 training questions and focuses on multilingual question answering.']],\n",
       "  [['This kind of system is evaluated in the QALD open challenge [33].']],\n",
       "  [['QALD-4 [28] comprises 50 natural language biomedical questions, involving SPARQL queries from SIDER, Drugbank, and Diseasome domains.']]],\n",
       " [[['We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset [1], which is one of the largest datasets for multi-label text classification, and label hierarchies in the 2015 MeSH vocabulary.']],\n",
       "  [['BioASQ BioASQ [132,1,10,11] is a benchmark challenge which ran until September 2015 and consists of semantic indexing as well as an SQA part on biomedical data.']],\n",
       "  [['This approach has been shown to be highly successful in the recent BioASQ 2 challenge evaluations [44][45][46] and has also been adopted by many others [47,48].'],\n",
       "   ['In 2014, the BioASQ challenge task In 2014, the BioASQ challenge task [45] ran for six consecutive periods (batches) of 5 weeks each.']],\n",
       "  [['In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ [16][17][18], a challenge on large-scale biomedical semantic indexing and question answering, held an international competition from 2013 to 2017.']],\n",
       "  [['The FP7 BIOASQ project aims to push research towards highly precise biomedical information access systems by establishing a series of challenges in which systems from teams around the world compete The FP7 BIOASQ project aims to push research towards highly precise biomedical information access systems by establishing a series of challenges in which systems from teams around the world compete [2].']]],\n",
       " [[['There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].']],\n",
       "  [[', 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014;Shibuki et al.']],\n",
       "  [[', 2014) offers comparative evaluation for solving real-world university entrance exam questions; The dataset of CLEF QA Entrance Exams Task (Rodrigo et al., 2015) is extracted from standardized English examinations for university admission in Japan; ARC dataset (Clark et al.']]],\n",
       " [[[\"Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab [7], SemEval Twitter SA [8], and aspect based SA tasks [9,10].\"]],\n",
       "  [['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter, see PAN [18] and RepLab [19] overviews for more details.'],\n",
       "   ['The RepLab 2014 \"Author Ranking\" task was specifically focused on influence The RepLab 2014 \"Author Ranking\" task was specifically focused on influence [19], as explained in more details in Section III.'],\n",
       "   ['The CLEF RepLab 2014 dataset The CLEF RepLab 2014 dataset [19] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum2 (CLEF).'],\n",
       "   ['The RepLab framework The RepLab framework [19] uses the Mean Average Precision (MAP) to evaluate the estimated rankings.'],\n",
       "   ['Actually, in RepLab 2014 [19], the organizers were not able to conclude on significant differences between certain participants due to the number of considered domains.']],\n",
       "  [['3 These tasks were organized as a CLEF evaluation task [8,9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['Keeping these different dimensions in view, the task of reputation dimensions classification was first introduced within RepLab 2014 [9].'],\n",
       "   ['9 In the month of June 2015.']],\n",
       "  [['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter: PAN [59] and RepLab [3].'],\n",
       "   ['The RepLab Challenge 2014 dataset The RepLab Challenge 2014 dataset [3] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum 1 (CLEF).'],\n",
       "   ['The RepLab framework The RepLab framework [3] uses the traditional Mean Average Precision (MAP) to evaluate the estimated rankings.'],\n",
       "   ['In RepLab 2014 [3], the organizers were not able to conclude on significant differences between participants (and features or methods used) due to the small number of considered domains.']],\n",
       "  [[\"We use the context of RepLab We use the context of RepLab [2,3] tasks to evaluate our proposal that is to say: to propose an overview of 61 entity's (drawn in 4 domains: Automotive, Banking, Music and University) e-Reputation regarding experts taxonomies using provided set and pertaining of Micro-Blogs concerning each entity.\"],\n",
       "   ['It leads us to consider probability re-estimation of a document d in a class c using a smoothing as defined in (3).']],\n",
       "  [['Specifically, we evaluate the models for five applications: (1) predict whether the sentiment of tweet is positive, negative or neutral (SA) [26], (2) predict the entity the tweet belongs to (EI) [27], (3) predict the priority of the topic the tweet belongs to (TP) [ From the results, we find that Paragraph2Vec has poor performance for all the tasks compared to BOW.']],\n",
       "  [['There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [13], event and rumor detection [3,16], topic popularity and summarization [2,23], information diffusion [11], popularity prediction [18], and reputation monitoring [1].'],\n",
       "   [\"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\"]],\n",
       "  [['(1)[293] Other(8) [62,171,179,[197][198][199]214,294] Hotel Reviews(24) [[93][94][95][96][97][98][99][100]102,103,[127][128][129][130][131][132][133][135][136][137][138][139][140][141][142] https://doi.']],\n",
       "  [['In 2014, RepLab focused on two aspects of reputation analysis, which is Reputation Dimensions Classification and Author Profiling (Amigo et al., 2014).']],\n",
       "  [['Finally, the Twitter subcorpus was constructed in cooperation with RepLab [3] in order to address also the reputational perspective (e.']],\n",
       "  [[', , 2013(Amig√≥ et al., , 2014) ) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance.']],\n",
       "  [['There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [24], event and rumor detection [16,28], topic popularity and summarization [2,42], information diffusion [18], popularity prediction [34], and reputation monitoring [1].'],\n",
       "   [\"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\"]],\n",
       "  [['Authorities receive interactions in particular from hubs whereas hubs connect to a lot of authorities [21].']],\n",
       "  [['O gerenciamento da reputa√ß√£o digital √© uma importante an√°lise que serve para medir como √© a reputa√ß√£o de uma empresa em rela√ß√£o a certos grupos de interessados [1].'],\n",
       "   ['Para avaliar experimentalmente a proposta, ela foi experi-mentada usando o conjunto de publica√ß√µes do desafio do RepLab 2014 [1], que consiste de publica√ß√µes no Twitter extra√≠das em 2012 durante o per√≠odo de 1 o de Junho at√© 31 de Dezembro, com cerca de 48 mil tweets rotulados em 8 assuntos.'],\n",
       "   ['[11], vencedores da RepLab2014 [1], prop√µem resolver esta tarefa usando a Wikip√©dia para enriquecer as publica√ß√µes e treinando um classificador SVM (Support Vector Machines) para aprender a classificar as dimens√µes de reputa√ß√£o.'],\n",
       "   ['Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 [1] e as m√©tricas de avalia√ß√£o.']],\n",
       "  [['The main idea is then to predict For selecting relevant users we could make use of existing resources such as analytical platforms, like Socialbakers \\uf0d2 , or existing datasets, like RepLab 2014 [34].']],\n",
       "  [['[70] utilising the RepLab 2014 corpus [71] and Azzouza et al.'],\n",
       "   ['It was noticeable that the majority of studies sought to create their own training data, with only 6 studies using publicly available precollected data sets, and only 4 of these originated from gold-standard data sets produced as part of NLP community challenges like RepLab and SemEval, focused on reputational classification [71] and sentiment [69,63].']],\n",
       "  [['Other studies aimed at specific user profiles such as campaign promoters, bots, influencers, political position and polarity (Amig√≥ et al., 2014;Li et al.'],\n",
       "   ['In the RepLab 2014 edition (Amig√≥ et al., 2014), one of the objectives was author profiling in the automotive and banking domains.'],\n",
       "   ['We also evaluated our results against the RepLab dataset ( We also evaluated our results against the RepLab dataset ( Amig√≥ et al., 2014) with the revision of tags presented in (Nebot et al.'],\n",
       "   ['It must be noticed that the evaluation results are not comparable to those published in It must be noticed that the evaluation results are not comparable to those published in (Amig√≥ et al., 2014) and (Nebot et al.']],\n",
       "  [['Regarding reputation, most work has been focused on identifying the influential users in a specific domain Regarding reputation, most work has been focused on identifying the influential users in a specific domain (Amigo ¬¥et al. 2014).'],\n",
       "   [\"These categories are inspired in the RepLab 2014 dataset and designed according to the experts' criteria involved in the project (Amigo ¬¥et al. 2014).\"],\n",
       "   ['The first one is the RepLab 2014 dataset (Amigo ¬¥et al. 2014), which contains a track for the automotive domain.']],\n",
       "  [['Although significant advances have been made in RepLab 1 [1,2].'],\n",
       "   ['Most of the contributions on reputation monitoring to extract sets of tweets requiring a particular attention from a reputation manager have been proposed in the last editions of RepLab [1,2].'],\n",
       "   [\"RepLab'2014 [2] focused on the reputation dimension classification.\"],\n",
       "   [\"We perform a supervised classification over Replab'2013-14 dataset We perform a supervised classification over Replab'2013-14 dataset [1,2].\"]]],\n",
       " [[['Language technologies hold the potential for making information easier to understand and access [17].']],\n",
       "  [['1) [1].']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].']],\n",
       "  [['The work by [35] focused on evaluating the impacts and designing a patient portal for inpatient that would provide patients with a 24 hours free admittance to particular medical facts with internet connection from anywhere.'],\n",
       "   ['Work by [35] proposed that sufficient consideration should to be accorded to the information requests of various classes of information seekers.']]],\n",
       " [[['The dataset created for the ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task is similar to the previous one The dataset created for the ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task is similar to the previous one [7].']],\n",
       "  [['Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.']]],\n",
       " [],\n",
       " [[['Nevertheless, and understandably, the focus resided on image processing and, so far, the methods used for text similarity for the purpose of image retrieval are fairly mainstream [17].']],\n",
       "  [['In this work we adopt the ImageCLEF 2012 Photo Annotation dataset [44] as our training set.']],\n",
       "  [['We consider three different sources for unpaired text data to train the language model: (1) MSCOCO consists of all captions from the MSCOCO train set (2) Text from Image Description Corpora (Caption-Txt) consists of text data from other paired image and video description datasets: Flickr1M [13], Flickr30k [11], Pascal-1k [25] and ImageCLEF-2012 [32] and sentence descriptions of Youtube clips from the MSVD training corpus.']],\n",
       "  [['Experiments have been carried out using two datasets, namely a subset of the MIRFLICKRExperiments have been carried out using two datasets, namely a subset of the MIRFLICKR1 collection proposed for the ImageCLEF 2012 Photo Flickr Annotation Task [24] and the MICC-Flickr101 dataset [1].']],\n",
       "  [['Triggered through benchmark collections, for image retrieval [37] and benchmarking tasks [33], a large body of works focuses on how to detect objects in images (e.']],\n",
       "  [['However, the focus resided on image processing and, so far, the methods used for text similarity for the purpose of multimodal retrieval are fairly mainstream [24].']],\n",
       "  [[', 2011;Thomee and Popescu, 2012], shows that the first model outperforms the state-of-the-art methods on three out of five datasets and the second proposed model outperforms the state-of-the-art methods on the five considered datasets on a tag-based image annotation task.'],\n",
       "   [', 2010;Thomee and Popescu, 2012].'],\n",
       "   [', 2011;Thomee and Popescu, 2012] shows that our framework achieves comparable and better results compared to more sophisticated state-of-the-art approaches as summarized in Table 1.'],\n",
       "   [\"‚Ä¢ ImageClef '12 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2012 photo annotation challenge ‚Ä¢ ImageClef '12 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2012 photo annotation challenge [Thomee and Popescu, 2012].\"]]],\n",
       " [[['In 2011, 2012 and 2013 respectively 8, 10 and 12 international research groups did cross the finish line of this large collaborative evaluation by benchmarking their images-based plant identification systems (see [14], [15] and [19] for more details).'],\n",
       "   ['in [22,5,9] or in the more recent methods evaluated in [15]).']],\n",
       "  [['Our descriptors have been tested on three leaf datasets: the Flavia dataset Our descriptors have been tested on three leaf datasets: the Flavia dataset [23], the ImageCLEF dataset in 2011 [8] and in 2012 [9].'],\n",
       "   ['The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see [9] for details).']],\n",
       "  [['2012, Pl@ntNet-Identify: an important milestone was marked in 2012 with the launch of the Pl@ntNet-Identify web application 2012, Pl@ntNet-Identify: an important milestone was marked in 2012 with the launch of the Pl@ntNet-Identify web application 6 and the development of an end-to-end innovative workflow involving the members of the Tela Botanica social network [17].'],\n",
       "   ['Details about the data, the methodology, the participants and the results can be found in the overview working notes produced each year [13,12,11].']],\n",
       "  [['The ImageCLEF 2012 The ImageCLEF 2012 [26] image dataset was adopted in this work in order to evaluate the proposed methodology by considering the plant identification species from its leaves.']],\n",
       "  [['In 2011, 2012 and 2013 respectively 8, 11 and 12 international research groups participated in this large collaborative evaluation by benchmarking their image-based plant identification systems (see [16,17,18] for more details).']],\n",
       "  [['Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset [35], the Flavia dataset [38], the Im-ageCLEF dataset in 2011 [16] and in 2012 [17].'],\n",
       "   ['The formula used to rank the runs in the ImageCLEF2012 plant identification task The formula used to rank the runs in the ImageCLEF2012 plant identification task [17] is nearly the same as in 2011 (see [17] for details).'],\n",
       "   ['The formula used to rank the runs in the ImageCLEF2012 plant identification task The formula used to rank the runs in the ImageCLEF2012 plant identification task [17] is nearly the same as in 2011 (see [17] for details).']],\n",
       "  [['Evaluation results on scans and scan-like leaf images of the ImageCLEF 2011 and ImageCLEF 2012 plant identification tasks [9,10] are reported and discussed in Section 3.'],\n",
       "   ['Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 [9] and in 2012 [10].'],\n",
       "   ['The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details).'],\n",
       "   ['The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details).']],\n",
       "  [['To do so, we use a subset of the training set of ImageCLEF 2012 plant identification task [2] freely available 2 : the training sets of scans and scan-like images are merged into one to form our knowledge database.']],\n",
       "  [[', 2012;Go√´au et al., 2012), the analysis of plant vein structure (Dhondt et al.']],\n",
       "  [['While plant related datasets exist for leaf or flower recognition (Go√´au et al., 2012;Silva et al.']],\n",
       "  [['We ran experiments on the following datasets: Flavia We ran experiments on the following datasets: Flavia [21], Foliage [10], Plant-CLEF2012 [6] and PlantCLEF2013 [5].']],\n",
       "  [['In 2011, 2012 and 2013 respectively 8, 11 and 12 research groups participated in this large collaborative evaluation by benchmarking their image-based plant identification systems (see [17,18,16] for more details).']],\n",
       "  [['The ImageCLEF 2011 [50] and ImageCLEF 2012 [51] datasets were used for the experiments.']],\n",
       "  [['In order to measure progress in a sustainable and repeatable way, the LifeCLEF [15] research platform was created in 2014 as a continuation and extension of the plant identification task [37] that had been run within the ImageCLEF lab [12] since 2011 [32][33][34].']],\n",
       "  [['The Pl@antLeaves II The Pl@antLeaves II [48] dataset is a subset of the ImageCLEF2012 [49] dataset, which contains different types of leaves from trees of the Mediterranean region of France.']]],\n",
       " [[['Unreleased sequences from ViDRILO have been successfully used in the RobotVision at Image-CLEF competition [13] in 2013 [3] and 2014 [2].']],\n",
       "  [['Finally, the fifth edition [17] included unprocessed 3D information in the form point cloud files (PCD format [18]).'],\n",
       "   ['This dataset includes images of the environment and point cloud files (in PCD format) [17].'],\n",
       "   ['The fifth edition The fifth edition [17] encouraged participants for using 3D information (point cloud files) with the inclusion of rooms completely imaged in dark.']]],\n",
       " [[['Some web-accessible retrieval systems such as Goldminer 7 or Yottalook8 allow users to filter the search results by modality [36].']],\n",
       "  [['ImageCLEFmed1 , an annual evaluation campaign on retrieval of images from the biomedical open access literature [8].']],\n",
       "  [['For this reason, the ImageCLEF2012 medical data set was used [28].'],\n",
       "   ['For this reason, the ImageCLEF2012 medical data set was used [28].']],\n",
       "  [['The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].'],\n",
       "   ['Similar to retrieval, it has shown that the classification results have better accuracy by combining text and images, in most cases, than the results using either text or image features alone [10][11][12].'],\n",
       "   ['-And finally, a performed systematic retrieval evaluation in a standard benchmark ImageCLEFmed 2012 evaluation [12] dataset of more than 300,000 images with asso-ciated annotations that demonstrated significant improvement in performance comparatively.'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['[27]) for ImageCLEFmed 2012 classification task [12] to a new hierarchy with the same acronyms for classes as shown in Fig.'],\n",
       "   ['For the purposes of this research, we used the Image-CLEFmed 2012 dataset For the purposes of this research, we used the Image-CLEFmed 2012 dataset [12] which contains over 300,000 images from 75,000 biomedical journal articles in the open access literature subset of the PubMed Central 11 database.'],\n",
       "   ['For the purposes of this research, we used the Image-CLEFmed 2012 dataset For the purposes of this research, we used the Image-CLEFmed 2012 dataset [12] which contains over 300,000 images from 75,000 biomedical journal articles in the open access literature subset of the PubMed Central 11 database.']],\n",
       "  [['2D images from the same anatomic regions as the selected 3D volumes (head, chest) were chosen from the publicly available ImageCLEF medical database 2D images from the same anatomic regions as the selected 3D volumes (head, chest) were chosen from the publicly available ImageCLEF medical database [18] as query images.']],\n",
       "  [['Since 2004, the medical task of ImageCLEF (Im-ageCLEFmed) aims at evaluating the performance of medical image retrieval systems [7,8].'],\n",
       "   ['Already many web-accessible search systems such as OpenI2  [16], Goldminer3 or Yottalook4 allow users to limit the search results to a particular modality [8] as this is a feature often requested by end users [17].']],\n",
       "  [['Therefore, medical image retrieval has attracted much more attention in recent years [11,12,7,4].'],\n",
       "   ['We notice that most works target at retrieving similar objects in image content [11,12] or the same imaging modalities [7,4] for a given query image.']],\n",
       "  [['Already, many web-accessible search systems such as Goldminer or Yottalook allow users to limit the search results to a particular modality (M√ºller, Garc√≠a Seco de Herrera, et al., 2012), as this is a feature often requested by end users (Markonis et al.'],\n",
       "   ['For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past (M√ºller, Garc√≠a Seco de Herrera, et al., 2012).']],\n",
       "  [['The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 [1,[11][12][13].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['For the development of modality classification models, we employed the challenging ImageCLEF-2012 medical image dataset 12,13 .'],\n",
       "   ['presented an overview of ImageCLEF-2012 image retrieval task for the 9th edition of Image-CLEF Muller et al.']],\n",
       "  [['After the evolution of ImageCLEF, three additional data sets were added: 230,088 images for the 2011 [50] data set, and 306,539 images for both the 2012 [51] and 2013 [52] data sets.']],\n",
       "  [['The image dataset, topics and ground truth of ImageCLEF 2012 medical image retrieval task The image dataset, topics and ground truth of ImageCLEF 2012 medical image retrieval task [10] were used in this evaluation.']]],\n",
       " [[['eu/portal/) and other datasets of annotations extracted from it [Petras et al. 2012].']],\n",
       "  [['The CHiC track The CHiC track (Petras et al. 2012(Petras et al.']]],\n",
       " [[['RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.']],\n",
       "  [[', 2013), the RepLab workshop in the CLEF conference (Amig√≥ et al., 2012), and the Sentiment Analysis in Twitter task (Task 2) in the last SemEval workshop (Nakov et al.']],\n",
       "  [[', 2017) and RepLab (Amig√≥ et al., 2012(Amig√≥ et al.']]],\n",
       " [[['Entity retrieval was studied at the TREC entity retrieval track [2], at INEX with the entity ranking [12] and linked data tracks [32], the workshop on Entity Oriented and Semantic Search [1,5], and other venues.']]],\n",
       " [],\n",
       " [],\n",
       " [[['We study this in the context of the INEX Social Book Search Track2  [12,14,15].'],\n",
       "   ['The INEX Social Book Search Track The INEX Social Book Search Track [12,14,15] investigates book search in collections with both professional metadata and social media content.'],\n",
       "   ['For the retrieval system, we use the Amazon/LibraryThing collection [2] that is also used in the INEX Social Book Search Track [15].']],\n",
       "  [['INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time [43].']],\n",
       "  [['2009) or in the social book search lab (Bogers et al. 2014).']],\n",
       "  [['The general background of the INEX workshop is best summarized in INEX reports published within INEX workshops The general background of the INEX workshop is best summarized in INEX reports published within INEX workshops [24][25][26][27][28][29][30] and the SIGIR Forum [1,2,22], the biannual publication of the ACM Specific Interest Group on Information Retrieval (SIGIR).'],\n",
       "   [\"All these tasks, as well as the participants' approaches, are described in the corresponding Book Track overviews [23][24][25][26][27][28][29][30].\"]],\n",
       "  [['Other topics of research for which online book discussion has served as input includes support for readers advisory services in the public library (Ridenour and Jeong 2016;Spiteri and Pecoskie 2016) and for social book search from an information retrieval perspective (Koolen et al. 2013).']],\n",
       "  [['Social Book Search (SBS) Social Book Search (SBS) (Koolen et al. 2014) finally really ended the earlier scanned books tasks, and fully focused on the social book data also used in the iSBS track.']]],\n",
       " [[['As reported in [39], there are no special studies regarding human judgement on text informativeness; however, it is a common evaluation criterion in the INEX Tweet Contextualization task at CLEF [40], [12], [13].']],\n",
       "  [['This motivated the proposal of a new track of Tweet Contextualization at INEX 1 in 2011 [71] which became a CLEF Lab 2 in 2012 [72] and that we fully depict in this paper.'],\n",
       "   ['It appeared that the use of paragraphs improves readability but does not allow informativeness optimization [72].'],\n",
       "   ['Dissimilarity values are very close, however differences are often statistically significant (details are not provided here but can be found in [72] for 2012 and in [8] for 2013).']],\n",
       "  [[\"Nous pr√©sentons dans cet article le mod√®le de RI puis l'approche de r√©sum√© automatique qui constituent notre syst√®me de contextualisation, puis nous √©valuons notre approche en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d 'INEX 2012(SanJuan et al., 2012).\"],\n",
       "   [\"Nous utilisons la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs Nous utilisons la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs (SanJuan et al., 2012).\"],\n",
       "   [\"Les organisateurs ont donc propos√© une mesure d'√©valuation qui calcule une divergence entre le contexte produit et les phrases jug√©es pertinentes (SanJuan et al., 2012).\"]],\n",
       "  [['To evaluate our method, we compare the system rankings we obtained using the informativeness measure proposed in [6] with the official system rankings based on document relevance, considering various TREC collections on adhoc tasks.']],\n",
       "  [[\"Nous √©valuons les performances de l'approche propos√©e en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d'INEX Nous √©valuons les performances de l'approche propos√©e en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d'INEX [22], qui propose un cadre exp√©rimental permettant d'√©valuer la contextualisation de Tweets r√©alis√©e √† l'aide de phrases issues de Wikip√©dia.\"],\n",
       "   [\"Nous utilisons dans un premier temps la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs Nous utilisons dans un premier temps la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs [22].\"],\n",
       "   [\"Les organisateurs ont donc propos√© une mesure d'√©valuation qui calcule une divergence entre le contexte produit et les phrases jug√©es pertinentes [4,22].\"]],\n",
       "  [['Tweet Contextualization (TC) Tweet Contextualization (TC) (SanJuan et al. 2012) was also a new track, but directly derived from the INEX 2011 Question Answering Track, which focused on more NLP-oriented tasks and moved to multidocument summarization.']],\n",
       "  [[\"La collection utilis√©e pour l'√©valuation dans campagnes INEX Tweet Contextualization 2011 et 2012 a √©t√© d√©crite dans La collection utilis√©e pour l'√©valuation dans campagnes INEX Tweet Contextualization 2011 et 2012 a √©t√© d√©crite dans (SanJuan et al., 2012).\"]]],\n",
       " [[['Although there are corpora available for other BioNLP tasks, such as text classification 16 and question answering 17 , these are not covered in this survey.']],\n",
       "  [[\"Question Answering for Machine Reading Evaluation (QA4MRE): Biomedical Text about Alzheimer's Disease QA4MRE2 for biomedical data [25] differs from TREC datasets because the focus of the dataset is on passage comprehension and multiple answers are already provided with each question.\"]],\n",
       "  [['The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease [28].']]],\n",
       " [[['The detection of the correct answer is specifically designed to require various types of inference, and the consideration of prior knowledge acquired from a collection of reference documents [6,7].']],\n",
       "  [['The entrance exam task was first proposed in 2013 as a pilot task The entrance exam task was first proposed in 2013 as a pilot task [12] in the Question Answering for Machine Reading Evaluation (QA4MRE) lab, which has been offered at the CLEF conference1 since 2011 [10,11].']],\n",
       "  [['1975), statAP (Pavlu and Aslam 2007), MTC (Carterette et al.']],\n",
       "  [[\"While contributing to performance on certain benchmarks, the over-reliance on specific entity names leads to an overestimation of model's actual ability to read and comprehend the provided passage (Pe√±as et al., 2011).\"]],\n",
       "  [['Pe√±as et al. (2011) take a coarser, epistemic approach, asking whether events are asserted, negated, or speculated, and Saurƒ± et al.']]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[['An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators (21) in which a ranked list of suspects is automatically created to prioritize the investigation.'],\n",
       "   ['We followed the logic of the automatic identification of sexual predators We followed the logic of the automatic identification of sexual predators (21).']],\n",
       "  [['It could also be used to learn the general profile of a group of persons such as sexual predators (Inches, Crestani 2012).']],\n",
       "  [['k See Inches & Crestani k See Inches & Crestani [51] for further detail related to creation of the Dataset.'],\n",
       "   ['p See participant run for gomezhidalgo12-2012-06-15-1 in Inches & Crestani p See participant run for gomezhidalgo12-2012-06-15-1 in Inches & Crestani [51].']],\n",
       "  [[\"Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN '12) [11], has focused on the automatic identification of predators in chat-room logs.\"]],\n",
       "  [['[ [ Inches and Crestani 2012] cover the 2012 International Sexual Predator Identification Competition, detailing a common evaluation framework against which 16 methods for identification of sexual predators could be evaluated in a comparable manner.'],\n",
       "   ['Additional comparable approaches can be seen in the results reported by [Inches and Crestani 2012], our figure for [Villatoro-Tello et al.']],\n",
       "  [['[32]: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).'],\n",
       "   [\"Therefore, we included texts from the previous year's shared task on sexual predator identification [14].\"]],\n",
       "  [['Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 [Inches and Crestani, 2012].']],\n",
       "  [['Para a composic ¬∏√£o das conversas n√£o-predat√≥rias, √© apresentado um m√©todo de extrac ¬∏√£o, transformac ¬∏√£o e selec ¬∏√£o de chats em comunidades virtuais, baseado em um dos trabalhos desenvolvidos na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012].'],\n",
       "   ['Para a composic ¬∏√£o das conversas n√£o-predat√≥rias, √© apresentado um m√©todo de extrac ¬∏√£o, transformac ¬∏√£o e selec ¬∏√£o de chats em comunidades virtuais, baseado em um dos trabalhos desenvolvidos na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012].'],\n",
       "   ['Nesse cen√°rio, o presente trabalho apresenta tr√™s contribuic ¬∏√µes: (i) criac ¬∏√£o do conjunto de dados PREDADORES-BR de acordo com o m√©todo descrito em [Inches and Crestani 2012], utilizado na competic ¬∏√£o PAN-2012, composto por conversas regulares ocorridas em comunidades virtuais e conversas com a presenc ¬∏a de predadores sexuais; (ii) an√°lise estat√≠stica do conjunto de dados PREDADORES-BR com base no m√©todo proposto em [Sokolova and Bobicev 2018]; (iii) avaliac ¬∏√£o experimental considerando os algoritmos que correspondem ao estado da arte em aprendizado de m√°quina no dom√≠nio da pesquisa.'],\n",
       "   ['Na competic ¬∏√£o PAN-2012 Na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012], √© proposto o uso de duas sub-categorias para conversas n√£o-predat√≥rias: 1) Conversas regulares, isto √©, conversas pertencentes √† diversas categorias, realizadas, originalmente, em formato textual e que n√£o apresentam termos sexuais; e 2) Conversas pertencentes a categoria adulta.'],\n",
       "   ['Essa caracter√≠stica do modelo CNN-RMSPROP √© considerada importante, visto que, no mundo real, por conta de todo o tempo desprendido e da mobilizac ¬∏√£o de profissionais da lei para atuar na investigac ¬∏√£o de um caso de suspeita de atividade predat√≥ria, a assertividade na identificac ¬∏√£o de uma conversa predat√≥ria √© priorizada, e essa caracter√≠stica se encontra refletida na medida Precis√£o [Inches and Crestani 2012].'],\n",
       "   ['A obtenc ¬∏√£o de conversas regulares √© considerada uma tarefa complexa, porque, normalmente, apresentam informac ¬∏√µes sens√≠veis ou restritas √†s pessoas envolvidas nas conversas particulares [Inches and Crestani 2012].']],\n",
       "  [[\"Moreover, since the seduction stage often shows similar characteristics with adults' or teenagers' flirting, initial studies trying to detect predatory behaviour directly on the user level typically resulted in numerous false positives when they were applied to non-predatory sexually-oriented chat conversations in the PAN 2012 dataset [27].\"],\n",
       "   ['Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults [27].'],\n",
       "   ['Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults [27].']],\n",
       "  [['NLP research in understanding these chats is crucial because law enforcement agencies have become overwhelmed with online cases; automated systems are needed in order to sift through the available textual data and transcripts to improve case triage through identification of criminal activity (Inches and Crestani, 2012).']],\n",
       "  [['This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).'],\n",
       "   ['This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).'],\n",
       "   ['This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).'],\n",
       "   ['In 2012, we saw a spike in the literature, likely due to the PAN12 competition for predator detection [67].'],\n",
       "   ['The most popular public datasets used in the literature for identifying sexual groomers include Perverted Justice (PJ) dataset [47] (N=22, 30%) and dataset from PAN-2012 competition (N=16, 22%) [67].'],\n",
       "   ['PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].'],\n",
       "   ['PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].'],\n",
       "   ['PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].'],\n",
       "   ['PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].'],\n",
       "   ['PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].'],\n",
       "   ['As the data acquisition for underage victims or law enforcement officers posing as children is challenging due to the laws and procedure involved [67], the PJ dataset focused on data from predators and adult volunteers posing as children.']],\n",
       "  [['We demonstrate the applicability of the proposed approach on the publicly available PAN 2012 dataset [24] where our results indicate a high detection rate of true positives.'],\n",
       "   ['The PAN 2012 [24] competition on identifying online child groomers collected these conversations from several resources to cover all characteristics of real-world data.'],\n",
       "   ['All the mentioned conversations were in two different datasets as training and testing set All the mentioned conversations were in two different datasets as training and testing set [24].'],\n",
       "   ['5 as the scale for measuring the performance of the grooming detection approaches [2,24].'],\n",
       "   ['5 -score is essential in the case of grooming detection [24].']],\n",
       "  [['Another strain of literature has focused on identifying predators from a mixed corpus of illicit and everyday conversations Another strain of literature has focused on identifying predators from a mixed corpus of illicit and everyday conversations [20,26,27,29,41,45].']]],\n",
       " [[['Juola [42] Authorship Profiling (detecting age and gender): e.']],\n",
       "  [[') PAN12: from the state-of-the-art corpus, especially created for the use in authorship identification for the PAN 2012 workshop5  (Juola, 2012), all closed-classed problems have been chosen -type: misc, authors: 3-16, documents: 6-16, samples per author: 2.']],\n",
       "  [['It was evaluated over two datasets, a baseline developed by the author and the dataset of the international workshop on plagiarism detection, author identification and author profiling (PAN) 2012 [33] .']],\n",
       "  [['In 2012, emphasis was put on smaller candidate sets and fiction in English [15].'],\n",
       "   ['[32]: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).']],\n",
       "  [['‚Ä¢ PAN12 [42] unlike the short email lengths per author in PAN11, this dataset consisted of dense volumes per author.']],\n",
       "  [['Table 2 presents the structure of each PAN benchmark [40].']],\n",
       "  [['This is a basic design principle used in user profiling to check whether questionable content is written by the same user who publishes a wide range of similar content [21] .']]],\n",
       " [],\n",
       " [[[\"The BioASQ challenge is a notable example of recent use of deep learning to bibliometrics The BioASQ challenge is a notable example of recent use of deep learning to bibliometrics [22] related task, but BioASQ's researcher and bibliometrician have so far explored the topic in parallel.\"],\n",
       "   ['Several were based on BERT and BioBERT [22].']],\n",
       "  [['org/ resul ts/ 8b/ phaseB/ ing task in BioASQ 6b and 7b challenges; in 8b challenge, they experimented with different pre-trained language models, such as BERT [5], BioBERT [7], XLNet [13] and SpanBERT [33], combining with transfer learning and voting method [34], to better solve biomedical factoid QA task.']],\n",
       "  [['To execute this task, we used the BioASQ-8b dataset (Nentidis et al., 2020) for different question types, i.'],\n",
       "   ['For each task, we gather the best performance, and specifically, they are BioASQ-8b (Nentidis et al., 2020), Chemprot (Peng et al.']]],\n",
       " [],\n",
       " [[[', , 2020;;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['Exploiting sources of evidence for Arabic rumor verification in Twitter is still under-studied; existing studies exclusively focused on the tweet text for verification [18,20,28,2,31,5].']],\n",
       "  [[', 2018;Barron-Cedeno et al., 2020).']],\n",
       "  [[', 2020;Shaar et al., 2020;Hasanain et al.']],\n",
       "  [[', , 2020;;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['Study in [79]focused on the task of claim verification, which involves a collection of claims and corresponding evidence in the form of text snippets sourced from web pages [93].']]],\n",
       " [[[', 2020;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media [1,27,36].']],\n",
       "  [[', , 2020;;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems (Shaar et al., 2020).']],\n",
       "  [['In light of the wave of misinformation associated with COVID-19 pandemic researchers have been collecting relevant datasets of scientific publications, news articles and their headlines, social media posts and claims about COVID-19 In light of the wave of misinformation associated with COVID-19 pandemic researchers have been collecting relevant datasets of scientific publications, news articles and their headlines, social media posts and claims about COVID-19 (Shaar et al., 2020;Dharawat et al.']],\n",
       "  [[', 2020;Shaar et al., 2020;Hasanain et al.']],\n",
       "  [[', , 2020;;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['Automated fact-checking systems are pivotal not only for combatting false information on digital media but also for reducing the workload of fact-checkers Automated fact-checking systems are pivotal not only for combatting false information on digital media but also for reducing the workload of fact-checkers [1,2].'],\n",
       "   ['A key functionality of these systems is the retrieval of already debunked narratives for misinformation claims, which essentially means retrieving previously fact-checked similar claims [2][3][4].'],\n",
       "   ['Previous methods for training debunked-narrative retrieval models heavily rely on annotated pairs of misinformation claims and debunks Previous methods for training debunked-narrative retrieval models heavily rely on annotated pairs of misinformation claims and debunks [2,4,5].'],\n",
       "   ['Lab shared task 2020, 2021 and 2022 [2,3,14,23] focus on debunked-narrative retrieval task and release different datasets for training and testing.'],\n",
       "   ['Lab task datasets which include CLEF 22 2A [3], CLEF 21 2A [14] and CLEF 20 2A [2].'],\n",
       "   ['We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers [2,3,14,16].'],\n",
       "   ['com/view/clef2021-checkthat (4) CLEF 20 2A-EN [2] https://sites.']],\n",
       "  [['In that year, the team utilizing RoBERTa secured the first position in the English category [9].'],\n",
       "   ['Feature representation methods, including word embeddings, Bag of Words, Named Entity Recognition, and Part of Speech tagging [6,7,9] have been widely used to enhance model understanding of the task.']]],\n",
       " [],\n",
       " [],\n",
       " [[['Clinical coding standardizes medical records for health information management systems so as to perform research studies, monitor health trends or facilitate medical billing and reimbursement [31].']],\n",
       "  [['CodiEsp dataset CodiEsp dataset [40] is a public dataset released by the CLEF eHealth 2020 conference 5 .']],\n",
       "  [['It prompted the search for tools that assist manual standardization [6].']],\n",
       "  [['‚Ä¢ An updated list of term variants documented in real domain texts, namely the Spanish versions of MedlinePlus [18], and state-of-the-art annotated medical corpora: datasets used in recent shared tasks (CODIESP [19], CANTEMIST [20], Phar-maCoNER [21]), the Chilean Waiting List Corpus (CLWC) [22] and the CT-EBM-SP corpus of texts about clinical trials [23].'],\n",
       "   ['covd- 19), tokenization mistakes or words with hashtags (e.']],\n",
       "  [['Our query revealed a number of medical NER challenges for the Spanish language (Table 3), including CLEF eHealth (2020-21) [92][93][94][95][96][97][98][99][100][101], Iber-LEF (2020-22) [17,70,[102][103][104][105][106][107][108][109][110][111][112][113][114][115], and CLEF BioASQ (2022) [116][117][118][119].']]],\n",
       " [],\n",
       " [[['Being orders of magnitude bigger than previously annotated corpora in historical French Being orders of magnitude bigger than previously annotated corpora in historical French (Ehrmann et al. 2020), contemporary French (Sagot, Richard, and Stern 2012) and even English (Pradhan et al.']],\n",
       "  [[\"[60] evaluated systems' performances on various entity noise levels, defined as the length-normalised Levenshtein distance between the OCR surface form of an entity and its manual transcription.\"],\n",
       "   [\"[60] evaluated systems' performances on various entity noise levels, defined as the length-normalised Levenshtein distance between the OCR surface form of an entity and its manual transcription.\"],\n",
       "   ['[55], second on the occasion of the HIPE-2020 shared task [60].'],\n",
       "   ['The recently organised HIPE shared tasks on named entity recognition and linking in multilingual historical documents are essential first step towards alleviating this situation: both editions, HIPE-2020 [57,60] and HIPE-2022 [58,63,65], have produced significant datasets for the evaluation of NE processing systems on historical material.'],\n",
       "   ['In the context of the CLEF-HIPE-2020 shared task In the context of the CLEF-HIPE-2020 shared task [60], Dekhili and Sadat [46] proposed different variations of a BiLSTM-CRF network, with and without the in-domain HIPE flair embeddings and/or an attention layer.']],\n",
       "  [['In this regard, the first CLEF-HIPE-2020 editionIn this regard, the first CLEF-HIPE-2020 edition5  [5] proposed the tasks of NE recognition and classification (NER) and entity linking (EL) in ca.']],\n",
       "  [[', 2019) and HIPE (Ehrmann et al., 2020) annotated historical NER datasets using prototypical NER systems built on the previous and current generation of models, trained on contemporary annotations from ACE 2005 (Walker et al.'],\n",
       "   ['HIPE (Ehrmann et al., 2020) is a collection of digitized documents covering three different languages: English, French, and German.']],\n",
       "  [['AI methods and models have provided significant improvements for all of the above applications, for instance, for recognizing text in historical prints (  (Brantl/Schweter 2022), or in the area of named entity recognition and linking (Ehrmann et al. 2020;2022).']],\n",
       "  [[\"Ainsi, pour la reconnaissance et la classification d'entit√©s nomm√©es, mais aussi de mani√®re g√©n√©rale, les efforts sont consacr√©s √† la mani√®re de transf√©rer efficacement les connaissances pour l'adaptation au domaine en d√©veloppant des syst√®mes robustes inter-domaines et en explorant l'apprentissage zero-shot ou few-shot pour traiter la coh√©rence et l'inad√©quation des domaines et des annotations dans des contextes inter-domaines (Ehrmann et al., 2020c(Ehrmann et al.\"]]],\n",
       " [[['Research into lifelogging has been gaining in popularity with many collaborative benchmarking workshops taking place recently -the NTCIR Lifelog task [12], the Lifelog Search Challenge (LSC) [13] and the ImageCLEFlifelog [23].'],\n",
       "   ['The baseline for the comparison was the modified version of the Mysc√©al system [29] that had been used in the ImageCLEFlifelog2020 benchmarking workshop and achieved third place out of six participants [23].']],\n",
       "  [['In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].']],\n",
       "  [['Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges [82][83][84]; Lifelog Search Challenge [85][86][87], which aims to encourage the development of efficient interactive lifelog retrieval systems; and NTCIR Lifelog Tasks [77].']],\n",
       "  [[\"Out of the five R's, retrieving lifelog data, typically lifelog photos, has been the subject of the majority of lifelog research, as seen in various workshops [11,12,21].\"]],\n",
       "  [['In the last years, shared lifelogging retrieval tasks were set up on different continents In the last years, shared lifelogging retrieval tasks were set up on different continents [9,14,16,28] in order to bring the attention of a wider audience to lifelogging and to promote research into some of its key challenges.'],\n",
       "   ['Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].'],\n",
       "   ['In the last editions of ImageCLEFlifelog In the last editions of ImageCLEFlifelog [4,28], we proposed some preliminary work presenting a baseline approach to solve the task of lifelog moment retrieval (LMRT) [29,30].']],\n",
       "  [['Although multiple data sources are recorded in multimodal personal datasets [12], only the combination of visual and related metadata including semantic locations, daily-life activities, date and time are employed extensively in research [10,19] while others has not yet been exploited.']],\n",
       "  [['The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [23] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [24][25][26]55] and the ImageCLEF Lifelog tasks [12][13][14]46].']],\n",
       "  [['To benchmark for different search engines in indexing and retrieving multimodal lifelog data, there have been a number of interactive lifelog retrieval challenges such as NTCIR Lifelog [7,8], ImageCLEF Lifelog [4,5,20], and Lifelog Search Challenge (LSC) [9,10].']],\n",
       "  [['There are various challenges and workshops have been organized in the field of lifelog image retrieval There are various challenges and workshops have been organized in the field of lifelog image retrieval [5,6,22].']],\n",
       "  [['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.']],\n",
       "  [['Datasets: Lifelogging tasks, such as the Lifelog Search Challenge Datasets: Lifelogging tasks, such as the Lifelog Search Challenge [4] and ImageCLEF lifelog [11] task, have been developed and maintained to facilitate research in the lifelog retrieval domain.']]],\n",
       " [[[', 2017) and the VQA-Med dataset (Ben Abacha et al., 2020).']],\n",
       "  [[', multimodal components), VQA for the medical domain has its own new challenges 9 .']],\n",
       "  [['The mainstream VQA datasets in the medical domain include VQA-MED-2018 [40], VQA-MED-2019 [6], and VQA-MED-2020 [7], which were proposed by the challenge tasks.'],\n",
       "   ['The mainstream VQA datasets in the medical domain include VQA-MED-2018 [40], VQA-MED-2019 [6], and VQA-MED-2020 [7], which were proposed by the challenge tasks.']],\n",
       "  [['Thus, it has become a very active area of research, with competitive benchmarks and yearly competitions (Abacha et al., 2021).'],\n",
       "   ['Due to the lack of diversity in the semantics of the questions in the ImageCLEF VQA-Med 2021 Challenge (Abacha et al., 2021), the winning teams (Gong et al.'],\n",
       "   [', 2020(Abacha et al., , 2021) ) often finetune an ensemble of pre-trained VGG (Simonyan and Zisserman, 2014) and various ResNet (Lei et al.']],\n",
       "  [['VQA-MED-2018 [9], VQA-RAD [14], VQAMED-2019 [4], RadVisDial [13], PathVQA [11], VQA-MED-2020 [3], SLAKE [18], and VQA-MED-2021 [5].']]],\n",
       " [],\n",
       " [[['Later ImageCLEF Caption tasksLater ImageCLEF Caption tasks[77,78] only required systems to assign labels to medical images, without requiring diagnostic tex to be generated.']],\n",
       "  [[', for face and object detection [4] or for concept detection [70].']],\n",
       "  [['In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.']],\n",
       "  [['‚Ä¢ ImageCLEFmed [204]: A benchmark dataset for multimodal biomedical information retrieval that includes medical images, captions, and text descriptions.']],\n",
       "  [['The ROCO dataset has been used in the medical caption tasks The ROCO dataset has been used in the medical caption tasks [3][4][5][6] at the Image Retrieval and Classification Lab of the Conference and Labs of the Evaluation Forum (ImageCLEF) 7 .']]],\n",
       " [[['The 4th edition of the task will follow a similar format to previous editions [2][3][4] where participants automatically segment and label a collection of images that can be used in combination to create three-dimensional models of an underwater environment.']]],\n",
       " [[[\"Other datasets contain only images of one disease, such as Andrew's Kaggle dataset, JSRT Other datasets contain only images of one disease, such as Andrew's Kaggle dataset, JSRT [57], Optical Coherence Tomography (OCT) and Chest X-ray Images [58], RSNA Pneumonia Detection Challenge Dataset [59], RIH-CXR [60], NCI Genomic Data Commons [61], COVID Chest X-ray Dataset, ImageCLEF [62], ChestX-ray images (Pneumonia) [58], Montgomery and Shenzhen datasets [63], Shenzhen datasets [63], COVID-CT Dataset [64], Autofocus database [65], Sajid's Kaggle dataset, LDOCTCXR [66], Sunnybrook Cardiac MRI dataset [67], CPTAC-LUAD dataset [68], and the LIDC-IDRI dataset [69].\"]]],\n",
       " [[['To obtain an accurate flower identity descriptor ùêπ ùëì ùëñùëë , we adopt the EfficientNet [57] and pretrain it on LifeCLEF2021 Plant Identification [5,[23][24][25].']],\n",
       "  [['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']],\n",
       "  [['Datasets in agriculture are mainly plant leaves or plant images, such as PlantCLEF 5 , VNPlan t 6 , BJFU100 7 and other data sets.']]],\n",
       " [[[', 2020;Moorthy, 2020;Picek et al., 2020).']],\n",
       "  [['The AI-based Snake Species Identification Challenge (SnakeCLEF) has introduced a new platform for automatic snake species recognition experiments by providing labeled data with geographical information The AI-based Snake Species Identification Challenge (SnakeCLEF) has introduced a new platform for automatic snake species recognition experiments by providing labeled data with geographical information [33].']]],\n",
       " [[['More recent BirdCLEF competitions have made use of audio recordings of over 900 bird species and evaluated methods on 10-min soundscapes from four distinct geographic regions [13].'],\n",
       "   ['More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning [13,17].']],\n",
       "  [['With the rapid development of artificial intelligence, deep neural network-based bird classification methods have been proposed, including bird classification by the acoustic [6][7][8][9] and visual [10][11][12] features of birds.']],\n",
       "  [['Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity, with several tens of thousands of recordings covering up to 1,500 species [9,10].']]],\n",
       " [],\n",
       " [],\n",
       " [[['Thus, the task of style change detection aims at detecting positions of author changes within a collaboratively written text [33].']]],\n",
       " [[['\" [68] The models implemented in this research differ on the treatment over lexical features.']],\n",
       "  [[\"A common approach is based on the analysis of the author's writing style or content-based characteristics to reveal the author's various attributes, including age [28].\"]]],\n",
       " [[['‚Ä¢ In the 2020 PAN authorship verification task [15], methods employing neural networks made an appearance, particularly in the form of Siamese neural networks.']],\n",
       "  [['These biases were taken in consideration by PAN starting from its 2020 edition [57].']]],\n",
       " [[['[35] organised an evaluation shared task where the participants had to build a system that could determine whether a user is a potential fake news spreader or not.']],\n",
       "  [['To model user endogenous preferences, existing works have attempted to utilize historical posts as a proxy and have shown promising performance to detect sarcasm [13], hate speech [20], and fake news spreaders [21] on social media.']],\n",
       "  [['False information spreading involves various research tasks, including: fact checking [4,40], topic credibility [15,41], fake news spreaders profiling [34], and manipulation techniques detection [8].']],\n",
       "  [[', 2018), Profiling Fake News Spreaders at PAN (Rangel et al., 2020) and SemEval rumor detection (Derczynski et al.']],\n",
       "  [['Ya≈ü, cinsiyet, eƒüitim gibi temel √∂zelliklerin tespiti g√∂revlerinin yanƒ±nda, herhangi bir yazarƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ±nƒ±n belirlenmesi gibi √∂zel g√∂revler de bulunmaktadƒ±r [1].'],\n",
       "   ['CLEF konferanslarƒ±nda d√ºzenlenen PAN workshoplarƒ±nda temel ama√ß yazar profili olu≈üturmaktƒ±r CLEF konferanslarƒ±nda d√ºzenlenen PAN workshoplarƒ±nda temel ama√ß yazar profili olu≈üturmaktƒ±r [1].'],\n",
       "   ['CLEF konferanslarƒ±nda d√ºzenlenen PAN workshoplarƒ±nda temel ama√ß yazar profili olu≈üturmaktƒ±r CLEF konferanslarƒ±nda d√ºzenlenen PAN workshoplarƒ±nda temel ama√ß yazar profili olu≈üturmaktƒ±r [1].'],\n",
       "   ['n= [1,2,4,5,10] olmak √ºzere 5 farklƒ± deƒüer se√ßilmi≈ütir.']],\n",
       "  [['Another work related to author profiling is mentioned in [24].']],\n",
       "  [['Automatically identifying and analyzing the behavior of active citizens in social networks is important for diffusion of misinformation prevention at the user level Automatically identifying and analyzing the behavior of active citizens in social networks is important for diffusion of misinformation prevention at the user level [13,47,54].'],\n",
       "   ['[47] focus on identifying Twitter users who diffuse unreliable news stories either on post level or news media level.'],\n",
       "   ['This is approximately 100 and 15 times larger than the datasets used in prior work [13,47].']],\n",
       "  [['One of the shared tasks at the PAN @ CLEF conference in 2020 was Profiling Fake News Spreaders on Twitter (Rangel et al. 2020).']],\n",
       "  [['Some of the most common tasks, often co-located with international conferences, are those about fake news (Rangel et al., 2020), hate speech (Bosco et al.']],\n",
       "  [['This process can be used to identify, for instance, people who originated or spread misinformation or fake news [66], ensuring that authorities adopt measures to contain their actions.'],\n",
       "   ['[66] presented an author profile task using long short-term memory with posts from Twitter in two languages (English and Spanish) to identify fake news authors.']],\n",
       "  [['Recently, the author profiling shared task at the PAN 2020 conference focused on determining whether or not the author of a Twitter feed was keen to spread fake news Recently, the author profiling shared task at the PAN 2020 conference focused on determining whether or not the author of a Twitter feed was keen to spread fake news [63].'],\n",
       "   ['In this case, we argue that social network platforms should combine recommendation strategies with detection algorithms that are able to identify superspreader users from others [63,71].']],\n",
       "  [['The five datasets encompass a wide range of subject areas, including health-related datasets (MM COVID The five datasets encompass a wide range of subject areas, including health-related datasets (MM COVID [17] and ReCOVery [43]), a political dataset (LIAR [33]), and multi-domain datasets (MC Fake [23] and PAN2020 [27]).'],\n",
       "   ['The five datasets encompass a wide range of subject areas, including health-related datasets (MM COVID The five datasets encompass a wide range of subject areas, including health-related datasets (MM COVID [17] and ReCOVery [43]), a political dataset (LIAR [33]), and multi-domain datasets (MC Fake [23] and PAN2020 [27]).']]],\n",
       " [[[', 2020b(Bondarenko et al., , 2021) ) featured a related track.']],\n",
       "  [['‚Ä¢ Controversial arguments: Touch√© ‚Ä¢ Controversial arguments: Touch√© (Bondarenko et al., 2020) and ArguAna (Wachsmuth et al.']],\n",
       "  [[', 2019;Bondarenko et al., 2021).']],\n",
       "  [['0 5183 300 Quora Not reported 522931 10000 Touch√©-2020 (Bondarenko et al., 2020) CC BY 4.']],\n",
       "  [[', 2020), Touch√©-2020 (Bondarenko et al., 2020), and SciFact (Wadden et al.']],\n",
       "  [['To evaluate the robustness of our approach in an out-of-domain setting we take a subset of the BEIR benchmark sets [41], due to the greater computational expense of current list-wise rankers multiple sparsely annotated test sets would be infeasible under our compute budget, as such we choose test sets with a smaller set of densely annotated topics namely, TREC Covid [42] and Touche [43].']],\n",
       "  [['Touch√©-2020 dataset (Bondarenko et al., 2020)  and categorized questions and answers across various domains.']],\n",
       "  [['We experiment with all of the following 15 MTEB retrieval datasets: Arguana We experiment with all of the following 15 MTEB retrieval datasets: Arguana [44], ClimateFEVER [10], CQADupstackRetrieval [20], DBPedia [19], FEVER [42], FiQA2018 [32], HotpotQA[49], MSMARCO [3], NFCorpus [7], NQ [28], QuoraRetrieval [41] SCIDOCS [9], SciFact [45], Touche2020 [6], TRECCOVID [43].'],\n",
       "   ['6 from https://huggingface.']],\n",
       "  [['‚Ä¢ Text Retrieval -BEIR ‚Ä¢ Text Retrieval -BEIR [53] (ArguAna [56], FEVER [54], FIQA [40], MS MARCO [7], NQ [29], Quora, SciFact [57], Touch√©-2020 [11], HotPotQA [63]): https://github.']]],\n",
       " [[['More recent projects and contests have consistently shown that ML is a promising methodology in the context of text mining of clinical narratives [7].']],\n",
       "  [['A Dictionary-based concept recognition system overcame CRF and SVM classifiers in CLEF 2015 [13] on the MEDLINE corpus, according to the Exact Match metric, which considers a term (word or group of words that have a label) as correctly classified only if all the words in the term received the correct label.']],\n",
       "  [['Newly generated features are expected to improve recognition results [16].']],\n",
       "  [[', 2014, N√©v√©ol et al., 2015, Suominen et al.']],\n",
       "  [[', 2018), including French (N√©v√©ol et al., 2015;Cardon et al.']]],\n",
       " [[['To demonstrate how multi-objective optimization for balancing multiple relevance criteria works in practice, we perform experiments on two datasets: (i) balancing readability and topical relevance in a health setting (CLEF eHealth 2015 task 2 To demonstrate how multi-objective optimization for balancing multiple relevance criteria works in practice, we perform experiments on two datasets: (i) balancing readability and topical relevance in a health setting (CLEF eHealth 2015 task 2 [8]), and (ii) balancing diversity and topical relevance in a web search dataset annotated for sub-topic relevance (TREC 2012 Web Track diversity task).']],\n",
       "  [['The 2015 task has instead focused on supporting consumers searching for self-diagnosis information [23], an important type of health information seeking activity [7].'],\n",
       "   ['Previous research has shown that di‚Üµerent users tend to issue di‚Üµerent queries for the same information need and that the use of query variations for evaluation of IR systems leads to as much variability as system variations [1,2,23].'],\n",
       "   ['Note that we explored query variations also in the 2015 IR task [23], and we found that for the same image showing a health condition, di‚Üµerent query creators issued very di‚Üµerent queries: they di‚Üµer not only in terms of the keywords contained in the query, but also with respect to their retrieval e‚Üµectiveness.']],\n",
       "  [['Investigation on the effectiveness of search engines in retrieving information about medical symptoms has been conducted, focusing on designing systems which improve health search [19].']],\n",
       "  [['Evaluation campaigns and resources in this domain are presented, including TREC Medical Records Track [43,45,46], TREC Clinical Decision Support Track [36][37][38], CLEF eHealth (consumer health search [13,14,35,55] and as of 2017 search systems for the compilation of systematic reviews), i2b2 Shared Task Challenges¬≥, ALTA Shared Task ¬≥https://www.']],\n",
       "  [['Investigation on the effectiveness of search engines in retrieving information about medical symptoms has been conducted, focusing on designing systems which improve health search (Palotti et al. 2015).']],\n",
       "  [['See[44] for details.']],\n",
       "  [[\"CLEF's e-health track had a clinical IR subtask from 2013 to 2016 (95)(96)(97)(98).\"]]],\n",
       " [[['[19], for example, but should rather be understood as a presentation of a methodology with quantitative evidence.']]],\n",
       " [[['Therefore, the ImageCLEFmed4 image classification and retrieval benchmark proposed in 2015 and 2016 a multi-label task aiming at labeling all compound figures with each of the modalities of the subfigures contained without knowing the subfigure separations that are contained in the image [10,11].'],\n",
       "   ['Figure 2 shows that hierarchy of images classes that was used [10,11] to classify all subfigures into types.'],\n",
       "   ['In 2015, 1,568 were distributed for the ImageCLEFmed multi-label task [10] In 2016, ImageCLEFmed proposed 5 tasks: compound figure detection; compound figure separation; multi-label classification; subfigure classification and caption prediction.']],\n",
       "  [['The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].'],\n",
       "   ['The best results are obtained by a combination of cross-media predictions [5,6].'],\n",
       "   ['39% in ImageCLEF2015 [5] and 92.'],\n",
       "   ['The best results are obtained by a combination of cross-media predictions [5,6].'],\n",
       "   ['39% in ImageCLEF2015 [5] and 92.'],\n",
       "   ['For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset [5,6] using a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF 2015 [5], the training set contains 10,433 figures and the test set 10,434 figures.']],\n",
       "  [['Take ImageCLEF medical [5,33,34] as an example; it provides thousands of labeled medical images for modality classification, which is a much smaller amount than the ImageNet dataset [20], which contains 1.'],\n",
       "   ['The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF2015 [33], the training set contains 4532 figures and the test set 2244 figures.'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF2015 [33], the training set contains 4532 figures and the test set 2244 figures.'],\n",
       "   ['We obtained good performance We obtained good performance [36] using CNN-6 in the Compound Figure Detection Task [33,34].']],\n",
       "  [['Again, several challenges in ImageCLEF tackled this problem with often very good results for both detecting the subfigures and separating them [16].']],\n",
       "  [['For the ImageCLEF 2015 Medical Tasks [27], late fusion methods were applied in Pelka et al.']]],\n",
       " [],\n",
       " [[['Even if the relationship between the surrounding text and images varies greatly, with much of the text being redundant and/or unrelated to the visual content, a large amount of information about an image can be found in the textual context of the Web pages Even if the relationship between the surrounding text and images varies greatly, with much of the text being redundant and/or unrelated to the visual content, a large amount of information about an image can be found in the textual context of the Web pages [14].']],\n",
       "  [[', 2015), escrita criativa po√©tica seguindo as m√©tricas dos diferentes estilos (ZHANG;LAPATA, 2014;GABRIEL;CHEN;NICHOLS, 2015), descri√ß√£o de imagens (KARPATHY; FEI-FEI, 2015;GILBERT et al., 2015), comunica√ß√£o com crian√ßas e adolescentes com necessidades especiais (TINTAREV et al.']],\n",
       "  [['Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.']],\n",
       "  [['Putting all this together, our system is the winning entry in the ImageCLEF 2015 image sentence generation task [13,14].'],\n",
       "   [', the MSCOCO Captioning Challenge [9] and the ImageCLEF 2015 image sentence generation task [14], have demonstrated outstanding performance of LSTM based solutions [3,13].'],\n",
       "   ['To verify the effectiveness of our approach, we participated in the ImageCLEF 2015 image sentence generation task To verify the effectiveness of our approach, we participated in the ImageCLEF 2015 image sentence generation task [14].'],\n",
       "   ['Following the protocol [14], we use the METEOR score to assess the performance of image captioning.']]],\n",
       " [],\n",
       " [[['The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors.'],\n",
       "   ['The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors.']],\n",
       "  [[', the bird species identification challenge LifeCLEF [6]).'],\n",
       "   ['The dataset we used comprises the 200 tropical bird species most numerously represented in Xeno-Canto, gathered from field sites in Brazil, Colombia, Venezuela, Guyana, Suriname and French Guiana [6].']],\n",
       "  [['With appearance of publicly available audio datasets, such as freefield1010 [2], Warblr [3] or Chernobyl dataset from TREE project 1 and challenges for bird recognition, such as the LifeCLEF Bird Identification Task [4] and the recent Bird Audio Detection (BAD) Challenge [3], the problem has received considerable attention from audio research community.']],\n",
       "  [['The dataset was the same as the one used for BirdCLEF 2017 The dataset was the same as the one used for BirdCLEF 2017 [4], mostly based on the contributions of the Xeno-Canto network.'],\n",
       "   ['The dataset was the same as the one used for BirdCLEF 2017 The dataset was the same as the one used for BirdCLEF 2017 [4], mostly based on the contributions of the Xeno-Canto network.'],\n",
       "   [') can be found in the overview working note of BirdCLEF 2017 [4].']],\n",
       "  [['It was shown in previous editions of BirdCLEF [35,36] that systems for identifying birds from mono-directional recordings are now performing very well and several mobile applications implementing this are emerging today.']]],\n",
       " [[['This paper presents the participation of Inria ZENITH team to the 2015-edition of this challenge [9,19].']],\n",
       "  [['We focused our experiments on the use of Convolution Neural Networks We focused our experiments on the use of Convolution Neural Networks [29], which have been shown to considerably improve the accuracy of automated plant species identification compared to previous methods [20,30,31].'],\n",
       "   [\"Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge [31,33] on species identification, there is no doubt that they are able to capture discriminant visual patterns of the plants in a much more effective way than previously engineered visual features.\"],\n",
       "   ['[30,31]).']]],\n",
       " [[[\"Du point de vue de l'√©valuation de ces mod√®les, une premi√®re initiative dans le cadre de la campagne ¬´ Living Labs for IR ¬ª (LL4IR) Du point de vue de l'√©valuation de ces mod√®les, une premi√®re initiative dans le cadre de la campagne ¬´ Living Labs for IR ¬ª (LL4IR) (Schuth et al., 2015) √† CLEF 2015 (Mothe et al.\"],\n",
       "   ['La campagne d\\'√©valuation \"Living Labs for Information Retrieval\" (LL4IR) La campagne d\\'√©valuation \"Living Labs for Information Retrieval\" (LL4IR) (Schuth et al., 2015 ;Cappellato et al.'],\n",
       "   [\"Ce syst√®me est bas√© sur l'historique des clics (Schuth et al., 2015).\"],\n",
       "   [\"489) qui d√©passe les valeurs des m√©triques obtenues pour l'ensemble des participants ainsi que la Baseline fournie par les organisateurs de la campagne d'√©valuation (Schuth et al., 2015).\"]],\n",
       "  [['The living lab approach has been applied in Information Retrieval (IR) research to primarily involve users in IR evaluation The living lab approach has been applied in Information Retrieval (IR) research to primarily involve users in IR evaluation [3,16].']],\n",
       "  [['The thesis is based on in total 8 publications [15,[161][162][163][164][165][166][167].'],\n",
       "   ['We believe we succeeded as our platform, and thereby the OpenSearch paradigm, has been adopted by both theliving labs for IR evaluation lab at CLEF (LL4IR) [164] initiative and the upcoming OpenSearch track at TREC [197].'],\n",
       "   ['This chapter is based on two papers by Balog, Kelly, and Schuth This chapter is based on two papers by Balog, Kelly, and Schuth [15] and Schuth, Balog, and Kelly [164].'],\n",
       "   ['This chapter is based on two papers by Balog, Kelly, and Schuth This chapter is based on two papers by Balog, Kelly, and Schuth [15] and Schuth, Balog, and Kelly [164].'],\n",
       "   ['Details on our implementation and the results from the LL4IR experiments run with the OpenSearch paradigm can be found in the paper by Schuth, Balog, and Kelly Details on our implementation and the results from the LL4IR experiments run with the OpenSearch paradigm can be found in the paper by Schuth, Balog, and Kelly [164].']],\n",
       "  [['‚Ä¢ How to run online LTR experiments at home ‚Ä¢ How to run online LTR experiments at home [25] ‚Ä¢ CLEF LL4IR: Living Labs for IR Evaluation [26] ‚Ä¢ TREC OpenSearch -Academic Search [32] [15 minutes] Discussion and conclusion.']],\n",
       "  [['To tackle this gap, two main user-centered initiatives, namely the LL4IR [2] and the NewsREEL [1] benchmarks running for CLEF, have been launched.']],\n",
       "  [['2004) and living labs (Schuth et al. 2015) provide and use a large range of different modalities in order to optimize the retrieval results.']],\n",
       "  [['Further, it might happen (and as we show in (Schuth et al, 2015a) it indeed did happen) during the test period that new products arrive; experimental systems were unable to include these in their ranking (this was the same for all participants), while the production system might return them.'],\n",
       "   ['A more in-depth analysis of the results is provided in the LL4IR extended lab overview paper (Schuth et al, 2015a).'],\n",
       "   ['Again, we refer to the LL4IR extended lab overview paper (Schuth et al, 2015a) for full details.'],\n",
       "   ['7 A more detailed analysis of the use-cases, including results from a second unofficial evaluation round, and a discussion of ideas and opportunities for future development is provided in the LL4IR extended lab overview paper (Schuth et al, 2015a).']],\n",
       "  [['The Living Labs for Information Retrieval Evaluation (LL4IR) held as part of CLEF 2015 presented a unique collaboration between industry and academia [Schuth et al., 2015, Balog et al.']],\n",
       "  [['The CLEF LL4IR track [11] featured product search and web search as use cases.']]],\n",
       " [[['This is typically achieved via simple most-popular news recommendations, especially since this approach has been shown to provide accurate recommendations in offline evaluation settings [11].']]],\n",
       " [[['Cross-Language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008) extends the classical ESA (Gabrilovich and Markovitch, 2007) to work in a cross-language scenario.'],\n",
       "   ['In 2015 the task invited for the first time to submit datasets (Potthast et al., 2015;Franco-Salvador et al.']],\n",
       "  [['While a complete survey of the results of the PAN initiative is out of the scope of this paper, we refer to the latest overviews of the respective tasks, namely for authorship attribution [19], authorship verification [33], author profiling [28], and the two subtasks of reuse detection, text alignment and source retrieval [15,27].']],\n",
       "  [['Table Table 1 shows the results of plagiarism detection in terms of Precision, Recall and Plagdet [11] scores.'],\n",
       "   ['Table Table 1 shows the results of plagiarism detection in terms of Precision, Recall and Plagdet [11] scores.']],\n",
       "  [['proposed some automatic and manual methods to evaluate and validate submitted corpora on the first shared task on plagiarism detection data submission (Potthast, Goering, Rosso, & Stein, 2015).']]],\n",
       " [[['Many works focus on socio-professional categories: age [1,59,61], gender [1,59,61], ethnicity/regional origin [55,61], city [14,49], country [37,49], political orientation [30,1,17,50,55,61], business domain [55].'],\n",
       "   ['Many works focus on socio-professional categories: age [1,59,61], gender [1,59,61], ethnicity/regional origin [55,61], city [14,49], country [37,49], political orientation [30,1,17,50,55,61], business domain [55].'],\n",
       "   ['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter: PAN [59] and RepLab [3].']],\n",
       "  [['While a complete survey of the results of the PAN initiative is out of the scope of this paper, we refer to the latest overviews of the respective tasks, namely for authorship attribution [19], authorship verification [33], author profiling [28], and the two subtasks of reuse detection, text alignment and source retrieval [15,27].']],\n",
       "  [[', 2014;Rangel et al., 2015).']],\n",
       "  [[', 2014;Rangel et al., 2015;Schler et al, 2006;Tausczik and Pennebaker, 2010).'],\n",
       "   [', 2014;Rangel et al., 2015).'],\n",
       "   ['The prize winners of the 15th evaluation lab on digital text forensics PAN 2015, which was held in a bid to find the most accurate ways of identifying the gender, age, and psychological traits in accordance with the Five Factor Theory (extroversion, emotional stability/neuroticism, agreeableness, conscientiousness, openness to experience) of Twitter users (Rangel et al., 2015) applied two types of features.']],\n",
       "  [['For example, such data has been utilized by several studies and evaluations devoted to automatic personality profiling, such as TwiSty (Verhoeven, Daelemans, and Plank 2016) or PAN (Rangel et al. 2015).']],\n",
       "  [['A huge effort has been made in developing content-based classification systems for author profiling (Rangel et al., 2015).'],\n",
       "   ['This result confirms those obtained in the author profiling challenge PAN (Rangel et al., 2015), where the best systems used contentbased features (bag of words, TF-IDF n-grams, etc.']],\n",
       "  [['Automatic methods for personality trait recognition have been studied for a while in natural language processing Automatic methods for personality trait recognition have been studied for a while in natural language processing [32], [33], [34], [35], [36], [37].']],\n",
       "  [['Numerous works on the topic have been published based on the results of the shared Author Profiling Tasks at digital text forensics events by PAN initiative [2,5,7,[27][28][29][30].']],\n",
       "  [['In 2013 In 2013 [10], 2014 [11] and 2015 [12] PAN competition age and gender profiling was done on the English and Spanish datasets with the traditional supervised machine learning approaches: Logistic Regression, Random Forest, SVMs, etc.']],\n",
       "  [['We used the 2015 Author Profiling challenge dataset (PAN 2015) We used the 2015 Author Profiling challenge dataset (PAN 2015) (Rangel et al. 2015), which includes user tweets in four languages -English (en), Spanish (es), Italian (it) and Dutch (nl), where the personality labels were obtained via self-assessment using the BFI-10 item personality questionnaire (Rammstedt and John 2007).']],\n",
       "  [['Apart from improving the model and testing with different data sources, such as PAN-AP-15 dataset [37] and that of Rangel et al.']],\n",
       "  [['Age classes included a gap in between: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27), 30s (33-48).'],\n",
       "   ['The best results (above 80% accuracy) were obtained on English data [24].']],\n",
       "  [['For our text approach, we used the database DBT coming from PAN 2015 [10] that gathers 152 users (in the english dataset) with 100 tweets each and contains labels for age and gender for each user.']],\n",
       "  [[', 2005;Mairesse and Walker, 2006;Rangel et al., 2015) as well as political ideology (Koppel et al.'],\n",
       "   ['As a result of this thesis work, the following list the papers derived from this research: For this competition, we focus on the representation of the documents, to improve the representation of tweets for the Author Profiling task As a result of this thesis work, the following list the papers derived from this research: For this competition, we focus on the representation of the documents, to improve the representation of tweets for the Author Profiling task (Rangel et al., 2015;√Ålvarez-Carmona et al.']],\n",
       "  [['State-of-the-art gender prediction on Twitter for English, the most common platform and language used for this task, is approximately 80-85% (Rangel et al., 2015;Alvarez-Carmona et al.'],\n",
       "   ['While Italian has featured in multi-lingual gender prediction at PAN While Italian has featured in multi-lingual gender prediction at PAN (Rangel et al., 2015), this is the first task that addresses author profiling for Italian specifically, within and across genres.'],\n",
       "   ['Compared to previous results reported for gender detection on Twitter in Italian, as obtained at the 2015 PAN Lab challenge on author profiling (Rangel et al., 2015), and on the TwiSty dataset (Verhoeven et al.']],\n",
       "  [['Unlike Facebook Unlike Facebook [10] and Twitter [11][12][13][14], there is only a handful of studies dissecting age demographics across cQA services [2,15].'],\n",
       "   ['html (accessed on 1 February 2021)) in Twitter, blogs and social media [11][12][13][14].']],\n",
       "  [['As part of the PAN at CLEF initiative [1] there have been multiple author profiling challenges in the past years, which generated a substantial body of research in this field, summarized in [18], [20] and [19].']],\n",
       "  [[', , 2014;;Rangel et al., 2015).']],\n",
       "  [['Another example of personality-annotated datasets of posts from social networks is the PAN-AP-2015 corpus [94], consisting of Twitter posts in English, Spanish, Italian, and Dutch from users who also took the BFI test.']],\n",
       "  [[', 2019) and evaluate it on an existing personality trait corpus (Rangel et al., 2015).'],\n",
       "   ['The data for the PAN-author-profiling shared task in 2015 has been collected in a similar way The data for the PAN-author-profiling shared task in 2015 has been collected in a similar way (Rangel et al., 2015).'],\n",
       "   ['However, it should be noted that the concepts that appear to be more challenging in our setup show also lower evaluation measures in related work (see for instance Table 3 in Rangel et al., 2015, note that their evaluation measure is an RSME, lower is therefore better).']],\n",
       "  [[', 2015;Rangel et al., 2015;Buraya et al.']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   [', 119 , while three 34,49,66 used data sets that were created for the PAN-CLEF author profiling shared tasks [103][104][105] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [['‚Ä¢ PAN2015 (Rangel Pardo et al. 2015): collected from the data science competition PAN20152 and includes four languages datasets.']]],\n",
       " [],\n",
       " [[['We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset [1], which is one of the largest datasets for multi-label text classification, and label hierarchies in the 2015 MeSH vocabulary.']],\n",
       "  [['BioASQ BioASQ [132,1,10,11] is a benchmark challenge which ran until September 2015 and consists of semantic indexing as well as an SQA part on biomedical data.']],\n",
       "  [['This approach has been shown to be highly successful in the recent BioASQ 2 challenge evaluations [44][45][46] and has also been adopted by many others [47,48].'],\n",
       "   ['In 2014, the BioASQ challenge task In 2014, the BioASQ challenge task [45] ran for six consecutive periods (batches) of 5 weeks each.']],\n",
       "  [['In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ [16][17][18], a challenge on large-scale biomedical semantic indexing and question answering, held an international competition from 2013 to 2017.']],\n",
       "  [['Organizers of this challenge provided a largescale question answering competition, in which the systems are required to cope with all stages of a question answering task, including the retrieval of relevant articles and snippets as well as the provision of natural language answers [30,31].']]],\n",
       " [[['There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].']]],\n",
       " [[[\"Today's artificially intelligent agents are good at answering factual questions about our world Today's artificially intelligent agents are good at answering factual questions about our world [9,15,41].\"],\n",
       "   ['There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].']],\n",
       "  [['Datasets: We manually annotate superlative expressions from QALD-4 evaluation dataset (Unger et al., 2014) and TREC QA (2002QA ( , 2003) ) datasets (NIST, 2003), and guarantee that all the labeled superlative instances can be grounded to gradable Freebase predicates.']],\n",
       "  [['QALD-5 questions were compiled from the QALD-4 training and test questions, slightly modified in order to account for changes in the DBpedia dataset [107].']]],\n",
       " [[['In 2015 the MS interface was updated and re-designed according to previous results and user comments In 2015 the MS interface was updated and re-designed according to previous results and user comments [2].']],\n",
       "  [['The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF [8], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.'],\n",
       "   ['This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( [5,6]) and is closely related to the Interactive Track of the CLEF Social Book Search Lab of 2015 and 2016 [7,8].']],\n",
       "  [['Efforts have been undertaken to transfer this paradigm to less systems-focused research, including the TREC Interactive and Session tracks [12], the INEX Interactive track [14,16], the Interactive Social Book Search track [6,7,11], and the RePAST archive [5].']],\n",
       "  [['The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab [4,5,6], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.']]],\n",
       " [],\n",
       " [[['The requests for news items on a news aggregator site is sometimes greater than 100/sec and the expected response should preferably be sent within 100 ms in order to provide news in real-time (Kille et al. 2017).']],\n",
       "  [['In the 2017 edition of NewsREEL 87 participants are registered [92], and two systems achieved CTRs higher than 2% in the online evaluation task.']]],\n",
       " [[['The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors.'],\n",
       "   ['The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors.']],\n",
       "  [['(Note however that the LifeCLEF bird identification challenge now includes several hours of soundscape recording [Goeau et al., 2017]).']],\n",
       "  [['45 [23], which was improved to 0.']],\n",
       "  [[', Lasseck, 2013;Murcia and Paniagua, 2013;Go√´au et al., 2014;Stowell et al.']]],\n",
       " [[['PlantCLEF 2017 PlantCLEF 2017 [43] addressed a practical problem of training a very fine grained classifier (10,000 species) from data with noisy labels: Besides 256 thousand labelled images in the \"trusted\" training set, the organizers also provided URLs to more than 1.'],\n",
       "   ['This was also the case of the PlantCLEF challenges [37,38,43], where the deep learning submissions [41,42,65,66] outperformed combinations of hand-crafted methods significantly.'],\n",
       "   ['With large amounts of training data, state-of-the-art convolutional neural network architectures achieve the best results on such tasks, as validated by results of the recent PlantCLEF challenges [38,43].'],\n",
       "   ['That was demonstrated by the results of the recent PlantCLEF challenges [38,43], where the proposed deep learning methods performed competitively, finishing among the top 3 teams in both 2016 and 2017.']],\n",
       "  [['Only a few studies train CNN classifiers on large plant image datasets, demonstrating their applicability in automated plant species identification systems [68].']],\n",
       "  [['As training data, we provided all the previous datasets used during the previous PlantCLEF challenge [3].']],\n",
       "  [[', 10,000 in PlantClef [6]) is very small in comparison with the number of plant species on the earth (e.'],\n",
       "   ['Some famous networks such as AlexNet [20], GoogLeNet [25], VGG [24] have also been applied for plant identification, especially in PlantClef competition from 2014 to 2017 and have obtained higher results compared to traditional methods based on hand-designed features [6], [14], [17], [26], [27].']],\n",
       "  [['[13,1,17,16,4,5,11]) and it is still attracting much research today, in particular in deep learning [3,6,14].']],\n",
       "  [['For instance in the LifeCLEF competition, which attempts to classify plants using images of different parts such as the leaves, the fruit, or the stem, the best performing methods all employed deep learning [17], [18], [19], [20].']],\n",
       "  [['Examples from the field of vegetation science are apps like Flora Incognita or Pl@ntNet, which assign a species name to plant photographs 12,16,17 .']],\n",
       "  [['PlantCLEF 2017 PlantCLEF 2017 [5] was a plant species recognition challenge organized as part of the LifeCLEF workshop [9].']],\n",
       "  [['To our knowledge, research on automated identification of organisms using deep learning, although an active research area ( [10][11][12][13][14][15]), has not addressed the issue when only small datasets are available.']],\n",
       "  [['This area continues to expand rapidly, particularly due to recent advances in deep learning [25,28,31,42,[45][46][47].']],\n",
       "  [['To obtain an accurate flower identity descriptor ùêπ ùëì ùëñùëë , we adopt the EfficientNet [57] and pretrain it on LifeCLEF2021 Plant Identification [5,[23][24][25].']],\n",
       "  [['Fourthly, we assessed the effect on predictive performance of an ensemble approach combining different CNN models (setup 4) 22 .'],\n",
       "   ['Given that we restricted the number of images per species to a maximum of 8, while successful deep learning-based plant species identification usually requires thousands of images 12,22 , it seems very unlikely that the models inferred traits from species-specific plant features visible in the imagery.'],\n",
       "   ['in plant species identification tasks 22 .'],\n",
       "   ['Former studies revealed their applicability in vegetation science tasks such as plant species identification using plant images 12,22,48 and identification of plant communities utilising imagery from unmanned aerial vehicles (UAV) 11 .']],\n",
       "  [['The same holds for the problem of fine-grained visual categorization (FGVC), where datasets and challenges like PlantCLEF [14][15][16], iNaturalist [17], CUB [18], and Oxford Flowers [19] have triggered the development and evaluation of novel approaches to fine-grained domain adaptation [20], domain specific transfer learning [21], image retrieval [22][23][24], unsupervised visual representation [25,26], few-shot learning [27], transfer learning [21] and prior-shift [28].']],\n",
       "  [['To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data [7,8] and with learning to classify ImageNet [19] classes from a long-tailed noisy training dataset downloaded from the web, Webvision 1.']],\n",
       "  [[', 2017;Go√´au et al., 2017).']],\n",
       "  [['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']],\n",
       "  [['5% accuracy on a far more complex task encompassing 10,000 plant species, characterized by imbalanced, heterogeneous, and noisy visual data [9].']],\n",
       "  [['For the local datasets, we utilize several datasets to represent the personalized local data, including The Oxford-IIIT Pet (Pet) [50], CUB-200-2011 Bird (Bird) [51], and PlantCLEF 2017 (Plant) [52].'],\n",
       "   ['We select the Plant [52] and Bird datasets [51], as these datasets contain the smallest and largest number of classes respectively.']],\n",
       "  [['Powered by the high precision achieved in recent years by ML models, they focus instead on the generation of global models based on extreme datasets, consisting of millions of images representing tens of thousands of individual species (for comparison, it is estimated that the world has ‚àº300K plant species), as exemplified by the iNat Challenge [26] and PlantCLEF/LifeCLEF [27,28].']]],\n",
       " [[[', author obfuscation [3,4,12,15]) used primarily term frequencies [5,9] and features from stylometry [8,9,18].']],\n",
       "  [[', whether the semantics of the original text are preserved) (Hagen et al., 2017).']]],\n",
       " [[['In 2017 [14] two more languages (i.']],\n",
       "  [[', Face++, but discarded these since GITHUB profile photos are scarcely available; and tools that can infer gender from text [61], but discarded these since we have a very limited amount of text for each user -mostly commit messages, which are usually too short to provide enough information.']],\n",
       "  [['2017) While deep learning approaches are gradually taking over different areas of NLP, the best approaches to AP still use more traditional classifiers and require extensive feature engineering (Rangel, Rosso, Potthast et al. 2017).']],\n",
       "  [['To evaluate the competitiveness of our proposed method, we compare its performance with the best participating systems at the author profiling shared task of PAN 2017 (Rangel et al. 2017).'],\n",
       "   ['2017), as well as in the Arabic subtask of the Author Profiling shared task (Rangel et al. 2017) at PAN 2017 (20 teams).'],\n",
       "   ['More information on this corpus is available in the shared task overview paper (Rangel et al. 2017).'],\n",
       "   ['In Table 5, we compare the LDSE results to the average results of the systems at PAN, as reported in (Rangel et al. 2017).']],\n",
       "  [['Age classes included a gap in between: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27), 30s (33-48).'],\n",
       "   ['The lowest results were obtained for Arabic with an accuracy of 80% for gender and 83% for language variety identification [23].']],\n",
       "  [['This traditional techniques try to analyze the semantics and relationship between words [4,7] using techniques like word2vec, bag of words or TF-IDF [8].']],\n",
       "  [['Features used in dialect identification problems are content-based and style-based features Features used in dialect identification problems are content-based and style-based features [8].'],\n",
       "   ['In our research, we used training dataset from PAN conference 2017 In our research, we used training dataset from PAN conference 2017 [8].']],\n",
       "  [[', 2015;Rangel et al., 2017;Basile et al.']],\n",
       "  [['Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.']],\n",
       "  [['In [12,13] the authors used Term Frequency-Inverse Document Frequency (TF-IDF) to extract features from tweets in the PAN17 corpus [16], and reported accuracies for gender classification around 81%.'],\n",
       "   ['In [12,13] the authors used Term Frequency-Inverse Document Frequency (TF-IDF) to extract features from tweets in the PAN17 corpus [16], and reported accuracies for gender classification around 81%.'],\n",
       "   ['We are particularly working with the Spanish data of the corpus, namely PAN-CLEF 2017 We are particularly working with the Spanish data of the corpus, namely PAN-CLEF 2017 [16].']],\n",
       "  [['Essa base, rotulada com o g√™nero de cada autor, foi utilizada na tradicional competic ¬∏√£o PAN-CLEF [Rangel et al., 2017] promovida em 2017.']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [[', 2016  (Rangel et al., 2017).']],\n",
       "  [[', 2017;Rangel et al., 2017).'],\n",
       "   [', 2017;Rangel et al., 2017).'],\n",
       "   ['We also evaluated the performance of the classifier on three datasets, annotated with English varieties: 1) the web corpora GloWbE (Davies, 2013) and NOW (Davies, 2016), 2) the manually-annotated news DSL-TL dataset, and 3) the Twitter PAN17 dataset (Rangel et al., 2017).'],\n",
       "   ['Similar results were obtained on the Twitter dataset 7 from the PAN 2017 shared task on author profiling Similar results were obtained on the Twitter dataset 7 from the PAN 2017 shared task on author profiling (Rangel et al., 2017).']],\n",
       "  [['The PAN 2015, 2016, 2017 and 2018 datasets The PAN 2015, 2016, 2017 and 2018 datasets [19,20] consisting of tweets in different languages.']],\n",
       "  [['Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions [11,18,19], require manual vetting, and often need to be reconstructed from pointers to proprietary APIs (e.']]],\n",
       " [[['Seit 2015 ist DBIS im Organisationsteam des international angesehenen PAN-Workshops vertreten, der j√§hrliche Tasks rund um das Thema Textanalyse veranstaltet [17,22].']],\n",
       "  [['Recently, a shared task has been organised to address this research direction (Tschuggnall et al. 2017).']],\n",
       "  [['Clustering segments according to observed critical values Author Clustering typically follows the style breach detection stage and employs pairwise comparisons of passages identified in the previous stage to group them by author [247].'],\n",
       "   ['Widely analyzed lexical features include character n-grams, word frequencies, as well as the average lengths of words, sentences, and paragraphs [247].'],\n",
       "   ['Widely analyzed lexical features include character n-grams, word frequencies, as well as the average lengths of words, sentences, and paragraphs [247].']],\n",
       "  [['In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection [9], and Style Change Detection [7].'],\n",
       "   ['For the style breach detection sub-task, we chose as a baseline model the approach of Khan For the style breach detection sub-task, we chose as a baseline model the approach of Khan [8] which is state-ofthe-art on PAN 2017 Competition [9].']],\n",
       "  [['Following the practice of customisation of document length for the topic modelling (Schofield, Magnusson, and Mimno 2017) and the author style detection (Tschuggnall et al. 2017), we defined one page as the document unit, and splitted the entire scope of 72 volumes into 52396 pages.']],\n",
       "  [['In 2017, we asked participants to detect whether a given document is multi-authored and if this is indeed the case, to determine the positions at which authorship changes [28].']],\n",
       "  [['O trabalho em Tschuggnall et al. (2017) apontou resultados com patamares diferentes para um mesmo m√©todo, quando aplicado a c√≥rpus em diferentes l√≠nguas.'],\n",
       "   ['‚Ä¢ Liga√ß√£o autoral: √© uma vers√£o mais flex√≠vel do agrupamento autoral, em que pares de documentos s√£o ordenados pela confian√ßa de pertencerem ao mesmo autor ‚Ä¢ Liga√ß√£o autoral: √© uma vers√£o mais flex√≠vel do agrupamento autoral, em que pares de documentos s√£o ordenados pela confian√ßa de pertencerem ao mesmo autor (TSCHUGGNALL et al., 2017).']],\n",
       "  [['See [19] for a summary of previous work on style breach detection and related tasks.'],\n",
       "   ['We use the dataset from the PAN-2017 competitionWe use the dataset from the PAN-2017 competition11  [19], which consists of 187 documents each containing 1,000-2,400 word tokens.']]],\n",
       " [[[\"This year's project was the participation to the CLEF eHealth challenge, more precisely task 1: multilingual Information Extraction [9] which concerns the automatic coding of death certicates.\"]],\n",
       "  [['In this paper, we report the experimental results of the IMS group that participated for the first time to the CLEF eHealth Lab In this paper, we report the experimental results of the IMS group that participated for the first time to the CLEF eHealth Lab [8], in particular to Task 1: \"Multilingual Information Extraction -ICD10 coding\" [11].']],\n",
       "  [['The objective of CLEF eHealth 2017 Task 2 The objective of CLEF eHealth 2017 Task 2 [69] is to annotate death certificates with ICD-10 codes both in French and in American English.']],\n",
       "  [[', 2015), and a French corpus of free-text death certificates (N√©v√©ol et al., 2017(N√©v√©ol et al.']],\n",
       "  [['Recently [3,4,5], the C√©piDC task consisted in extracting ICD-10 codes from death reports in several languages (French in 2016, French and English in 2017, French, Hungarian and Italian in 2018).']],\n",
       "  [['C√©PIDC C√©PIDC [34] (the latter in the context of the participation of SIFR Annotator in the CLEF eHealth 2017 challenge [45]) 13 .'],\n",
       "   ['-Death certificates obtained from the C√©piDC causes of death corpus made available to the participants of CLEF eHealth 2017 task 1 -Death certificates obtained from the C√©piDC causes of death corpus made available to the participants of CLEF eHealth 2017 task 1 [34].']],\n",
       "  [[', 2018;N√©v√©ol et al., 2017;Poibeau et al.']],\n",
       "  [['The CLEF eHealth challenges (N√©v√©ol et al., 2017(N√©v√©ol et al.']],\n",
       "  [[', 2016) and CLEF (N√©v√©ol et al., 2017).']]],\n",
       " [[['Recently, BMI and a method independently derived from CAL have produced results that compare favorably to competing methods for systematic review (Kanoulas et al, 2017;Cormack and Grossman, 2017b;Baruah et al, 2016).']],\n",
       "  [['Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant [10][11][12][13].']],\n",
       "  [['A number of solutions have arisen to address the amount of time spent screening documents, including: screening prioritisation (which seeks to re-rank the set of retrieved documents to show more relevant documents first, thus starting the full-text screening earlier), and stopping estimation (which seeks to predict at what point continuing to screen will no longer contribute gain) A number of solutions have arisen to address the amount of time spent screening documents, including: screening prioritisation (which seeks to re-rank the set of retrieved documents to show more relevant documents first, thus starting the full-text screening earlier), and stopping estimation (which seeks to predict at what point continuing to screen will no longer contribute gain) [25,26,40].'],\n",
       "   ['The empirical results obtained on the CLEF Technology Assisted Review datasets [25,26] show that CLF significantly outperforms existing state-of-the-art methods that consider similar settings, including the ranking method currently used in PubMed (a popular database to search for literature for systematic reviews).'],\n",
       "   ['The CLEF Technology Assisted Reviews (TAR) track The CLEF Technology Assisted Reviews (TAR) track [25,26] considers both screening prioritisation and stopping prediction tasks.'],\n",
       "   ['Empirical evaluation is conducted on the CLEF TAR 2017 and 2018 collections Empirical evaluation is conducted on the CLEF TAR 2017 and 2018 collections [25,26].'],\n",
       "   ['Participant runs are chosen for comparison if they are a fully automatic, unsupervised method, which does not use the training data or explicit relevance feedback, and do not set a threshold (as categorised in the TAR overview papers [25,26]).']],\n",
       "  [['The most common method for developing a search strategy is the conceptual method The most common method for developing a search strategy is the conceptual method [2,12] (although other methods have been investigated that do not produce a Boolean query [6,7]).'],\n",
       "   ['Furthermore, we evaluate this method on a large set of 40 systematic reviews from a collection used for the evaluation of automation methods in this context [6] and further replicate a small study by Hausner et al.'],\n",
       "   ['We evaluate the computational method for objectively deriving systematic review search strategies on the CLEF 2018 Technology Assisted Reviews (TAR) collection We evaluate the computational method for objectively deriving systematic review search strategies on the CLEF 2018 Technology Assisted Reviews (TAR) collection [6].']],\n",
       "  [['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].'],\n",
       "   ['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].']],\n",
       "  [['-We validate the effectiveness of the proposed framework and provide a detailed analysis of its components on various datasets including the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the Text Retrieval Conference (TREC) Total Recall datasets [16], and the TREC Legal datasets [13].'],\n",
       "   ['Three datasets have been released-namely, EMED 2017 Three datasets have been released-namely, EMED 2017 [20], EMED 2018 [21,34], and EMED 2019 [22].'],\n",
       "   ['[9] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [20,21,22].'],\n",
       "   ['To examine the effectiveness of the proposed framework, we compared it against the Knee, Target, SCAL, SD-training, and SD-sampling methods and provided detailed analysis on various datasets including the CLEF Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the TREC Total Recall datasets [16], and the TREC Legal datasets [13].']],\n",
       "  [['[1,2,6].'],\n",
       "   ['This approach was demonstrated to be robust by evaluating it using rankings of varying effectiveness produced by participants from the CLEF eHealth task ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].'],\n",
       "   ['The approach was demonstrated to be robust on a set of rankings of varying effectiveness produced by participants in a shared task [6].'],\n",
       "   ['Following Following [13], we utilise the publicly available submissions to the CLEF 2017 e-Health Lab Task 2 ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].']],\n",
       "  [['HRR tasks include electronic discovery in the law (eDiscovery) [3], systematic review in medicine [22][23][24]47], document sensitivity review [34], online content moderation [55], and corpus annotation to support research and development [60].']],\n",
       "  [['loss er is introduced for the first time in [37] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [82,84,86].']],\n",
       "  [['Other applications include open government document requests Other applications include open government document requests [3] and systematic review in medicine [14][15][16]32].']],\n",
       "  [['Unfortunately, it is not clear these measures were actually used: neither track overview discusses results on them [24,25].'],\n",
       "   ['A similar asymmetry can occur in systematic review in medicine [24,25].']],\n",
       "  [['TAR evaluation conferences TAR evaluation conferences [17,[23][24][25]38] have emphasized interventional stopping rules, i.']],\n",
       "  [[\"CLEF TAR 2018 [11] This collection adds 30 diagnostic test accuracy systematic reviews as topics to the existing 2017 collection; however, it also removes eight because they are not 'reliable for training or testing purposes.\"],\n",
       "   [\"This ranking of studies has come to be known as 'screening prioritisation', as popularised by the CLEF TAR tasks which aimed to automate these early stages of the systematic review creation pipeline [9,11,10].\"],\n",
       "   ['We reproduced the SDR for systematic reviews method by Lee and Sun We reproduced the SDR for systematic reviews method by Lee and Sun [16] on all the available CLEF TAR datasets [9,11,10].']],\n",
       "  [['We use topics from the CLEF TAR task from 2017, 2018, and 2019 We use topics from the CLEF TAR task from 2017, 2018, and 2019 [11][12][13].']],\n",
       "  [['In addition, several recent challenges, such as TREC [17,24] and CLEF eHealth task 2 [25][26][27], further promote the development of automatic document screening.'],\n",
       "   ['It has been shown that this active learning solution outperforms its counterparts in many real-world cases [17,[24][25][26][27].']],\n",
       "  [['We explore initial experiments on the CLEF TAR 2019 dataset [16].'],\n",
       "   ['The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings [16,27,34], often sourced at the title and abstract screening level, rather than at the full-text level.'],\n",
       "   ['Cost-based and economic-based metrics are also used, especially in the context of the query formulation task in the CLEF TAR shared task [14][15][16], e.'],\n",
       "   ['Traditionally, retrieval was conducted at the level of publications [14][15][16].'],\n",
       "   ['We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets [16].'],\n",
       "   ['However, there are several other types of systematic reviews, such as diagnostic test accuracy reviews, prognostic reviews, and qualitative research reviews, each of which presents unique challenges for automation and evaluation [16].']],\n",
       "  [['The TREC Legal [1,6,18,20], TREC Total Recall [10], and the CLEF eHealth Technology-Assisted Review Tasks [11,12] have provided researchers with access to datasets and standardised evaluation methods.']],\n",
       "  [['These methods are evaluated and compared against alternative approaches on a range of datasets used to evaluate TAR approaches: the CLEF Technology-assisted Review in Empirical Medicine [30][31][32], the TREC Total Recall tasks [24], and the TREC Legal Tasks [17].'],\n",
       "   ['Its development was informed by experience from TREC Total Recall tracks [24,49] and also adopted by the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32].'],\n",
       "   ['We follow the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32] and Li and Kanoulas [39] in choosing a = 1 and b = 100.']],\n",
       "  [['We also include in our experiments three corpora from the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets from the years 2017, 2018, and 2019 [24][25][26].']],\n",
       "  [['Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews [13,[16][17][18], satisfying legal disclosure requirements [2,12,23], social media content moderation [38] and test collection development [22].'],\n",
       "   ['CLEF 2017/2018/2019 [16][17][18]: Collections of systematic reviews produced for the Conference and Labs of the Evaluation Forum (CLEF) 2017, 2018, and 2019 e-Health lab Task 2: Technology-Assisted Reviews in Empirical Medicine.']],\n",
       "  [['We rely on the CLEF-TAR 2017, 2018 and 2019 Subtask 2 datasets [22][23][24] (abbreviated as .']]],\n",
       " [[['In [11], one of the shared tasks involved the retrieval of medical forum posts related to the search queries provided.']]],\n",
       " [],\n",
       " [[['We use the corpus provided for the MC2 CLEF 2017 labWe use the corpus provided for the MC2 CLEF 2017 lab1 which contains 70 million tweets [6].']]],\n",
       " [],\n",
       " [[['As a preliminary step, we apply our preliminary version As a preliminary step, we apply our preliminary version [15] of the proposed baseline search engine to solve the problems in Image-CLEFlifelog 2017 [3] of ImageCLEF 2017 [10].']],\n",
       "  [['Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog [25,26], NTCIR 13 -Lifelog 2, and the LifeLog task [13] at ImageCLEF 2017 [29], which aim to bring the attention of personal live archive analytics to a wide audience and to motivate research into some of the key challenges of the field.'],\n",
       "   ['Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog [25,26], NTCIR 13 -Lifelog 2, and the LifeLog task [13] at ImageCLEF 2017 [29], which aim to bring the attention of personal live archive analytics to a wide audience and to motivate research into some of the key challenges of the field.'],\n",
       "   ['For more details of these campaigns, please see in [13,25,26].']],\n",
       "  [['As such, one can only resort to visual features and sensor metadata, and unsupervised techniques such as K-Means [8] and probabilistic models [11].'],\n",
       "   ['However, the size of the publicly available LTR datasets is very limited: 170 days in CLEF [8] and NTCIR [19], and 66 in EDUB-Seg [11] and EDUB-SegDesc [7], spanning a total of 2, 700 hours and 261, 845 images.'],\n",
       "   ['To ensure that the model is not biased toward this dataset, a 20% of both CLEF [8] and NT-CIR [19] is also included in the training set.']],\n",
       "  [['Based on the collections from NTCIR-12 and NTCIR-13, rigorous comparative benchmarking initiatives have been organised: the NTCIR 12 -Lifelog [9], and ImageCLEFlifelog2017 [3] exploited the NTCIR-12 collection and NTCIR-13 Lifelog 2 [10], ImageCLEFlifelog2018 [4] were proposed based on the NTCIR-13 collection.']],\n",
       "  [['The main goal of the Lifelog task since its first edition The main goal of the Lifelog task since its first edition [6] has been to advance the state-of-the-art research in lifelogging as an application of information retrieval.']],\n",
       "  [['Other publicly available datasets of egocentric image sequences, such as CLEF Other publicly available datasets of egocentric image sequences, such as CLEF [59], NTCIR [60] and the more recent R3 [12], do not have ground truth event segmentations.']],\n",
       "  [['In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].']],\n",
       "  [['In 2017, the Second Workshop on Lifelogging Tools and Applications was organized simultaneously with the lifelog evaluation tasks, NTCIR-13 Lifelog-2 Task [76] and ImageCLEFlifelog 2017 Task [81].']],\n",
       "  [['Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].']],\n",
       "  [['The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [23] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [24][25][26]55] and the ImageCLEF Lifelog tasks [12][13][14]46].']]],\n",
       " [[[\"For instance, one important task was establishing the ImageCLEF data set, which allowed users to determine TB type and treatment resistance using coaxial tomography images (28,30); researchers have also used images from radiography to support health professionals' decision making (31)(32)(33).\"]]],\n",
       " [],\n",
       " [],\n",
       " [[['In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included [77,78].'],\n",
       "   ['The majority of these approaches employ unsupervised learning [77,78].']],\n",
       "  [['Accordingly, recent evaluation forums have focused on detecting people suffering from depression through the analysis of their social media posts [5][6][7].'],\n",
       "   ['Some other works based on topics have explored the use of resources and techniques such as LIWC and LDA [26][27][28].'],\n",
       "   ['The search was focused on: (i) the number of trees (10,20,30,40,50) and (ii) their depth (3,5,6,9,10).']],\n",
       "  [['In the context of the CLEF eRisk Pilot Task 2017In the context of the CLEF eRisk Pilot Task 20174 on early risk detection on the Internet [Losada et al. 2017], the work in [Almeida et al.'],\n",
       "   ['The experiments were conducted with the dataset initially provided by The experiments were conducted with the dataset initially provided by [Losada and Crestani 2016] and published as part of the CLEF eRisk 2017 Task [Losada et al. 2017].'],\n",
       "   ['Considering a binary decision (d) made by an early risk detection system at k, ERDE o is defined as: where, following [Losada et al. 2017], c f n = c tp = 1 and c f p is set according to the proportion of positive cases in the test set (0.'],\n",
       "   ['Considering a binary decision (d) made by an early risk detection system at k, ERDE o is defined as: where, following [Losada et al. 2017], c f n = c tp = 1 and c f p is set according to the proportion of positive cases in the test set (0.']],\n",
       "  [[', 2022;Losada et al., 2017Losada et al.'],\n",
       "   [', 2022) for our experiments and also verify our results with a smaller Reddit eRisk (Losada et al., 2017(Losada et al.'],\n",
       "   ['Reddit eRisk dataset: To further add to our findings, we also included the eRisk 2018 dataset (Losada et al., 2017(Losada et al.'],\n",
       "   ['We combined users and their instances from both the training set (which is from the eRisk 2017 task (Losada et al., 2017)) and the test set (from the eRisk 2018 task (Losada et al.']]],\n",
       " [],\n",
       " [],\n",
       " [[[\"El objetivo del iCLEF'2002 El objetivo del iCLEF'2002 (Gonzalo and Oard, 2002) fue proporcionar un marco de referencia com√∫n para realizar experimentos comparando dos sistemas de recuperaci√≥n de informaci√≥n transling√ºe que permitan a un usuario que desconoce el idioma de los documentos realizar una expansi√≥n interactiva de la consulta, una selecci√≥n interactiva de documentos (al igual que el a√±o anterior), o ambas opciones a la vez.\"]],\n",
       "  [['(Oard & Gonzalo, 2002)).']],\n",
       "  [['The interactive CLEF experimental framework The interactive CLEF experimental framework (Gonzalo & Oard, 2002) was followed, but additional measurements (both objective and subjective) were recorded.']]],\n",
       " [[['We decided to test the BoC representation in the Geographic Information Retrieval (GIR) environment using the CLEF collection for evaluating the Geo-CLEF task [10,11].']],\n",
       "  [['It operated from 2005 to 2008 [11,12,21,22].']],\n",
       "  [['Recent development on GIR systems Recent development on GIR systems [6] evidence that: i) traditional IR systems are able to retrieve the majority of the relevant documents for most queries, but that, ii) they have severe difficulties to generate a pertinent ranking of them.'],\n",
       "   ['GIR has been evaluated at the CLEF forum GIR has been evaluated at the CLEF forum [3] since year 2005, under the name of the GeoCLEF task [6].']],\n",
       "  [['To date, most GIR systems focus on individual map features, with particular emphasis on text-based retrieval [4].']],\n",
       "  [['We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval [6], 2008 Geographic IR [15], 2008 Image Retrieval [2], and 2008 Robust IR [1].']],\n",
       "  [['Another witness of this trend are geographic information retrieval systems [5,12,17,19] and in particular local search services, such as Google Maps1 , Yahoo!']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['For example, the GeoCLEF 2007 corpus consisted of three sub-corpora in English, German, and Portuguese, each with around 200,000 documents [41].'],\n",
       "   ['28 in GeoCLEF 2007 [41] and from 0.'],\n",
       "   [', [21,41,53]).']],\n",
       "  [['The GeoCLEF search task examined geographic search in text corpus [18].']],\n",
       "  [[', 2006), -GeoClef : Geographic Cross Language Evaluation Forum (Mandl et al., 2009), -ACE : Automatic Content Extraction program (Strassel et al.']],\n",
       "  [[', 2006), -GeoClef : Geographic Cross Language Evaluation Forum (Mandl et al., 2009), -ACE : Automatic Content Extraction program (Strassel et al.']],\n",
       "  [['2015), locations in geographical IR (Mandl et al. 2009) or timestamps in time-aware IR (Li and Croft 2003).'],\n",
       "   ['2005;Hashemi and Kamps 2014;Macdonald et al. 2015).'],\n",
       "   ['A lot of other retrieval research sub-fields such as geographical IR (Mandl et al. 2009), image retrieval (Villegas et al.'],\n",
       "   ['Let us explore multimodal document collections such as used in GeoCLEF Let us explore multimodal document collections such as used in GeoCLEF (Mandl et al. 2009) or in the social book search lab (Bogers et al.'],\n",
       "   ['For the experiments with the geographical modality, we use the topics and collection of the GeoCLEF 2008 For the experiments with the geographical modality, we use the topics and collection of the GeoCLEF 2008 (Mandl et al. 2009) monolingual English search task.']],\n",
       "  [['Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.'],\n",
       "   ['This task was organized by Microsoft Research Asia (Mandl et al., 2007).'],\n",
       "   ['Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.']],\n",
       "  [[', Geo-CLEF [4]) and they often lack in rich annotations as would be required for the examples above.']],\n",
       "  [['This includes seminal GIR research based on heuristics [5,28,32], combining BM25 ranking together with geo-spatial criteria derived from gazetteers or general knowledge bases, and also more recent GIR/GeoQA methods based on machine learning and, more recently, neural networks.'],\n",
       "   ['As for results, the authors performed tests on data from a previous CLEF competition [28] and noted that using both textual and geographical features yielded the best results in terms of MAP.']],\n",
       "  [['Second, our geoparsing methods to extract place mentions and geocode them to their correct location on Earth would need to be adapted (Mandl et al. 2009).']],\n",
       "  [['But spatial evaluation campaigns like GeoCLEF (Mandl et al., 2007) do not give accurate resources (like polygons) and do not handle French documents.']]],\n",
       " [[['To this end, the goal was to provide a collection of more than 150,000 images; such a collection would be, for instance, much larger than the IAPR TC-12 image collection (Grubinger et al, 2006) that consists of 20,000 photographs and that was, at the time, employed in the ImageCLEF 2008 photo retrieval task (Arni et al, 2009).']],\n",
       "  [['(2008), which was the winning entry of the 2008 Im-ageClef Retrieval challenge (Arni et al., 2008).'],\n",
       "   [', 2008) 35  , 2008), the winners of the challenge (Arni et al., 2008).']],\n",
       "  [[\"La deuxi√®me appel√©e ImageCLEFphoto (Arni et al., 2008) est une t√¢che de recherche d'images bas√©e sur les informations textuelles et visuelles, et propose d'√©tudier les probl√®mes soulev√©s par la diversit√©.\"]],\n",
       "  [['ImageCLEFPhoto used the IAPR TC-12 collection for the past three years [10,11,12], and it was extended to allow diversity measurement, by grouping the relevance judgments of existing topics into clusters that reflect relationships between relevant images in the collection.'],\n",
       "   ['Detail on the processes of topic selection and cluster assessment can be found in [12].']],\n",
       "  [['ImageClef Retrieval evaluations [41,4,2].'],\n",
       "   ['However, methods that combine different modalities can improve retrieval performance on multimodal databases [41,4,2].'],\n",
       "   ['With our re-implementation of [1], using the same features and their weighting w = [1, 2] ‚ä§ and k = 2, we obtain results slightly below the ones reported by ImageCLEF in [4].']],\n",
       "  [['We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval [6], 2008 Geographic IR [15], 2008 Image Retrieval [2], and 2008 Robust IR [1].']],\n",
       "  [['We used six different multimedia corpora, five of which came from TRECVID We used six different multimedia corpora, five of which came from TRECVID [17] and one from ImageCLEF [4].']],\n",
       "  [['This work is a follow up to the study by Villa and Halvey [15], which looked at the effort involved in judging the relevance of text documents.'],\n",
       "   ['The latter three variables were based on the topic classifications defined in the ImageCLEF 2007 [4,5] and allow us to investigate how the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.']],\n",
       "  [['the visual content of images as well as semantic information are available), standard information retrieval techniques have reported very good results for the task of image retrieval [15,16,17].'],\n",
       "   ['At the moment, the image collection has been used for evaluating cross-language TBIR methods, CBIR methods, and methods that combine information from both text and images [16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].']],\n",
       "  [['Thirdly, a search engine should maximize results precision and cover different aspects of the query in the same time [4] but these two measures are often difficult to maximize simultaneously [1].'],\n",
       "   ['They implement a dynamic programming algorithm applied on top of a greedy selection and test their approach on a heterogeneous test database (ImageCLEF 2008 photo retrieval task [1]).']],\n",
       "  [['The IAPR TC-12 photographic collection consists of 60 topics and 20,000 still natural images taken from locations all around the world and including an assorted cross-section of still natural images [11].']],\n",
       "  [['In these challenges, our image and trans-media similarity based retrieval systems demonstrated very good performance (see [14]).']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks.']],\n",
       "  [['A base de imagens utilizada foi a da ImageCLEF Photographic Retrieval Task [Arni et al. 2009], composta por 20.'],\n",
       "   ['2014], para isso avaliou-se os rankings nas primeiras 20 posic ¬∏√µes dentre as 1000 retornadas como proposto pelo desafio em [Arni et al. 2009] al√©m do ranking total.']],\n",
       "  [['Nesse trabalho foi utilizada a ImageCLEF Photographic Retrieval Task collection Nesse trabalho foi utilizada a ImageCLEF Photographic Retrieval Task collection [Arni et al. 2009].']],\n",
       "  [[\"La deuxi√®me appel√©e ImageCLEFphoto (Arni et al., 2008) est une t√¢che de recherche d'images bas√©e sur les informations textuelles et visuelles, et propose d'√©tudier les probl√®mes soulev√©s par la diversit√©.\"]]],\n",
       " [[['More interestingly, the ImageCLEF medical image retrieval task targets modality classification of medical images and have attracted wide attention from the area in recent years [27].']],\n",
       "  [['In addition to images, this collection contains image captions, URLs to the full-text scientific publications from which the images were extracted, their PubMed identifiers from the GoldMiner collection7 , 30 information need requests expressed as image retrieval questions (topics containing text and images), and judgments about the relevance of images retrieved by teams participating in the evaluation [12,13].'],\n",
       "   ['12, which is within the range of visual retrieval results reported for the ImageCLEFmed 2008 medical image retrieval task, and is consistent with the observation that visual retrieval techniques can degrade the overall performance [12].'],\n",
       "   ['The difference in classification precision cannot be explained by the nature of the questions, as the better and worse performing questions were distributed evenly over question categories, complexity levels, and difficulty for retrieval measured by the average Mean Average Precision obtained for these topics in the ImageCLEFmed 2008 evaluation The difference in classification precision cannot be explained by the nature of the questions, as the better and worse performing questions were distributed evenly over question categories, complexity levels, and difficulty for retrieval measured by the average Mean Average Precision obtained for these topics in the ImageCLEFmed 2008 evaluation [12].']],\n",
       "  [['Particularly, the medical image retrieval task of ImageCLEF, presented in (M√ºller et al., 2008), proposes a publicly-available benchmark for the evaluation of several multimodal retrieval systems.']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['In this way, the research study likewise considers comparative label circulations to lift transfer learning, where the comparability of two name disseminations is processed utilizing Jensen Shannon divergence (JSD) [5,18,35,36].'],\n",
       "   [\"There is a trade-off between ''too much capacity'' and ''not enough capacity'' There is a trade-off between ''too much capacity'' and ''not enough capacity'' [36].\"]],\n",
       "  [['Figure 2 shows the scores of our Ambiguous Proximity Distribution Kernel (Ambiguous PDK), along with visual retrieval algorithms submitted by additional groups [62].']]],\n",
       " [[['Evaluation of video retrieval is also very active and standardized, with important contributions from TRECVID,5 videoCLEF [81,82], and MultimediaEval.']],\n",
       "  [['Sound and Vision makes it possible for the wider multimedia community to work on tasks related to the important issue of keyword recommendation through its support of the VideoCLEF video analysis and retrieval benchmark evaluation Sound and Vision makes it possible for the wider multimedia community to work on tasks related to the important issue of keyword recommendation through its support of the VideoCLEF video analysis and retrieval benchmark evaluation [17].']]],\n",
       " [],\n",
       " [[['The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections [244,245].'],\n",
       "   ['First, we note that the results obtained for the query likelihood model are comparable to or better than the mean results of all the participating groups in the respective TREC Genomics [129][130][131] and CLEF Domain-specific tracks [244,245].']]],\n",
       " [[['For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].'],\n",
       "   ['As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.'],\n",
       "   ['The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].']],\n",
       "  [['It was then used for producing reports and overview graphs about the submitted experiments [1,3].']],\n",
       "  [['To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are [12,13].'],\n",
       "   ['This is the case, for example, of the CLEF 2005 multilingual merging track [12], which provided participants with some of the CLEF 2003 multilingual experiments as list of results to be used as input to their merging algorithms.']],\n",
       "  [['Please refer to (Agirre et al., 2008) for a more detailed discussion about this data.']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['After ten year of CLEF, the best bilingual systems went up to about 85%-95% of the best monolingual ones (Agirre et al., 2009;Ferro and Silvello, 2014a) for most language pairs.']],\n",
       "  [['We evaluate the final retrieval models on HC4 We evaluate the final retrieval models on HC4 [26], a newly constructed evaluation collection for CLIR, for Chinese and Persian, NTCIR [31] for Chinese, CLEF 08-09 for Persian [1,14], and CLEF 03 [4] for French and German.'],\n",
       "   ['While this effect deserves further investigation, we note that queries for this collection were originally created in Persian and then translated into English, possibly initially by nonnative speakers [1,14].']]],\n",
       " [],\n",
       " [],\n",
       " [[[\"The TREC Filtering Track [31] and CLEF INFILE [6] focused on the multiple filtering tasks including adaptive filtering, where systems aim to select relevant documents from a stream of incoming documents based on a user's profile.\"]]],\n",
       " [],\n",
       " [[[', , 2004;;Vallin et al., 2005;Magnini et al.']],\n",
       "  [['In the QA and information retrieval domains progress has been assessed via evaluation campaigns (Voorhees and Harman, 2005;Forner et al., 2008;Ayache et al.']],\n",
       "  [['Finally, the question answering field has been developed significantly [2].']]],\n",
       " [[['The test set is constituted from the AVE 2006 The test set is constituted from the AVE 2006 [11] campaign data for French.']],\n",
       "  [['Today, there is also experimental evidence for the claim that question answering can profit from the use of logical subsystems: A comparison of answer validators in the Answer Validation Exercise 2006 found that systems reported to use logic generally outperformed those without logical reasoning [2].']],\n",
       "  [['In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus (Pe√±as et al., 2007).'],\n",
       "   ['(2006); Pe√±as et al. (2007).'],\n",
       "   ['These values are comparable to performance of state-of-the-art RTE systems Pe√±as et al. (2007); Giampiccolo et al.']],\n",
       "  [['This result pushes us to use this system for Answer Validation (Pe√±as et al., 2007).']],\n",
       "  [['We performed the same experiment joining the Answer Validation Exercise4 (AVE) 2006 English data set (Pe√±as et al., 2006) and the Microsoft Research Paraphrase Corpus5 (MSRPC) (Dolan et al.']],\n",
       "  [['The aim of the AVE task was to automatically assess the validity of the answers given by QA systems [9].'],\n",
       "   ['The task of answer validation (AVE) The task of answer validation (AVE) [9] at CLEF was dedicated to validate answers to questions in relation with a justification passage, both provided by QA systems.']],\n",
       "  [['( [5]) give an overview of the first AVE launched during QA@CLEF 2006 campaign.']],\n",
       "  [[', 2008, Rodrigo et al., 2009).']],\n",
       "  [['Many evaluation campaigns and benchmarks are related to textual entailment, as well as paraphrase detection in general, among which PASCAL challenge [13], Answer Validation Exercise [71], the MSRP paraphrase corpus [15] or the SNLI corpus [10].']],\n",
       "  [['Les r√©sultats ont √©t√© √©valu√©s par la pr√©cision, le rappel et la f-mesure qui ont √©t√© calcul√©s de la fa√ßon suivante : pr√©cision = #paires jug√©es OU I correctement #jug√©es comme OU I , rappel = #paires jug√©es comme OU I correctement #paires OU I et f -mesure = 2 * pr√©cision * rappel pr√©cision+rappel 3 Travaux en validation de r√©ponses Les r√©sultats ont √©t√© √©valu√©s par la pr√©cision, le rappel et la f-mesure qui ont √©t√© calcul√©s de la fa√ßon suivante : pr√©cision = #paires jug√©es OU I correctement #jug√©es comme OU I , rappel = #paires jug√©es comme OU I correctement #paires OU I et f -mesure = 2 * pr√©cision * rappel pr√©cision+rappel 3 Travaux en validation de r√©ponses (Pe√±as et al., 2006) pr√©sentent le d√©roulement de la premi√®re campagne AVE.']],\n",
       "  [['Entailment has been also used for zero-shot and few-shot relation extraction [37], answer validation [35], event argument extraction [36], and claim verification in fact checking [16].']]],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['It operated from 2005 to 2008 [11,12,21,22].']],\n",
       "  [['Another witness of this trend are geographic information retrieval systems [5,12,17,19] and in particular local search services, such as Google Maps1 , Yahoo!']],\n",
       "  [['Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.'],\n",
       "   ['This task was organized by Microsoft Research Asia (Mandl et al., 2007).'],\n",
       "   ['Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.']],\n",
       "  [[', 2005) and 2006 (Gey et al., 2006) tracks for crosslanguage geographic information retrieval of the text.']]],\n",
       " [[['the visual content of images as well as semantic information are available), standard information retrieval techniques have reported very good results for the task of image retrieval [15,16,17].'],\n",
       "   ['At the moment, the image collection has been used for evaluating cross-language TBIR methods, CBIR methods, and methods that combine information from both text and images [16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].']],\n",
       "  [['The cross-language image retrieval campaign Image-CLEF The cross-language image retrieval campaign Image-CLEF [5,4] aims to evaluate different approaches of text and content based retrieval methods.']],\n",
       "  [['The International Association of Pattern Recognition (IAPR) TC-12 benchmark dataset The International Association of Pattern Recognition (IAPR) TC-12 benchmark dataset (Grubinger et al, 2006) was created for the cross-language image retrieval track of the CLEF evaluation campaign (ImageCLEF 2006) (Clough et al, 2006).']]],\n",
       " [[['At other times, corpora is automatically obtained (for instance, English and Czech interview recordings of Survivors of the Shoah Visual History Foundation using in CL-SDR 2006 [11] were transcribed using a ASR system with the consequent increase of transcription errors).']],\n",
       "  [['The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) [11,13].']]],\n",
       " [],\n",
       " [],\n",
       " [[['For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].'],\n",
       "   ['As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.'],\n",
       "   ['The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].']],\n",
       "  [['Our experiments and analyses are performed on the following corpora and data sets: The Excite query log (ENEx) of user queries, as distributed in the Pig query log analysis toolOur experiments and analyses are performed on the following corpora and data sets: The Excite query log (ENEx) of user queries, as distributed in the Pig query log analysis tool1 ; the 1M sentence English (EN1M) and 3M sentence German corpus from the Leipzig Corpora Collection2 ; the English and German Wikipedia article names (ENWi) 3 ; and the titles of 160 English and German topics which have been used in ad-hoc retrieval experiments at CLEF from 2003-2006 (see, for example [3]).']],\n",
       "  [['Regarding the evaluation corpus, the document collection to be used is the so-called LA Times 94 (56,472 documents, 154 MB), previously employed in the robust task of the ad hoc track of CLEF 2006 Regarding the evaluation corpus, the document collection to be used is the so-called LA Times 94 (56,472 documents, 154 MB), previously employed in the robust task of the ad hoc track of CLEF 2006 (Di Nunzio et al., 2006), which reused queries from previous CLEF Initiative (2015) events.']]],\n",
       " [[['The known-item document identification approach of Azzopardi and de Rijke ( The known-item document identification approach of Azzopardi and de Rijke ( 2006) has been shown to be effective when used for comparative evaluation of retrieval models (Balog, Azzopardi, Kamps, and de Rijke, 2007;Kim and Croft, 2009;Naji and Savoy, 2011).']]],\n",
       " [[['Several QA reports [6,14] indicate that the translation errors cause an important drop in accuracy for cross-language tasks with respect to the monolingual exercises.']],\n",
       "  [['As it was revealed in the Cross-Language Evaluation Forum (CLEF) 2006 [15], multilingual tracks of IR and QA tasks have been recognized as an important issue in information access.'],\n",
       "   ['The last edition of CLEF (2006) The last edition of CLEF (2006) [15] has confirmed that most of the implementations of current CL-QA systems [5,13,18,20,21]are based on the use of on-line translation services.'],\n",
       "   ['In fact, our precision loss of CL with respect to the monolingual run is around 17% whereas in the English-Spanish QA task at CLEF 2006 [15] the precision on English-Spanish CL-QA task was approximately 50% lower than for the monolingual Spanish task.']],\n",
       "  [['Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 [6].'],\n",
       "   ['Besides, section 5 presents and discusses the results obtained using all official English questions of QA CLEF 2004 [8] and 2006 [6].'],\n",
       "   ['This fact has been confirmed in the last edition of CLEF 2006 [6].'],\n",
       "   ['Nowadays, at CLEF 2006 [6], three different approaches are used by CL-QA systems in order to solve the bilingual task.'],\n",
       "   ['Furthermore, this affirmation is corroborated checking the official results on the last edition of CLEF 2006 [6] where our method [3] has being ranked first at the bilingual English-Spanish QA task.'],\n",
       "   ['07% at CLEF 2006) and than other current bilingual QA systems [6].']],\n",
       "  [['The resulting web QA system is evaluated on an established set of test questions: QA@CLEF 2004 (Magnini et al., 2005).']],\n",
       "  [['BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006).'],\n",
       "   ['For this purpose we have used the CLEF 2006 set of questions, the EFE corpora, the evaluation measures34 proposed by the CLEF organization (Magnini et al, 2006) and our official results in this competition.']],\n",
       "  [['These different kinds of approaches proved to be quite successful on texts from newspaper articles or texts of the same type, with better performances on English texts than on French texts (as for systems on other European languages These different kinds of approaches proved to be quite successful on texts from newspaper articles or texts of the same type, with better performances on English texts than on French texts (as for systems on other European languages [7]), certainly due to the lack of available reliable resources.']],\n",
       "  [['The Cross-Language Evaluation Forum or CLEFThe Cross-Language Evaluation Forum or CLEF3 , established in 2000, promotes multilingual question answering, where the question is posed in a different language than the language of the documents in the repository [73].']],\n",
       "  [[\"En question-r√©ponse (QR), l'√©valuation AVE (Answer Validation Exercise) (Giampiccolo et al., 2007) est une t√¢che similaire.\"]],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [[['Today, there is also experimental evidence for the claim that question answering can profit from the use of logical subsystems: A comparison of answer validators in the Answer Validation Exercise 2006 found that systems reported to use logic generally outperformed those without logical reasoning [2].']],\n",
       "  [['We performed the same experiment joining the Answer Validation Exercise4 (AVE) 2006 English data set (Pe√±as et al., 2006) and the Microsoft Research Paraphrase Corpus5 (MSRPC) (Dolan et al.']]],\n",
       " [],\n",
       " [[['ImageCLEF Wikipedia Image Retrieval Datasets ImageCLEF Wikipedia Image Retrieval Datasets [6] represent a project most similar to WIKImage.']],\n",
       "  [['We provide extensive experimental evidence for our conclusions, based on the ImageCLEF 2011 Wikipedia dataset [19].'],\n",
       "   ['al [19].']],\n",
       "  [['We show the applicability of our model on a multimodal domain by using the ImageCLEF 2011 Wikipedia collection dataset [47].'],\n",
       "   ['Related research on the ImageCLEF 2011 Wikipedia collection is generally based on a combination of text and image retrieval Related research on the ImageCLEF 2011 Wikipedia collection is generally based on a combination of text and image retrieval [47].'],\n",
       "   ['The topics are divided into four categories of easy (17 topics), medium (10 topics), hard (16 topics) and very hard (7 topics) [47].'],\n",
       "   ['In this experiment, we show the effect of different facet combination on recall behaviour of the topic categories (easy, medium, hard, very hard [47]).']],\n",
       "  [['Broadly speaking, the precision achieved by the different configurations are comparable to the best results obtained in the Image CLEF 2011 conference [55].']],\n",
       "  [['2006;Tsikrika, Popescu, and Kludas 2011;Aletras and Stevenson 2013;Benavent et al.']],\n",
       "  [['We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset [35].'],\n",
       "   ['For all our experiments we make use of the same LDA topic model learned on a corpus of 35, 582 English Wikipedia articles from the ImageCLEF Wikipedia collection For all our experiments we make use of the same LDA topic model learned on a corpus of 35, 582 English Wikipedia articles from the ImageCLEF Wikipedia collection [35].']],\n",
       "  [[\"Tsikrika et al. (2011) We tested over 20 features related to the user's profile(e.\"]],\n",
       "  [['For this task, we support our analysis on an information retrieval benchmark borrowed from the ImageCLEF 2011 track [10], which consists of a set of documents and a set of queries.'],\n",
       "   ['To build the ground truth we rely on the ImageCLEF 2011 track To build the ground truth we rely on the ImageCLEF 2011 track [10].'],\n",
       "   ['Broadly speaking, the precisions achieved by the different configurations are comparable to the best results obtained in the Image-CLEF 2011 conference [10].']]],\n",
       " [[['The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].'],\n",
       "   ['Similar to retrieval, it has shown that the classification results have better accuracy by combining text and images, in most cases, than the results using either text or image features alone [10][11][12].'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['By observing the retrieval results of the ImageCLEF track during the past several years [10,11], we concluded that the text-based retrieval systems overwhelmingly outperformed visual systems in terms of precision and other measures.']],\n",
       "  [['Extending the prior work inclusion criterion from text to other data modalities, the ImageCLEF lab included annual shared tasks on biomedical image processing from 2005 to 2013 [29][30][31].']],\n",
       "  [['The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 [1,[11][12][13].']],\n",
       "  [['After the evolution of ImageCLEF, three additional data sets were added: 230,088 images for the 2011 [50] data set, and 306,539 images for both the 2012 [51] and 2013 [52] data sets.']]],\n",
       " [[['Grey scale and colour cues are often combined due to their complementary nature that benefits the object category recognition Grey scale and colour cues are often combined due to their complementary nature that benefits the object category recognition [5], [40], [20], [41], [42], [43], [44], [45] and visual concept detection [46], [47], [48], [49], [11].'],\n",
       "   ['Bi-modal Second-order Occurrence Pooling in equations Bi-modal Second-order Occurrence Pooling in equations (45), (46), and (47) can also be readily derived from Bi-modal Higher-order Occurrence Pooling.'],\n",
       "   ['Pooling over œà 0 n , œà 1 n , and œà 2 n from equations ( Pooling over œà 0 n , œà 1 n , and œà 2 n from equations ( 53), (54), and (55)  given images i and j and adding such kernels is equivalent to operations in equations ( 45), (46), and (47).'],\n",
       "   ['The PascalVOC07 [58], Caltech101 [59], Flower102 [42], ImageCLEF11 [46], 15 Scenes [30], and PascalVOC10 Action Recognition [58] sets are used in evaluations.'],\n",
       "   ['ImageCLEF11 Photo Annotation [46] is a challenging collection of images represented by 99 concepts of a varied nature, including complex topics, e.']],\n",
       "  [['They have excelled in past competitions on visual concept recognition and ranking such as Pascal VOC [35,36] and ImageCLEF PhotoAnnotation [37].']],\n",
       "  [['2007;Nowak et al. 2011;Nowak and Huiskes 2010;Stanek et al.'],\n",
       "   ['Last years ImageClef challenge (Nowak et al. 2011) is among the first regarding the vocabularies taxonomy in their evaluation, in their case the Flicker Tag Similarity was used to denote similarity between words.'],\n",
       "   ['(2012) like WordNet could be employed together with an extended evaluation measure such as proposed in Nowak et al. (2011).']],\n",
       "  [[', Pascal VOC [5], TRECVID [6] and ImageCLEF [7,8,9,10].'],\n",
       "   ['While this approach has largely demonstrated its effectiveness in various challenges for VCR such as Pascal VOC [5] and is also the prevalent technique used in the ImageCLEF Image Annotation task [9], its major shortcoming is still its lack of descriptive power as regard to HLSCs because of its nature of low-level features.'],\n",
       "   ['We carried out extensive experiments on the MIR FLICKR image collection We carried out extensive experiments on the MIR FLICKR image collection [7,8] that was used within the ImageCLEF 2011 photo annotation challenge [9].'],\n",
       "   ['We carried out extensive experiments on the MIR FLICKR image collection We carried out extensive experiments on the MIR FLICKR image collection [7,8] that was used within the ImageCLEF 2011 photo annotation challenge [9].'],\n",
       "   ['The performance was quantitatively measured by the Mean interpolated Average Precision (MiAP) as the standard evaluation measure, while the example-based evaluation applies the example-based F-Measure (F-Ex) and Semantic R-Precision (SRPrecision) The performance was quantitatively measured by the Mean interpolated Average Precision (MiAP) as the standard evaluation measure, while the example-based evaluation applies the example-based F-Measure (F-Ex) and Semantic R-Precision (SRPrecision) [9].'],\n",
       "   ['Our best run, the prediction model multimodal model 5, achieved the best iAP performance on 13 visual concepts out of 99 [9].']],\n",
       "  [['For example, in the ImageCLEF 2011 photo annotation task [22], which originally motivated the present study, the learning problem involved 99 labels without any accompanying semantic meta-data, among which certain deterministic relationships did exist.']],\n",
       "  [['Shape, texture and colour cues are often combined for object category recognition Shape, texture and colour cues are often combined for object category recognition [5], [19], [35], [36], [37], [38] and visual concept detection [11], [39], [40], [41], [42].'],\n",
       "   ['It results from an expansion of Minor Polynomial Kernel in equation (36) according to Binomial theorem in a similar way to equations (38)(39)(40)(41).'],\n",
       "   ['The cross-term is also insufficient without the self-tensors in equations (39) and (41).']],\n",
       "  [['For example, ground truth acquisition for the ImageCLEF 2011 photo annotation and concept-based retrieval tasks was achieved via crowd-sourcing in batches of 10 and 24 images [4].']],\n",
       "  [['Nous appliquons notre syst√®me multimodal de d√©tection de concepts visuels au corpus de la t√¢che VCDT (Visual Concept Detection Task) de la campagne internationale ImageClef Nous appliquons notre syst√®me multimodal de d√©tection de concepts visuels au corpus de la t√¢che VCDT (Visual Concept Detection Task) de la campagne internationale ImageClef [19].'],\n",
       "   ['Pour mesurer les performances de notre syst√®me, nous avons utilis√© les mesures adopt√©es par la campagne internationale ImageClef 2011 Pour mesurer les performances de notre syst√®me, nous avons utilis√© les mesures adopt√©es par la campagne internationale ImageClef 2011 [19].']],\n",
       "  [['And in other methods [6], [7], [12], [14] a short list of tags are enriched after performing AIA.'],\n",
       "   ['In the first phase, the training dataset includes images with their associated text files (each image has a text file containing the list of user tags) [14].']],\n",
       "  [['From (19) and (20), it is shown that y 2 X 2 F < N Xy 2 .'],\n",
       "   ['ImageCLEF 2011 [20] photo annotation 8000 10000 99 11.']],\n",
       "  [[', 2010;Nowak and Huiskes, 2010;Nowak et al., 2011;Thomee and Popescu, 2012], shows that the first model outperforms the state-of-the-art methods on three out of five datasets and the second proposed model outperforms the state-of-the-art methods on the five considered datasets on a tag-based image annotation task.'],\n",
       "   [', 2010;Nowak et al., 2011], on which we obtain similar or better results than the state-of-the-art.'],\n",
       "   [', 2010;Nowak and Huiskes, 2010;Nowak et al., 2011;Thomee and Popescu, 2012] shows that our framework achieves comparable and better results compared to more sophisticated state-of-the-art approaches as summarized in Table 1.'],\n",
       "   ['Experimental results on two challenging datasets [Nowak and Huiskes, 2010;Nowak et al., 2011] show the effectiveness of the proposed framework.'],\n",
       "   [\"‚Ä¢ ImageClef '11 is the same dataset used within the ImageCLEF 2010 photo annotation challenge [Nowak et al., 2011] with a small difference is that images are annotated with 99 concepts.\"],\n",
       "   [\", 2010] and ImageClef'11 [Nowak et al., 2011] datasets were collected from Flickr but they differ significantly.\"]]],\n",
       " [[['9 -Varia√ß√£o no n√∫mero de lobos de uma folha de Ficus carica (132).']],\n",
       "  [['Our descriptors have been tested on three leaf datasets: the Flavia dataset Our descriptors have been tested on three leaf datasets: the Flavia dataset [23], the ImageCLEF dataset in 2011 [8] and in 2012 [9].'],\n",
       "   ['The ImageCLEF 2011 leaf dataset The ImageCLEF 2011 leaf dataset [8] contains three categories of images: scans of leaves acquired using a flat-bed scanner; scan-like leaf images acquired using a digital camera and free natural photos.'],\n",
       "   ['The ImageCLEF 2011 leaf dataset The ImageCLEF 2011 leaf dataset [8] contains three categories of images: scans of leaves acquired using a flat-bed scanner; scan-like leaf images acquired using a digital camera and free natural photos.'],\n",
       "   ['The ImageCLEF 2011 leaf dataset The ImageCLEF 2011 leaf dataset [8] contains three categories of images: scans of leaves acquired using a flat-bed scanner; scan-like leaf images acquired using a digital camera and free natural photos.']],\n",
       "  [['Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset [35], the Flavia dataset [38], the Im-ageCLEF dataset in 2011 [16] and in 2012 [17].'],\n",
       "   ['Let us now introduce the context of the plant identification task of ImageCLEF 2011 Let us now introduce the context of the plant identification task of ImageCLEF 2011 [16].']],\n",
       "  [['Challenges for the community have even been organized such as the ImageCLEF Plant Identification Task [2] since 2011.'],\n",
       "   ['The database used to test our algorithms is a subset of the Pl@ntLeaves The database used to test our algorithms is a subset of the Pl@ntLeaves [2] database, keeping only 3603 leaf images (out of 5436) on white, plain or natural backgrounds of the 50 species (out of 71) with non-compound leaves.']],\n",
       "  [['Evaluation results on scans and scan-like leaf images of the ImageCLEF 2011 and ImageCLEF 2012 plant identification tasks [9,10] are reported and discussed in Section 3.'],\n",
       "   ['Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 [9] and in 2012 [10].'],\n",
       "   ['Let us now introduce the context of the plant identification task of ImageCLEF 2011 Let us now introduce the context of the plant identification task of ImageCLEF 2011 [9].']],\n",
       "  [['This is the training subset of Pl@ntLeaves dataset that was used for the plant identification task organized within ImageCLEF 2011 [12].'],\n",
       "   ['Let us first introduce the context of the organized identification task ImageCLEF 2011 Let us first introduce the context of the organized identification task ImageCLEF 2011 [12].'],\n",
       "   ['More details about the methods and the complete list of scores can be found in [12].']],\n",
       "  [['There is a growing body of work investigating finegrained image classification of birds There is a growing body of work investigating finegrained image classification of birds [8,24,27], insects [15,18], flowers [6,19] and leaves [1,6,14].'],\n",
       "   ['We refer to [14] for details.'],\n",
       "   ['Figure 10 shows the scores of all the submitted runs of the eight participants; details about the participants can be found in [14].']],\n",
       "  [['These images are photographs taken from the Pl@ntLeaves database [Go√´a11].']],\n",
       "  [['A summary of recent work on algorithms for plant identification and as well as detailed evaluations are described in the CLEF 2011 plant images classification task [4].']],\n",
       "  [['There are several studies of plant classification built on the pictures of individual plant leaves [42][43][44][45].']]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[['The entrance exam task was first proposed in 2013 as a pilot task The entrance exam task was first proposed in 2013 as a pilot task [12] in the Question Answering for Machine Reading Evaluation (QA4MRE) lab, which has been offered at the CLEF conference1 since 2011 [10,11].']]],\n",
       " [],\n",
       " [[['For our experiments, we used used the Lucene IR System For our experiments, we used used the Lucene IR System 6 to index the English subset of CLEF-IP 2010 and CLEF-IP 2011 datasets7  [19,21] with the default settings for stemming and stop-word removal.']],\n",
       "  [['Recently, several evaluation campaigns Recently, several evaluation campaigns [4,5] have attracted attention on the importance of the role of information retrieval in the field of the intellectual property.']],\n",
       "  [['\" [30].'],\n",
       "   ['Hence, one of the main focus of this paper is to consider patent image classification according to image types as the ones identified and used in the Patent Image Classification task of Clef-IP 2011 [30], namely abstract drawing, graph, flowchart, gene sequence, program listing, symbol, chemical structure, table and mathematics.'],\n",
       "   [\"We would like first to recall briefly our participation in the Image-based Patent Retrieval task's at Clef-IP 2011 We would like first to recall briefly our participation in the Image-based Patent Retrieval task's at Clef-IP 2011 [30].\"],\n",
       "   [\"We would like first to recall briefly our participation in the Image-based Patent Retrieval task's at Clef-IP 2011 We would like first to recall briefly our participation in the Image-based Patent Retrieval task's at Clef-IP 2011 [30].\"]],\n",
       "  [['iPerFedPat currently integrates the results of four patent search systems: Clef-IP 2011 [46], Espacenet15 , Google Patents 16 and WIPO PatentScope 17 .']],\n",
       "  [['Most of the approaches to this task use standard information retrieval (IR) processes [7], [8].']],\n",
       "  [['In order to check the performance of the proposed algorithm, we established the training, validation and test datasets, all of which are subsets of the CLEF-IP 2011 dataset In order to check the performance of the proposed algorithm, we established the training, validation and test datasets, all of which are subsets of the CLEF-IP 2011 dataset [33].']],\n",
       "  [['The CLEF-IP dataset is also a collection of heterogeneous document images composed of 9 different classes The CLEF-IP dataset is also a collection of heterogeneous document images composed of 9 different classes [29].']],\n",
       "  [['Evaluation of the proposed approach is tested against the task of classification of the MNIST benchmark Evaluation of the proposed approach is tested against the task of classification of the MNIST benchmark [6] and the CLEF-IP dataset of patent images in [12].'],\n",
       "   [\"In addition, we also include representative examples of the sampled shapes extracted from CLEF-IP's [12] images extracted from patent documents.\"],\n",
       "   ['The final experiment on classification evaluates the proposed method against the CLEF-IP The final experiment on classification evaluates the proposed method against the CLEF-IP [12] patent image dataset.'],\n",
       "   ['Table Table 2: Classification performance on CLEF-IP [12] .']],\n",
       "  [['2009), 2010 (Piroi and Tait 2010) and 2011 (Piroi et al. 2011), and TREC-CHEM in 2009 (Lupu et al.'],\n",
       "   ['The CLEF-IP classification tasks in 2010 The CLEF-IP classification tasks in 2010 (Piroi and Tait 2010) and (Piroi et al. 2011) released test collections on IPC codes; these were larger than the WIPOalpha collection.'],\n",
       "   ['In addition, CLEF-IP performed completely new tasks, including ones on image-based retrieval (Piroi et al. 2011), image classification (Piroi et al.'],\n",
       "   ['In addition, CLEF-IP performed completely new tasks, including ones on image-based retrieval (Piroi et al. 2011), image classification (Piroi et al.']],\n",
       "  [['The dataset used in the auxiliary task comes from the CLEF-IP 2011 [46], including more than 40,000 images.']],\n",
       "  [['The CLEF-IP tracks included a patent classification task [46], [47], which provided a dataset of more than 1 million patents as the training set to classify 3,000 patents into their IPC subclasses.']],\n",
       "  [['DeepPatent outperformed other algorithms from the CLEF-IP (Piroi et al., 2011) competition and contributed the new USPTO-2M dataset, which is much larger than the previous benchmarks.'],\n",
       "   ['M-patent: Moreover, we used a smaller dataset, which is a subset of the CLEF-IP 2011 M-patent: Moreover, we used a smaller dataset, which is a subset of the CLEF-IP 2011 (Piroi et al., 2011) dataset.'],\n",
       "   [', 2018a) utilized the evaluation measures from the CLEP-IP competition (Piroi et al., 2011).']],\n",
       "  [['This retrieval setup is typical in professional, domain-specific tasks such as legal case law retrieval [1,2], patent prior art search [11,24,25], and scientific literature search [1,19,20].']],\n",
       "  [['Yet there are few computer vision research papers focused on    querying and retrieving abstract, technical drawings [38,51,69], and so it is unclear whether the computer vision approaches that are successful with general images are similarly successful with technical drawings.']],\n",
       "  [['The experiments reported in this article are based on the CLEF-IP standard test collection The experiments reported in this article are based on the CLEF-IP standard test collection (Piroi et al., 2011), which is an extract from the more extensive matrixware research collection (MAREC) collection (MAREC, Online), and it consists of 3,118,088 patent documents pertaining to 1,768,641 patents (i.']],\n",
       "  [[', by the CLEF-IP labs [25,26,27,28,29].']],\n",
       "  [['CLEF-IP 2011 [25] provides two such datasets, but with a mere 211 patents and broad image classification across nine categories, it is limited in granularity.'],\n",
       "   ['Most patent figure research targets figure-based patent querying Most patent figure research targets figure-based patent querying [17,31,25] and classification [14,36,19].']],\n",
       "  [['The collection is composed of 75,250 patents (46,324 for training and 28,926 for testing).'],\n",
       "   ['The CLEF-IP 2011 The CLEF-IP 2011 [46] collection is based on the CLEF-IP 2010 dataset.'],\n",
       "   ['The overviews of the tasks are presented in [47] for CLEF-IP 2010 and in [46] for CLEF-IP 2011.'],\n",
       "   ['In the CLEF-IP 2011 In the CLEF-IP 2011 [46], there were two classification tasks: the first was to classify the test patents in the subclass level of the IPC, the second was to classify the test patents in the subgroup level of the IPC provided the real subclass of each patent (i.']]],\n",
       " [],\n",
       " [[['This problem has been also addressed in PAN Workshop and Competition: Uncovering Plagiarism, Authorship and Social Software Misuse (Argamon, Juola 2011, Juola, Stamatos 2013).']],\n",
       "  [['This is a problem closer to authorship identification;2 see Stamatatos (2009a) and Argamon and Juola (2011) for an overview of state of the art authorship identification approaches that could be exploited when detecting plagiarism from this point of view.'],\n",
       "   ['(Potthast and Holfeld, 2011;Potthast, Stein, and Holfeld, 2010c) and authorship identification in 2011(Argamon and Juola, 2011) have been organised in PAN as well.']],\n",
       "  [['Argamon and Juola [74] collected the results of the PAN 2011 competition where 3001 electronic messages from 26 authors were classified using diverse features for which the best micro-averaged recall (i.'],\n",
       "   ['Indeed, our method is among the most successful authorship recognition approaches according to the literature [74].']],\n",
       "  [['In the 2011 edition, a dataset using emails (extracted from the Enron corpus) and relatively large sets of candidate authors was developed [4].']],\n",
       "  [['‚Ä¢ PAN11 ‚Ä¢ PAN11 [41] the small dataset contained 3, 001 documents by 26 authors while the large set had 9, 337 documents by 72 authors.']],\n",
       "  [['This dataset is based on the Enron email corpus 15 and developed for the PAN2011 authorship competition [2].']]],\n",
       " [],\n",
       " [[['For example, the CENTRE workshop [9] challenges participants to reconstruct IR systems and their results, whereas The Open-Source IR Replicability Challenge (OSIRRC) [7] motivated participants to package their retrieval systems and corresponding software dependencies in advance to prepare them for appropriate reuse.']]],\n",
       " [[[', 2018;Atanasova et al., 2019) and verification (Barr√≥n-Cede√±o et al.']],\n",
       "  [[', 2018;Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al.']],\n",
       "  [['False information spreading involves various research tasks, including: fact checking [4,40], topic credibility [15,41], fake news spreaders profiling [34], and manipulation techniques detection [8].']],\n",
       "  [[', 2018(Atanasova et al., , 2019) ) and verification (Barr√≥n-Cede√±o et al.']],\n",
       "  [['A non extensive list of papers includes PHEME [135], SOME-LIKE-IT-HOAX [103], SYMMETRIC FEVER [92], CLEF-2019 tasks 1 & 2 [4,37], CLIMATE-FEVER [30] , VITAMINC [91] and Real World search-engine-based claims [105].']],\n",
       "  [['Earlier works in identifying check-worthy claims were at the granularity of entire sentences Earlier works in identifying check-worthy claims were at the granularity of entire sentences [5,26,54,81].'],\n",
       "   ['[5].'],\n",
       "   ['[5]), sentences in medical newswire are long and complex, often positioning the primary claim(s) within a larger context of other information [96], we obtain 6,000 news articles from the \"Health\" category of Google News during April 2018 and augment this with the top 25 RSS feeds in the \"Health and Healthy Living\" category5 from November 2018 through April 2019 to get over 34,000 news articles.'],\n",
       "   ['Predicting the score of a claim-sentence pair is formulated as regression learning with target set Predicting the score of a claim-sentence pair is formulated as regression learning with target set [1,5] aligned with the Likert-type scale.']],\n",
       "  [[', 2018;Atanasova et al., 2019;Barr√≥n-Cedeno et al.']],\n",
       "  [[', 2018;Atanasova et al., 2019;Hasanain et al.']]],\n",
       " [[[', 2018;Hasanain et al., 2019;Hasanain et al.']],\n",
       "  [[', , 2018a;;Hasanain et al., 2019;Tokala et al.']],\n",
       "  [[', 2018;Hasanain et al., 2019Hasanain et al.']],\n",
       "  [[\"However, assessing the factuality of a claim may refer to assessing a claim's veracity [74] rather than assessing whether it is a factual or non-factual claim.\"]],\n",
       "  [['lab at CLEF2019 (Hasanain et al., 2019).'],\n",
       "   [', 2019;Hasanain et al., 2019).'],\n",
       "   [', 2019;Hasanain et al., 2019) to increase the number of training examples.'],\n",
       "   ['These systems were also evaluated using the dataset we are proposing in this work: CT19-T2 (Hasanain et al., 2019).'],\n",
       "   ['1 This work is a significant extension to an earlier work 1 This work is a significant extension to an earlier work (Hasanain et al., 2019).']],\n",
       "  [['A non extensive list of papers includes PHEME [135], SOME-LIKE-IT-HOAX [103], SYMMETRIC FEVER [92], CLEF-2019 tasks 1 & 2 [4,37], CLIMATE-FEVER [30] , VITAMINC [91] and Real World search-engine-based claims [105].']],\n",
       "  [[', 2019;Hasanain et al., 2019Hasanain et al.']]],\n",
       " [[['The CLEF eHealth 2019 Technology Assisted Review Task [27] focuses on the problem of systematic reviews, that is the process of collecting articles that summarize all evidence (if possible) that has been published regarding a certain medical topic.']],\n",
       "  [[\"Existing work on relevance assessment is mostly related to TREC and CLEF evaluation challenges through the means of constructing test collections and reporting annotators' experience [19][20][21].\"]],\n",
       "  [['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].'],\n",
       "   ['Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].']],\n",
       "  [['-We validate the effectiveness of the proposed framework and provide a detailed analysis of its components on various datasets including the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the Text Retrieval Conference (TREC) Total Recall datasets [16], and the TREC Legal datasets [13].'],\n",
       "   ['Three datasets have been released-namely, EMED 2017 Three datasets have been released-namely, EMED 2017 [20], EMED 2018 [21,34], and EMED 2019 [22].'],\n",
       "   ['[9] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [20,21,22].'],\n",
       "   ['To examine the effectiveness of the proposed framework, we compared it against the Knee, Target, SCAL, SD-training, and SD-sampling methods and provided detailed analysis on various datasets including the CLEF Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the TREC Total Recall datasets [16], and the TREC Legal datasets [13].']],\n",
       "  [['Among recent campaigns, we note the TREC Clinical Decision Support/Precision Medicine tracks [26,27], as well as the CLEF eHealth Lab for systematic reviews [13].'],\n",
       "   ['The use of advanced IR techniques to improve scientific literature search has become an essential part of modern biomedical search engines The use of advanced IR techniques to improve scientific literature search has become an essential part of modern biomedical search engines [10,13,27,32].']],\n",
       "  [['HRR tasks include electronic discovery in the law (eDiscovery) [3], systematic review in medicine [22][23][24]47], document sensitivity review [34], online content moderation [55], and corpus annotation to support research and development [60].']],\n",
       "  [['loss er is introduced for the first time in [37] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [82,84,86].']],\n",
       "  [['Other applications include open government document requests Other applications include open government document requests [3] and systematic review in medicine [14][15][16]32].']],\n",
       "  [['TAR evaluation conferences TAR evaluation conferences [17,[23][24][25]38] have emphasized interventional stopping rules, i.']],\n",
       "  [[\"CLEF TAR 2018 [11] This collection adds 30 diagnostic test accuracy systematic reviews as topics to the existing 2017 collection; however, it also removes eight because they are not 'reliable for training or testing purposes.\"],\n",
       "   [\"This ranking of studies has come to be known as 'screening prioritisation', as popularised by the CLEF TAR tasks which aimed to automate these early stages of the systematic review creation pipeline [9,11,10].\"],\n",
       "   ['We reproduced the SDR for systematic reviews method by Lee and Sun We reproduced the SDR for systematic reviews method by Lee and Sun [16] on all the available CLEF TAR datasets [9,11,10].']],\n",
       "  [['We use topics from the CLEF TAR task from 2017, 2018, and 2019 We use topics from the CLEF TAR task from 2017, 2018, and 2019 [11][12][13].']],\n",
       "  [['In addition, several recent challenges, such as TREC [17,24] and CLEF eHealth task 2 [25][26][27], further promote the development of automatic document screening.'],\n",
       "   ['It has been shown that this active learning solution outperforms its counterparts in many real-world cases [17,[24][25][26][27].']],\n",
       "  [['We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].'],\n",
       "   ['We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].']],\n",
       "  [['Future studies could make use of databases such as the CLEF TAR database [54] or the systematic review dataset repository [55].']],\n",
       "  [['We explore initial experiments on the CLEF TAR 2019 dataset [16].'],\n",
       "   ['The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings [16,27,34], often sourced at the title and abstract screening level, rather than at the full-text level.'],\n",
       "   ['Cost-based and economic-based metrics are also used, especially in the context of the query formulation task in the CLEF TAR shared task [14][15][16], e.'],\n",
       "   ['Traditionally, retrieval was conducted at the level of publications [14][15][16].'],\n",
       "   ['We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets [16].'],\n",
       "   ['However, there are several other types of systematic reviews, such as diagnostic test accuracy reviews, prognostic reviews, and qualitative research reviews, each of which presents unique challenges for automation and evaluation [16].']],\n",
       "  [['Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets [28][29][30] and systematic review collection with seed studies (Seed Collection) [79].']],\n",
       "  [[', 2014;Cormack and Grossman, 2016;Kanoulas et al., 2019;Lee et al.']],\n",
       "  [['We also include in our experiments three corpora from the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets from the years 2017, 2018, and 2019 [24][25][26].']],\n",
       "  [['We rely on the CLEF-TAR 2017, 2018 and 2019 Subtask 2 datasets [22][23][24] (abbreviated as .']]],\n",
       " [[['A previous larger training set and test set from this database has been previously used in another shared task for the assessment of the automatic assignment of ICD-10 codes (Neves et al., 2019).']],\n",
       "  [[', 2019) eHealth dataset is a curated collection of non-technical summaries (NTS) of animal experiments from Germany, which was used to organize the Multilingual Information Extraction Task (Task 1) in the CLEF eHealth Challenge 2019 (D√∂rendahl et al., 2019).']]],\n",
       " [[[', 2019;Losada et al., 2020;Low et al.']],\n",
       "  [['Similarly, insurance companies track daily posts from customers to detect and initiate early treatment of diseases Losada et al. [2019], Burdisso et al.']]],\n",
       " [[['Convolutional Neural Networks performed well in other fine-grained species identification tasks, including plant species classification [11,12], dog classification [19], bird classification [35,36], or classification of species in general [33].']],\n",
       "  [['Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].']],\n",
       "  [['For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.']]],\n",
       " [[['The training set used for the challenge will be a version of the 2019 training set The training set used for the challenge will be a version of the 2019 training set [40] enriched by new contributions from the Xeno-canto network and a geographic extension.']],\n",
       "  [['(2018), and in public competitions (Kahl et al., 2019).']],\n",
       "  [['The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge [31].'],\n",
       "   ['The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge [31].'],\n",
       "   ['The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge [31].'],\n",
       "   ['160, which made them win the second place [31], and our cmAP is much higher than that of the third place, which is 0.']],\n",
       "  [['[7] which uses an attention-based model for music genre prediction according to a commercial taxonomy, [8] that classify insects according to flying noise, dividing between pollinators and not pollinators) but also when the taxonomy exists and is (in general) accepted by domain experts: for example, [9] propose a dataset for bird recognition without providing the taxonomy, however biological taxonomy (although discussed and modified by biologists) can be accepted.']]],\n",
       " [[['They were constructed from various open datasets as explained in the protocol note [2].']],\n",
       "  [['Previous editions of the GeoLifeCLEF dataset [36,50] are also available, and are suitable for largescale plant-focused species distribution modeling in France using traditional covariates.']],\n",
       "  [['Specifically, we chose the commonly used top-k accuracy as suggested in Botella et al. (2019).']],\n",
       "  [['Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification.']]],\n",
       " [[['We modified an existing 3D convolutional neural network (CNN) architecture 59 -previously applied to the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark 60 -to accept multichannel input generated from the preprocessed dMRI: the b = 0 reference diffusion image, each of the three cardinal axis components of the DEC-FA image, and, optionally, automated QC metrics from QSIPrep.'],\n",
       "   ['The image processing part of the model architecture was identical for both models: a modification of an existing 3D CNN 59 previously applied to assess tuberculosis severity 60 .']],\n",
       "  [['Although X-rays are frequently used, they have side effects such as exposure to ionizing radiation harmful to the human body and relatively low information when compared to other imaging methods; ‚Ä¢ Computerized Tomography (CT): is a more advanced imaging test that can be used to detect disorders such as cancer that an X-ray could miss [36][37][38][39].']],\n",
       "  [['The primary dataset for training was obtained from the Image CLEF Tuberculosis 2019 dataset 40 and frozen after obtaining a 73.']]],\n",
       " [[['The 4th edition of the task will follow a similar format to previous editions [2][3][4] where participants automatically segment and label a collection of images that can be used in combination to create three-dimensional models of an underwater environment.']]],\n",
       " [[['To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task (2008)(2009)  [8], [26], ImageCLEF Lifelogging Task (2017-2019) [27], TREC Web Track: Diversity Task (2009-2012) [28], and MediaEval Retrieving Diverse Social Images Task (2013-2017) 1 .']],\n",
       "  [['In the recent ImageCLEF lifelog task [1] and the recent NTCIR14-Lifelog task [4], the retrieval tools are used by their creators who have in-depth knowledge about their systems, and the objective for participants mainly focuses on how to better extract information from visual lifelog data.']],\n",
       "  [['In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].']],\n",
       "  [['The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [14] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [11] and the ImageCLEF Lifelog tasks [7].']],\n",
       "  [['Previous studies also show that using a puzzle as an experimental task could help in simulating an information seeking process (Kelly, 2009;Samimi & Ravana, 2014;Jansen, Bos, van der Vet, Huibers, & Hiemstra, 2010;Hills, Todd, & Goldstone, 2010;Dang Nguyen et al., 2019).'],\n",
       "   [', 2010;Dang Nguyen et al., 2019) Satisfaction can be linked to stopping behaviours in the information seeking process, where previous research has formalised the stopping criteria used to terminate a search (Kraft & Lee, 1979).']],\n",
       "  [['Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges [82][83][84]; Lifelog Search Challenge [85][86][87], which aims to encourage the development of efficient interactive lifelog retrieval systems; and NTCIR Lifelog Tasks [77].']],\n",
       "  [['to address different problems in lifelog data to address different problems in lifelog data [4][5][6].']],\n",
       "  [['Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].'],\n",
       "   ['In the last editions of ImageCLEFlifelog In the last editions of ImageCLEFlifelog [4,28], we proposed some preliminary work presenting a baseline approach to solve the task of lifelog moment retrieval (LMRT) [29,30].']],\n",
       "  [['To benchmark for different search engines in indexing and retrieving multimodal lifelog data, there have been a number of interactive lifelog retrieval challenges such as NTCIR Lifelog [7,8], ImageCLEF Lifelog [4,5,20], and Lifelog Search Challenge (LSC) [9,10].']],\n",
       "  [['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.'],\n",
       "   ['Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.']]],\n",
       " [[['Later ImageCLEF Caption tasksLater ImageCLEF Caption tasks[77,78] only required systems to assign labels to medical images, without requiring diagnostic tex to be generated.']],\n",
       "  [['In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.']],\n",
       "  [['The ROCO dataset has been used in the medical caption tasks The ROCO dataset has been used in the medical caption tasks [3][4][5][6] at the Image Retrieval and Classification Lab of the Conference and Labs of the Evaluation Forum (ImageCLEF) 7 .']]],\n",
       " [],\n",
       " [[['\" [68] The models implemented in this research differ on the treatment over lexical features.']]],\n",
       " [],\n",
       " [[['It is interesting to mention that when bots were profiled as humans, they were mostly confused with males [19].']],\n",
       "  [['Among other problems, researchers have tried to address bot detection [22], rumour detection [21] and fact checking [7].']],\n",
       "  [['Rashkin et al. [2017] released TSHP-17, a balanced corpus with document-level annotation including four classes: trusted, satire, hoax, and propaganda.'],\n",
       "   ['Rashkin et al. [2017] defined a classical four-classes text classification task: propaganda vs trusted vs hoax vs satire, using the TSHP-17 dataset.'],\n",
       "   [', 2015], textual content [Rangel and Rosso, 2019], or profile information [Lee and Kim, 2014].'],\n",
       "   ['Contrarily, all techniques based on textual analyses, such as those that solely rely on natural language processing, are supervised detectors that analyze individual accounts [Rangel and Rosso, 2019].']],\n",
       "  [['Researchers have focused among others on the detection of fake news [22], rumours [23], clickbaits [24], bots [25], and fact checking [26].']],\n",
       "  [['The cumulative distribution of the average mentions of tweets in the dataset [32] of this paper is shown in Figure 2a.'],\n",
       "   ['We selected the author profiling task 2019 (CLEF2019) dataset [32] for this paper, which aims to identify the nature of the Twitter account, detect whether the account is a social media bot or a human, and determine the gender of the account.'],\n",
       "   ['We selected the author profiling task 2019 (CLEF2019) dataset [32] for this paper, which aims to identify the nature of the Twitter account, detect whether the account is a social media bot or a human, and determine the gender of the account.']],\n",
       "  [['Even though classic machine learning that learned from probabilistic representations of samples of text has proven to be effective in AP tasks Even though classic machine learning that learned from probabilistic representations of samples of text has proven to be effective in AP tasks (Pardo and Rosso, 2019), deep learning has been a less examined solution for author profiling, according to our survey (Aljohani et al.'],\n",
       "   ['Even though classic machine learning that learned from probabilistic representations of samples of text has proven to be effective in AP tasks Even though classic machine learning that learned from probabilistic representations of samples of text has proven to be effective in AP tasks (Pardo and Rosso, 2019), deep learning has been a less examined solution for author profiling, according to our survey (Aljohani et al.']],\n",
       "  [['Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.']],\n",
       "  [['Preliminary results obtained with the PAN dataset collection (Rangel and Rosso 2019) showed that the hybrid approach, with rules and a statistical classifier, works slightly better than using just the rules or the classifier.'],\n",
       "   ['As explained in Section 3, the preliminary results obtained through the compilation of PAN datasets (Rangel and Rosso 2019) showed that the hybrid approach, with rules and a statistical classifier, works somewhat better than the rules or the classifier alone.']],\n",
       "  [['Changes in writing styles is important for many problems: diagnosis of neurological diseases Changes in writing styles is important for many problems: diagnosis of neurological diseases [4], authorship attribution [5,6], author profiling [7,8], author identification [9] and fake news detection [10,11].']],\n",
       "  [['Esta tarefa, aqui denominada classifica√ß√£o de fundamentos morais pessoais, ou CFMP, pode ser vista como uma inst√¢ncia do problema de caracteriza√ß√£o autoral (RANGEL; ROSSO, 2019) comumente utilizada na infer√™ncia de vari√°veis sociais do autor de um texto, como g√™nero (TAKAHASHI et al.']],\n",
       "  [['This research line is very well summarized by Ikae and Savoy in [13] and has as corner stone the author profiling task at PAM-CLEF competitions [14,15,16,17,18,19,20].']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [['As a result, in recent years, researchers have dedicated a significant amount of attention to social media bot detection (Ali and Syed 2022;Ferrara 2018;Rangel and Rosso 2019;Yang et al.']],\n",
       "  [['pan-2019 [55] includes all of the components of cresci-2015, cresci-2017, varol-2017, plus caverlee-2011 and an additional collection of manually annotated bots and humans not found in any of these.']],\n",
       "  [['Bot detection has been addressed in the Author Profiling (AP) Task at PAN 2019 Bot detection has been addressed in the Author Profiling (AP) Task at PAN 2019 39 , where they focus on solving whether an author is a bot or a human and if it is a human, to determine its gender.'],\n",
       "   ['On the other hand, the PAN 2019 competition 39 has addressed the issue of bot detection on anonymized accounts.']],\n",
       "  [[', 2018;Pardo and Rosso, 2019;Das et al.']]],\n",
       " [[[\"For the next edition, we shall continue working with 'fanfiction' For the next edition, we shall continue working with 'fanfiction' [10,11].\"]],\n",
       "  [[', 2018;KESTEMONT et al., 2019).']],\n",
       "  [[', 2018(Kestemont et al., , 2019(Kestemont et al.'],\n",
       "   [', 2018(Kestemont et al., , 2019(Kestemont et al.']],\n",
       "  [[', 2017;Kestemont et al., 2019).']],\n",
       "  [['2018] and 2019 [Kestemont et al. 2019].']],\n",
       "  [['AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks [31,6,7,53,8] offer a very good overview of the most recent trends in the field.'],\n",
       "   ['The baselines presented in the 2019 edition The baselines presented in the 2019 edition [31] mirror the systems long considered standard, i.'],\n",
       "   ['We consider a SVM-based classifier as our AV system, due to the good performance SVM have demonstrated in text classification tasks in general over the years We consider a SVM-based classifier as our AV system, due to the good performance SVM have demonstrated in text classification tasks in general over the years [25], and in authorship analysis related tasks in particular [31].']]],\n",
       " [[[', 2019a, H√ºrriyetoƒülu et al. 2019b).'],\n",
       "   ['The crosscontext generalizability of the automated systems is already a challenging task (H√ºrriyetoƒülu et al. 2019b).']],\n",
       "  [['We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab (H√ºrriyetoglu et al., 2019).']],\n",
       "  [['Specifically, the shared tasks CLEF 2019 Protest News (H√ºrriyetoglu et al., 2019), AESPEN 2020 (H√ºrriyetoglu et al.']],\n",
       "  [['Various models have been developed for text classification in general and also for this particular task (H√ºrriyetoglu et al., 2019).']],\n",
       "  [['The detailed description of the subtasks can be found in The detailed description of the subtasks can be found in H√ºrriyetoglu et al. (2019) and H√ºrriyetoglu et al.']]],\n",
       " [[['To assess performance, we use the standard cluster recall at a cutoff at X images (CR@X) To assess performance, we use the standard cluster recall at a cutoff at X images (CR@X) [11], a measure of how many clusters from the ground truth are represented among the top X results provided by the retrieval system; the precision  [6] relevance GLRLM 0.']],\n",
       "  [['Relevance was more thoroughly studied in existing literature than diversification [11,31,36] and even though a considerable amount of diversification literature exists (mainly in the text-retrieval community), the topic remains important, especially in multimedia [27,33,35,39,43].'],\n",
       "   ['Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task [27] that proposes a dataset consisting of 498,920 news photographs (images and caption text) classified into sub-topics (e.'],\n",
       "   [', one of the state-of-the-art platforms) and addresses in particular the social dimension reflected in the nature of the data and methods devised to account for it; -while smaller in size than the ImageCLEF collections [27,41], the proposed dataset contains images that are already associated to topics by Flickr.']],\n",
       "  [['Relevance was more thoroughly studied in existing literature than diversification [1,2,3] and even though a considerable amount of diversification literature exists, the topic remains an important one, especially in social media [4,5,6,7,8].'],\n",
       "   ['(2) while smaller in size than ImageCLEF collections (2) while smaller in size than ImageCLEF collections [5,26], Div400 contains images that are already associated to topics by Flickr.'],\n",
       "   ['System performance was assessed on the testset using Cluster Recall at X (CR@X) -a measure assessing how many clusters from the ground truth are represented among the top X results (only relevant images considered) System performance was assessed on the testset using Cluster Recall at X (CR@X) -a measure assessing how many clusters from the ground truth are represented among the top X results (only relevant images considered) [5] and Precision at X (P@X) -measures the number of relevant photos among the top X results.']],\n",
       "  [['The increasing importance of diversity is reflected in a number of benchmarks, such as ImageCLEF 2009 [16] or MediaEval Diverse Social Images [9].']],\n",
       "  [['In terms of image search there are many ways of diversifying results; often these are implicit and involve removing near duplicates [3].']],\n",
       "  [['To effectively leverage the massive explosion of multimedia content, a large number of approaches have been proposed in the areas such as information retrieval, multimedia retrieval and computer vision [2,3,5,11,16].']],\n",
       "  [['The problems of image and text retrieval have been the subject of extensive research in the fields of information retrieval, computer vision, and multimedia The problems of image and text retrieval have been the subject of extensive research in the fields of information retrieval, computer vision, and multimedia [2], [10], [12], [27], [28].'],\n",
       "   ['These manual annotations are typically in the form of a few keywords, a small caption, or a brief image description [12], [13], [27].'],\n",
       "   ['In parallel with these developments, advances have been reported in multimodal retrieval systems [8], [9], [10], [11], [12], [13], [27].']],\n",
       "  [['[36,73,52,55,56,69]), the problem is not solved yet.']],\n",
       "  [['Recently, the idea of diversi cation for image search results has been studied by many researchers Recently, the idea of diversi cation for image search results has been studied by many researchers [26,52], and some international challenges have been also proposed to address this issue (ImageCLEF [30] and MediaEval Retrieving Diverse Social Images Task [17]).']],\n",
       "  [['image search diversification [23][24][25][26][27].'],\n",
       "   ['Another well-known example is the ImageCLEF 2009 Photo Task, which revolved around image diversification [25].']],\n",
       "  [['An effective retrieval system should also take into account the diversification of the results [8].'],\n",
       "   ['With this concept, cluster recall was introduced as a measure for diversity in image retrieval [8].'],\n",
       "   ['To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task (2008)(2009)  [8], [26], ImageCLEF Lifelogging Task (2017-2019) [27], TREC Web Track: Diversity Task (2009-2012) [28], and MediaEval Retrieving Diverse Social Images Task (2013-2017) 1 .'],\n",
       "   ['To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task (2008)(2009)  [8], [26], ImageCLEF Lifelogging Task (2017-2019) [27], TREC Web Track: Diversity Task (2009-2012) [28], and MediaEval Retrieving Diverse Social Images Task (2013-2017) 1 .']]],\n",
       " [[['The large-scale visual concept detection and annotation task (LS-VCDT) in ImageCLEF 2009 The large-scale visual concept detection and annotation task (LS-VCDT) in ImageCLEF 2009 [13] used the MIR Flickr collection [14] as the benchmarking dataset.']],\n",
       "  [['In particular, the Photo Annotation Task [27] is a subtask inside ImageCLEF.']],\n",
       "  [['A full description of how these images were classified can be found in [17].']],\n",
       "  [['Altogether 19 research groups participated and submitted 73 runs [22].']],\n",
       "  [['Techniques that could identify non-real world scenes for removal [76] would contribute to reducing the unbalance in the datasets classes, and reduce false positive error.']]],\n",
       " [[['The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach [1].']],\n",
       "  [[\"Afin d'√©valuer notre mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF Afin d'√©valuer notre mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF (Tsikrika et al.,, 2008 ;Tsikrika et al.\"],\n",
       "   ['Les principales caract√©ristiques de la collection qui a √©t√© utilis√©e dans le cadre de la comp√©tition ImageCLEF 2008 et 2009 (Tsikrika et al.,, 2008 ;Tsikrika et al.']],\n",
       "  [['In order to experiment our model, we have used the IR collection ImageCLEF In order to experiment our model, we have used the IR collection ImageCLEF [15].']],\n",
       "  [['This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['The titles of these topics can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['More detailed per topic analyses can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).']],\n",
       "  [['To address the cross media retrieval problem, advances have been reported over the last decades [7,26,28].']]],\n",
       " [[['[6] and by Akg√ºl et al.']],\n",
       "  [['Benchmarking of content-based image classification systems focusing on medical images has been tracked at the Image Cross-Language Evaluation Forum (ImageCLEF) [44].']],\n",
       "  [['In the ad-hoc retrieval task [20], the participants were given a set of 16 textual queries with 2-3 sample images for each query.']],\n",
       "  [[', in the machine vision and image analysis communities [8], [9], where the consensus is that it promotes comparison between methods and pushes forward progress.']],\n",
       "  [['This fact is supported by the high accuracy that text query-based systems can get compared to visual query-based systems [16].']],\n",
       "  [['However, visual-based retrieval systems have generally perform poorly compared to textual approaches [18] for general medical images acquired under different modalities and orientations, and depicting different anatomical structures.']],\n",
       "  [['[7,8] This tight bound on scope necessitated the combination of many techniques, often resulting in a complex framework.']],\n",
       "  [['The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].'],\n",
       "   ['Similar to retrieval, it has shown that the classification results have better accuracy by combining text and images, in most cases, than the results using either text or image features alone [10][11][12].'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['By observing the retrieval results of the ImageCLEF track during the past several years [10,11], we concluded that the text-based retrieval systems overwhelmingly outperformed visual systems in terms of precision and other measures.']],\n",
       "  [['A detailed account on imageCLEF 2009 and 2010 with the results of the official runs from all the participants and conclusions can be found in [1,2].']],\n",
       "  [['The collection is a subset of a larger collection of 77,000 images made available by the medical image retrieval track in 2010 [9] of ImageCLEF1 evaluation.']],\n",
       "  [['43 Due to the effectiveness of CEDD and FCTH features in ImageCLEF benchmark evaluation by several groups, 23,44 we selected these for comparative evaluation with the proposed concept-based image representation approaches.']],\n",
       "  [[', in the machine vision and image analysis communities [9,10], where it promoted the comparison between methods and it pushed progress.']],\n",
       "  [[', 2008), (M√ºller et al., 2010), (M√ºller et al.']],\n",
       "  [['The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 [1,[11][12][13].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['The Idiap research team [4] combined local binary patterns (LBP) with modSIFT [5], which is used to characterize image textures.']],\n",
       "  [['Depending on the corpus, documents are represented in a variety of formsincluding text (the most common form at TREC), images [19,48], or videos [25,42].']],\n",
       "  [['The Idiap research team [4] coupled LBP and modSIFT [5].']],\n",
       "  [['The collections are shown in Table 1 and consist of two relatively small data sets: 74,902 and 77,495 images for the 2009 [48] and 2010 [49] data sets, respectively.']]],\n",
       " [[['Aachen University of Technology provided both of the manually IRMA coded datasets [35].']],\n",
       "  [['To evaluate the CBIR system, both the mean average precision (MAP) [36] and the error score proposed by the ImageCLEF campaign [37] are used as the evaluation criteria to measure the retrieval performance of in our work.'],\n",
       "   ['Idiap Idiap [37] 178.']],\n",
       "  [['The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing [25], [26], [27].'],\n",
       "   ['Table Table II provides an overview of reported results in literature [25].']],\n",
       "  [['Image Test Data -The Image Retrieval in Medical Applications (IRMA) databaseImage Test Data -The Image Retrieval in Medical Applications (IRMA) database2 is a collection of more than 14,000 x-ray images (radiographs) randomly collected from daily routine work at the Department of Diagnostic Radiology of the RWTH Aachen University3  [8,17].']],\n",
       "  [['IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks [25].']],\n",
       "  [['developed a multi-cue approach based on the support vector machine (SVM) algorithm to annotate medical images automatically by combining global and local features [6]  [7], and achieved very good results in the Im-ageCLEF 2009 medical image annotation task [8].'],\n",
       "   ['All images were classified according to the IRMA code, which is a string of 13 characters and consists of four mono-hierarchical axes: the technical code T (imaging modality); directional code D (body orientations); anatomical code A (the body region); and biological code B (the biological system examined) [8].'],\n",
       "   ['Table Table 2 shows the resulting errors by comparing the method proposed in this paper with other published results [8]  [26].']],\n",
       "  [['Indicated by our experimental results on the benchmark dataset IRMA from ImageCLEFmed09 Indicated by our experimental results on the benchmark dataset IRMA from ImageCLEFmed09 [20], the new unsupervised training scheme introduced by us had a significant impact on the retrieval performance, decreasing the total error by 21.'],\n",
       "   ['To evaluate the retrieval performance, we used a benchmark dataset called IRMA (Image Retrieval Medical Applications) dataset as part of ImageCLEFmed09 initiative To evaluate the retrieval performance, we used a benchmark dataset called IRMA (Image Retrieval Medical Applications) dataset as part of ImageCLEFmed09 initiative [20] which has 12677 images for training and 1733 images for testing, all images classified using IRMA codes.']],\n",
       "  [['The repository contains 12, 677 training and 1, 733 testing x-ray images composed of cases of patients of different ages, genders, captured at different angular positions, and depicting various pathologies [17].']],\n",
       "  [['These techniques had very good results for several years until more elaborate machine learning approaches such as support vector machines (SVMs) really improved outcomes for all classification tasks (Tommasi et al, 2010).']]],\n",
       " [[['The RobotVision@ImageCLEF task The RobotVision@ImageCLEF task [18], [16] addresses the problem of visual place classification.'],\n",
       "   ['Seven groups registered and submitted at least one run for the 2009 edition of the Robot Vision task Seven groups registered and submitted at least one run for the 2009 edition of the Robot Vision task [18], with a total of 27 submitted runs.']],\n",
       "  [['This database is used as a benchmark in indoor scene recognition (Pronobis et al., 2010).']],\n",
       "  [['According to the official webpage, ImageCLEF is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign starting from 2003 According to the official webpage, ImageCLEF is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign starting from 2003 [19].'],\n",
       "   ['0 by the IDIAP team [19].'],\n",
       "   ['Results of the best 4 systems and their scores [19,32] Team/System SIMD Proposed CVIU IDIAP Score 916.']],\n",
       "  [['The first edition of the competition The first edition of the competition [13] included room annotations, but also pose annotations.'],\n",
       "   ['Twenty-nine different groups registered to the first edition of the Robot Vision Challenge Twenty-nine different groups registered to the first edition of the Robot Vision Challenge [13], organized in 2009.']],\n",
       "  [['localization of mobile robot using visual information) in the RobotVision task [1] based on language model [2].'],\n",
       "   ['This year is the first year of RobotVision track This year is the first year of RobotVision track [1] and of LIG participation in this track.']]],\n",
       " [[['The VideoCLEF 2009 tasks included a multimedia hyperlinking task which required participants to find related resources across languages This was based on linking videos to material on the same subject in a different language The VideoCLEF 2009 tasks included a multimedia hyperlinking task which required participants to find related resources across languages This was based on linking videos to material on the same subject in a different language [12].']],\n",
       "  [['Evaluation of video retrieval is also very active and standardized, with important contributions from TRECVID,5 videoCLEF [81,82], and MultimediaEval.']]],\n",
       " [],\n",
       " [[['As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 (Gonzalo et al., 2010), we have used several measures related to question reformulations: (i) correctness (the fraction of questions for which there is at least one right reformulation offered by our system), obtaining a 75.']]],\n",
       " [[['A first attempt to release a collection of log data with the aim of verifiability and repeatability was done within the Cross-Language Evaluation Forum (CLEF) in 2009 in a track named LogCLEF which is an evaluation initiative for the analysis of queries and other user activities A first attempt to release a collection of log data with the aim of verifiability and repeatability was done within the Cross-Language Evaluation Forum (CLEF) in 2009 in a track named LogCLEF which is an evaluation initiative for the analysis of queries and other user activities [3].']]],\n",
       " [[['While in the early years much CLIR research focused on the development of translation resources and methods specifically focused on the CLIR task, the rapid advances in Statistical Machine Translation (SMT) techniques in recent years means that it has played an increasing role in the improvement of CLIR systems [10], [4], [8].']],\n",
       "  [['In the following we consider four public experimental collections, whose characteristics are reported in Table In the following we consider four public experimental collections, whose characteristics are reported in Table 3: (i) CLEF 2003, Multilingual-4, Ad-Hoc Track [1]; (ii) TREC 13, 2004, Robust Track [15]; (iii) CLEF 2009, bilingual X2EN, The European Library (TEL) Track [7]; and, (iv) TREC 21, 2012, Web Track [6].']],\n",
       "  [['We evaluate the final retrieval models on HC4 We evaluate the final retrieval models on HC4 [26], a newly constructed evaluation collection for CLIR, for Chinese and Persian, NTCIR [31] for Chinese, CLEF 08-09 for Persian [1,14], and CLEF 03 [4] for French and German.'],\n",
       "   ['While this effect deserves further investigation, we note that queries for this collection were originally created in Persian and then translated into English, possibly initially by nonnative speakers [1,14].']]],\n",
       " [],\n",
       " [[['Although unsupervised learning of morphological segmenters does not reach the detail and accuracy of hand-built analyzers, it has proven useful for many NLP applications, including speech recognition Although unsupervised learning of morphological segmenters does not reach the detail and accuracy of hand-built analyzers, it has proven useful for many NLP applications, including speech recognition [3], information retrieval [4], and machine translation [5].']],\n",
       "  [['4806 for German [24].']],\n",
       "  [['The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP [34], which has become a standard for unsupervised morphological segmentation [46].']],\n",
       "  [['4806 for German [33].']],\n",
       "  [[', , 2008(Kurimo et al., , 2009) ) or \"Multilingual parsing\" (Zeman et al.']]],\n",
       " [],\n",
       " [],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['The data comes from the CLEF-IP 2010 task [ 5], where 134 French patent topics are used to search a collection of 1.'],\n",
       "   ['Queries were constructed from the translated patent topics based on the best runs submitted to the CLEF-IP 2010 Queries were constructed from the translated patent topics based on the best runs submitted to the CLEF-IP 2010 [ 5], where most of sections in the patent topics were used to formulate the query as described in [ 3].']],\n",
       "  [['Relevant prior-art patents have common technical aspects with a patent application, and include patents that can invalidate the novelty of the invention and patents that describe the state-of-the-art in the field of the invention on which the patent application is building [ 4,5].'],\n",
       "   ['Patent prior-art search task was addressed in the CLEF-IP task in both 2009 Patent prior-art search task was addressed in the CLEF-IP task in both 2009 [ 5] and 2010 [ 4].']],\n",
       "  [['In prior-art Task defined in Clef-IP 2010, participants were asked to find the prior-art for a given patent application [16].']],\n",
       "  [['For our experiments, we used used the Lucene IR System For our experiments, we used used the Lucene IR System 6 to index the English subset of CLEF-IP 2010 and CLEF-IP 2011 datasets7  [19,21] with the default settings for stemming and stop-word removal.']],\n",
       "  [['For patent search in compounding languages, the CLIR effectiveness is usually lower than for other language pairs [3,7].'],\n",
       "   ['The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments [7].'],\n",
       "   ['Translated patent topics were processed to form queries by adding terms occurring more than twice in the title, abstract, description, and claims sections combined and all bigrams that occur more than three times, using the term frequency as a weight for these terms Translated patent topics were processed to form queries by adding terms occurring more than twice in the title, abstract, description, and claims sections combined and all bigrams that occur more than three times, using the term frequency as a weight for these terms [7].']],\n",
       "  [['Different participating teams experimented with term distribution analysis in a language modeling setting employing the document structure of the patent documents in various ways (Piroi and Tait 2010).']],\n",
       "  [['Although a query is very long in patent prior art search, a significant term mismatch between queries and relevant documents has been reported earlier Although a query is very long in patent prior art search, a significant term mismatch between queries and relevant documents has been reported earlier [Roda et al., 2009;Magdy et al.'],\n",
       "   ['Therefore an important topic in patent retrieval is Cross-Language Information Retrieval (CLIR), where the topic is a patent application in one language and the objective is to find relevant prior-art patents in another language [Roda et al., 2009;Joho et al.'],\n",
       "   ['A significant term mismatch between the patent query and relevant patents has been mentioned the main cause for low effective patent prior art search in previous studies [Roda et al., 2009;Magdy, 2012].']],\n",
       "  [['In later years, different workshops such as the Japanese NTCIR [27] and more recently the CLEF-IP evaluation track [28] added patent categorization tasks.']],\n",
       "  [['Most of the approaches to this task use standard information retrieval (IR) processes [7], [8].'],\n",
       "   ['26 [7].']],\n",
       "  [['The recent advancement in patent search is driven by the \"Intellectual Property\" task initialized by CLEF [PT10].']],\n",
       "  [['A passage retrieval task was revisited in CLEF-IP in 2012 (Piroi et al. 2012) and 2013 (Piroi et al.'],\n",
       "   ['2011), flowchart/structure recognition (Piroi et al. 2012(Piroi et al.'],\n",
       "   [', 2013)), and chemical structure recognition (Piroi et al. 2012).']],\n",
       "  [['Research labs have the opportunity to test their methods on multiple shared tasks such as PR, patent classification, image-based PR, image classification, flowchart recognition, and structure recognition (Roda et al (2010); Piroi et al (2010Piroi et al ( , 2011Piroi et al ( , 2012Piroi et al ( , 2013))).'],\n",
       "   ['CLEF-IP 2012 Collection: this dataset was created as a test collection for three tasks: passage retrieval starting from claims, chemical structure recognition, and flowchart recognition CLEF-IP 2012 Collection: this dataset was created as a test collection for three tasks: passage retrieval starting from claims, chemical structure recognition, and flowchart recognition Piroi et al (2012).']],\n",
       "  [['The CLEF-IP tracks included a patent classification task [46], [47], which provided a dataset of more than 1 million patents as the training set to classify 3,000 patents into their IPC subclasses.']],\n",
       "  [[\"Furthermore we see recent advances in neural retrieval remain neglected for document-to-document retrieval despite the task's importance in several, mainly professional, domains [24,28,29,30].\"]],\n",
       "  [[', by the CLEF-IP labs [25,26,27,28,29].']],\n",
       "  [['Although multilingual patent datasets, such as MAREC/IREC Although multilingual patent datasets, such as MAREC/IREC (Piroi, 2021) and (Roda et al., 2009;Piroi, 2010;Piroi et al.']]],\n",
       " [[[\"Enfin, les r√©sultats de 2005 soulignent le besoin d'offrir une interface donnant plus de contr√¥le √† l'individu pour la formulation et la reformulation des requ√™tes (Clough et al, 2005).\"]],\n",
       "  [[\"Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval (Clough, 2003), confirming Flank's conclusion that image CLIR can be made to work.\"],\n",
       "   ['As part of preparations for the formation of the imageCLEF collection As part of preparations for the formation of the imageCLEF collection (Clough and Sanderson, 2003), a preliminary evaluation of image CLIR was conducted on the St.']],\n",
       "  [['In 2003, it started an image retrieval task (Clough and Sanderson, 2003).']],\n",
       "  [['The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']]],\n",
       " [[['Research on SCR initially investigated IR for planned speech content such as news broadcasts and documentaries [1], [2].']],\n",
       "  [[', news [8], or TED talks [9,16]).']]],\n",
       " [],\n",
       " [[['The work described in this paper provides a complementary capability to that provided by crosslingual question answering systems, as described, for example, in The work described in this paper provides a complementary capability to that provided by crosslingual question answering systems, as described, for example, in [4], [5] and [6].']],\n",
       "  [['2006;L√∂√∂f, Gollan, and Ney 2009), information retrieval (Nie 2010; Grefenstette 2012), question answering (Magnini et al. 2004;Neumann and Sacaleanu 2005), and plagiarism detection (Potthast et al.']],\n",
       "  [['More information about multi lingua system can be found in Magnini et al. (2003).'],\n",
       "   ['system developed by The Health on the Net Foundation supports English, French and Italian, while DIOGENE system supports only two languages: English and Italian, but in contrast to cross-lingual systems the question are asked in either Italian or English, retrieval of information is carried out in Italian or English, and the answer is given in the language of the query (Magnini et al., 2003).']],\n",
       "  [['Multilingual QA Much recent effort has been made to create non-English QA datasets to over- Our XOR-TYDI QA is also closely related to QA@CLEF 2003-2008 Multilingual QA Much recent effort has been made to create non-English QA datasets to over- Our XOR-TYDI QA is also closely related to QA@CLEF 2003-2008 (Magnini et al., 2003(Magnini et al.']]],\n",
       " [[['The task made use of the ShARe corpus The task made use of the ShARe corpus (Pradhan et al., 2013), which contains manually annotated clinical notes from the MIMIC II database4  (Saeed et al.']],\n",
       "  [['The task of identifying mentions to medical concepts in free text and mapping these mentions to a knowledge base was recently proposed in ShARe/CLEF eHealth Evaluation Lab 2013, attracting the attention of several research groups worldwide The task of identifying mentions to medical concepts in free text and mapping these mentions to a knowledge base was recently proposed in ShARe/CLEF eHealth Evaluation Lab 2013, attracting the attention of several research groups worldwide (Pradhan et al., 2013).']],\n",
       "  [['In CLEF 2013(Pradhan et al., 2013), the challenge was to recognize medication-related concepts.']],\n",
       "  [['Many clinical natural language processing systems have been developed to extract information from text for various downstream applications [12,13] but have challenges in performance and portability [14][15][16][17].']],\n",
       "  [['Specifically, in the clinical domain, there are only a handful of published datasets: ShARe2013 [14], SemEval-2014 Task 7 [15], SemEval-2015 Task 14 [16], and the recent MCN dataset [17].']],\n",
       "  [[', 2007;Pradhan et al., 2013), genes (Szklarczyk et al.']],\n",
       "  [[', 2014) and previous challenge 2013 (Pradhan et al., 2013), we had focused on the task of named entity recognition for disorder mentions in clinical texs, along with normalization to UMLS CUIs.']],\n",
       "  [['The original task is a joint entity recognition and normalization tasks [45].']],\n",
       "  [['Besides their usage of OnteNotes (Pradhan et al. 2013) with only a single entity type per task, we further use the larger and more complicated Few-NERD (Ding et al.']],\n",
       "  [['2007;Pradhan et al. 2013), genes (Szklarczyk et al.']],\n",
       "  [['The considered models performed also in line with the models that took part in (i) Task 1 of the ShARe/CLEF eHealth Evaluation Lab 2013 (Mowery et al. 2013), which focused on recognition of disorder entities in clinical reports written in English and (ii) Task 3 of the DEFT 2020 challenge (Cardon et al.'],\n",
       "   ['In the clinical domain, BNER has gained even more attention since some annotated datasets have become available in challenges such as ShARe/CLEF eHealth Evaluation Lab 2013 In the clinical domain, BNER has gained even more attention since some annotated datasets have become available in challenges such as ShARe/CLEF eHealth Evaluation Lab 2013 (Mowery et al. 2013), BC5CDR (Li et al.']],\n",
       "  [['com/fonshartendorp/dutch _ biomedical _ entity _ linking published (Pradhan et al., 2013), encouraging the development and evaluation of pure BEL-models without possible propagation of errors from the entity recognition module.']],\n",
       "  [['Content Generation Content Generation [124] generating new medical descriptions or knowledge based on a given input  [127] 2012 ‚àº4K English RE Link ShARe13 [128] 2013 ‚àº29K English NER Link GENIA13 [129] 2013 ‚àº5K English EE Link NCBI [21] 2014 ‚àº7K English NER Link ShARe14 [130] 2014 ‚àº35K English NER Link CADEC [20] 2015 ‚àº7.']]],\n",
       " [],\n",
       " [[[\"For example, in the ImageCLEF competition [12], the organizers introduced this scalability requirement by adding the concept as an input to the participants' systems rather than giving a pre-defined vocabulary of concepts, while in the ImagneNet competition they had to classify images with respect to a vocabulary of 1000 concepts.\"]],\n",
       "  [['It also provides a development and test sets of 1,000 and 2,000 images, respectively, both manually annotated for 95 and 116 concepts [57].'],\n",
       "   ['In this experiment, following the second strategy, we compare our approach with SVM classifiers learned by the provided precomputed BoW In this experiment, following the second strategy, we compare our approach with SVM classifiers learned by the provided precomputed BoW [57].'],\n",
       "   ['6 (for more detailed results see [57] 4 ).']],\n",
       "  [['The first set is a set of 250K images [13], collected by querying web image search engines.']],\n",
       "  [['All these statements are corroborated through extensive experiments measuring approximation accuracy and discrimination power as well as efficiency using different image annotation benchmarks (namely ImageCLEF Photo Annotation [38] and COREL5k [39]) as well as another classification task using the Banana dataset.'],\n",
       "   ['We consider two challenging and widely used annotation benchmarks: ImageCLEF [38], COREL5k [39], and also Banana (see details below).'],\n",
       "   ['The discrimination power of the learned DMN and DKN networks is measured following the protocol defined by challenge organizers and data providers (see The discrimination power of the learned DMN and DKN networks is measured following the protocol defined by challenge organizers and data providers (see [38] for ImageCLEF and [39] for COREL5k; see also extra details below).'],\n",
       "   ['denoted MF-C and MF-S) as well as the Mean Average Precision (MAP) [38]; high values of these measures imply better performances.']],\n",
       "  [['Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.']],\n",
       "  [['These models have been successfully applied to different pattern recognition tasks including image category recognition especially for small or mid-scale training problems (see for instance [24,33,46,14,62,52,23]).']],\n",
       "  [['More recently, an extension of kernels known as deep kernel networks (DKNs) has attracted a particular attention [3][4][5][6][7][8] following the resurgence of neural networks [9,10].'],\n",
       "   ['ImageCLEF dataset ImageCLEF dataset [10] contains more than 250k (training, dev and test) images belonging to 95 different concepts.']],\n",
       "  [['‚Ä¢ ImageCLEF ‚Ä¢ ImageCLEF [71]: it consists of more than 250k images belonging to 95 concepts and is split into training, dev and test data; we only consider the dev set, which includes 1,000 images equally split between training and testing, as the ground-truth is released on this dev set only.']]],\n",
       " [],\n",
       " [],\n",
       " [[['In the 2013 edition [5], the best textual run achieved the same performance as the best technique using both textual and visual features [6].']],\n",
       "  [['For evaluation, we selected some of the representative approaches that are known to perform well in many benchmarking scenarios [12,39,40,41] as well as which are adapted to our experimentation tasks (genre and action -based retrieval).']],\n",
       "  [['Furthermore, there is an evidence that the combination or fusion of information from textual and visual sources can improve the overall retrieval quality [17,27].'],\n",
       "   ['ImageCLEF 9 proposes a hierarchy of image types for document images occurring in the biomedical open access literature [17], Figure 4 shows the proposed hierarchy.'],\n",
       "   ['Compound figure separation is therefore a required first step to retrieving focused figures [17].'],\n",
       "   ['Text retrieval often has much better performance than visual retrieval in medical retrieval [17], therefore the right combination strategies need to be chosen to really improve performance.'],\n",
       "   ['In the past text retrieval for these tasks has obtained much better information that visual retrieval [17], but combinations can profit from the advantages of the two.'],\n",
       "   ['Currently, the used of only visual information still achieves low retrieval performance in this task and the combination of text and visual search is improving and seems promising Currently, the used of only visual information still achieves low retrieval performance in this task and the combination of text and visual search is improving and seems promising [17].'],\n",
       "   ['One of the main goals of the medical task of ImageCLEF (Image-CLEFmed) [17] is to investigate the effectiveness of combining text and images for medical image-and case-based retrieval [14].']],\n",
       "  [['Since 2004, the medical task of ImageCLEF (Im-ageCLEFmed) aims at evaluating the performance of medical image retrieval systems [7,8].'],\n",
       "   ['The goal of this task is to evaluate systems which, given a case including images and a textual description (anamnesis), retrieve articles describing cases that are useful for a differential diagnosis or match the exact diagnosis of the query [7].'],\n",
       "   ['A class hierarchy was proposed including diagnostic images, generic biomedical illustrations and compound or multi-plane images with several sub categories [7].'],\n",
       "   ['Therefore, in ImageCLEFmed 2013 a specific track on compound figure separation was added [7].'],\n",
       "   ['The data and evaluation scenario used in this text is reused from the ImageCLEFmed 2013 benchmark The data and evaluation scenario used in this text is reused from the ImageCLEFmed 2013 benchmark [7].']],\n",
       "  [['14 In 2007, its data set included nearly two thousand de-identified radiology reports in English from a US radiology department for children and in 2011, over a thousand suicide notes in English were used.']],\n",
       "  [['Therefore, medical image retrieval has attracted much more attention in recent years [11,12,7,4].'],\n",
       "   ['Therefore, medical image retrieval has attracted much more attention in recent years [11,12,7,4].']],\n",
       "  [['This paper discusses the details of the implementation and evaluates it using the ImageCLEFmed 3 2013 dataset This paper discusses the details of the implementation and evaluates it using the ImageCLEFmed 3 2013 dataset [14].'],\n",
       "   ['The ImageCLEFmed 2013 The ImageCLEFmed 2013 [14] modality classification dataset was used in this study.'],\n",
       "   ['Better accuracy scores than the ones reported in this paper were obtained by two participants in ImageCLEFmed 2013 [14](81.']],\n",
       "  [['Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].'],\n",
       "   ['Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].'],\n",
       "   ['Figure 2 shows that hierarchy of images classes that was used [10,11] to classify all subfigures into types.'],\n",
       "   ['These figures were distributed for the Im-ageCLEFmed 2016 multi-label and subfigure classification tasks7  [11] together with the figure captions.'],\n",
       "   ['More information can be found in the working notes of CLEF 2016 [11].']],\n",
       "  [['We briefly summarize the metrics here; please see [10], [31] for full details.']],\n",
       "  [['The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].'],\n",
       "   ['The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].'],\n",
       "   ['Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].'],\n",
       "   ['7% in ImageCLEF2016 [6].'],\n",
       "   ['The best results are obtained by a combination of cross-media predictions [5,6].'],\n",
       "   ['Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].'],\n",
       "   ['7% in ImageCLEF2016 [6].'],\n",
       "   ['For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset [5,6] using a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF 2016 In ImageCLEF 2016 [6], they expand their training set to 20,997 figures and reduce the size of the test set to 3456.']],\n",
       "  [['Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently [58].']],\n",
       "  [['Take ImageCLEF medical [5,33,34] as an example; it provides thousands of labeled medical images for modality classification, which is a much smaller amount than the ImageNet dataset [20], which contains 1.'],\n",
       "   ['The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].'],\n",
       "   ['The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].'],\n",
       "   ['For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.'],\n",
       "   ['In ImageCLEF2016 [34], they expand the training set to 6776 figures and the test set to 4166 figures.'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].'],\n",
       "   ['We obtained good performance We obtained good performance [36] using CNN-6 in the Compound Figure Detection Task [33,34].']],\n",
       "  [['Extending the prior work inclusion criterion from text to other data modalities, the ImageCLEF lab included annual shared tasks on biomedical image processing from 2005 to 2013 [29][30][31].']],\n",
       "  [['To evaluate our extraction performance, we use the standard evaluation metrics of precision and recall, defined as: is greater than 3/5 To evaluate our extraction performance, we use the standard evaluation metrics of precision and recall, defined as: is greater than 3/5 [19].']],\n",
       "  [['A similar problem was later reintroduced as the ImageCLEF subfigure classification sub-task in 2015 and 2016 which focused on modality classification of individual subfigures of compound figures, in effect removing the \"COMP\" or compound figure modality [14,15].']],\n",
       "  [['In addition, the feature extraction process is computationally expensive and demands expertise in developing algorithms, requiring extensive labeling and accounting for the limited visibility and variability in morphology and position of the region of interest (ROI) for modality detection [8].'],\n",
       "   ['National Library of Medicine (NLM), the ImageCLEF2013 modality classification challenge [8], and the World Wide Web.']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.'],\n",
       "   ['The evaluation required to have a minimum overlap for the subfigure division between the ground truth and the data supplied by the groups in their runs (Garc√≠a Seco de Herrera et al, 2013).']],\n",
       "  [['Text has been used in most retrieval applications [10] but has also obtained very good results in modality classification [17,2], as it is complementary to visual information.']],\n",
       "  [['After the evolution of ImageCLEF, three additional data sets were added: 230,088 images for the 2011 [50] data set, and 306,539 images for both the 2012 [51] and 2013 [52] data sets.']]],\n",
       " [],\n",
       " [[['Entity ranking has recently attracted a lot of attention from researchers, especially in the context of the INEX and TREC initiatives Entity ranking has recently attracted a lot of attention from researchers, especially in the context of the INEX and TREC initiatives [11,2,19].']],\n",
       "  [['In 2012, INEX introduced the Linked Data track In 2012, INEX introduced the Linked Data track [74] with the aim to investigate retrieval techniques over a combination of textual and highly structured data.']]],\n",
       " [],\n",
       " [[['Both, the identification of interesting and useful contents from large text-streams is a crucial issue in social media [11], and they have been widely employed for evaluation purposes in the context of Twitter [12], [13], [14].'],\n",
       "   ['As reported in [39], there are no special studies regarding human judgement on text informativeness; however, it is a common evaluation criterion in the INEX Tweet Contextualization task at CLEF [40], [12], [13].'],\n",
       "   ['(13)Best ES Manual NR vs.']],\n",
       "  [['Among these few, there is the IRIT [32] system that reached best readability scores in 2013 as shown in Table 7 and also in 2014 as reported in [9].']],\n",
       "  [['A baseline system composed of an IRS and an ASS has been made available online 3 .']]],\n",
       " [[['Although there are corpora available for other BioNLP tasks, such as text classification 16 and question answering 17 , these are not covered in this survey.']],\n",
       "  [[\"Question Answering for Machine Reading Evaluation (QA4MRE): Biomedical Text about Alzheimer's Disease QA4MRE2 for biomedical data [25] differs from TREC datasets because the focus of the dataset is on passage comprehension and multiple answers are already provided with each question.\"]],\n",
       "  [['The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease [28].']]],\n",
       " [[['The entrance exam task was first proposed in 2013 as a pilot task The entrance exam task was first proposed in 2013 as a pilot task [12] in the Question Answering for Machine Reading Evaluation (QA4MRE) lab, which has been offered at the CLEF conference1 since 2011 [10,11].'],\n",
       "   ['42 [12], while one of our systems got 0.']]],\n",
       " [[['The QALD 2 proceedings are included in ILD 2012, QALD 3 [25] and QALD 4 [137]  SQA Surveys For each participant, problems and their solution strategies are given: Athenikos and Han [9] give an overview of domain specific QA systems for biomedicine.']],\n",
       "  [['The main objective of question answering over linked data [17,26] is to facilitate, in part, multilingual access to the information originally produced in different culture and language.']],\n",
       "  [['In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series [20][21][22][23][24][25][26][27][28], which is a series of benchmarks for evaluating KGQA systems.']]],\n",
       " [],\n",
       " [],\n",
       " [[['This problem has been also addressed in PAN Workshop and Competition: Uncovering Plagiarism, Authorship and Social Software Misuse (Argamon, Juola 2011, Juola, Stamatos 2013).'],\n",
       "   [\"Current state-of-the art authorship identification systems achieve an F1-score of 75% on the PAN'13 corpus containing various documents in Spanish, English and Greek (Juola, Stamatos 2013).\"],\n",
       "   [\"Current state-of-the art authorship identification systems achieve an F1-score of 75% on the PAN'13 corpus containing various documents in Spanish, English and Greek (Juola, Stamatos 2013).\"]]],\n",
       " [[['Wright, Chin 13 trained support vector machine to classify the Five Factor personality.']],\n",
       "  [['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter, see PAN [18] and RepLab [19] overviews for more details.'],\n",
       "   ['The description of the Author Profiling task at CLEF PAN 2014 [18] provides a nice overview of the recent progress in this area.']],\n",
       "  [['Several studies have been proposed for user profiling Several studies have been proposed for user profiling [5] as well as the studies proposed for the identification of user roles on social media [6], [7], etc.']],\n",
       "  [[\"For what concerns the information about gender, it could be inferred form the user's photography and name, by following a methodology similar to the one exploited in [57] .\"]],\n",
       "  [[\"Author profiling, in general, is used to determine an author's gender, age, native language, personality type, etc [11].\"]],\n",
       "  [['While a complete survey of the results of the PAN initiative is out of the scope of this paper, we refer to the latest overviews of the respective tasks, namely for authorship attribution [19], authorship verification [33], author profiling [28], and the two subtasks of reuse detection, text alignment and source retrieval [15,27].']],\n",
       "  [[', 2014;Rangel et al., 2015).']],\n",
       "  [['Rangel et al (2013) presented the results of the first International Author Profiling Task in which, researchers examined blogs written in two different languages: English and Spanish.']],\n",
       "  [[', 2014;Rangel et al., 2015;Schler et al, 2006;Tausczik and Pennebaker, 2010).'],\n",
       "   [', 2014;Rangel et al., 2015).'],\n",
       "   ['The prize winners of the 15th evaluation lab on digital text forensics PAN 2015, which was held in a bid to find the most accurate ways of identifying the gender, age, and psychological traits in accordance with the Five Factor Theory (extroversion, emotional stability/neuroticism, agreeableness, conscientiousness, openness to experience) of Twitter users (Rangel et al., 2015) applied two types of features.']],\n",
       "  [['Automatic methods for personality trait recognition have been studied for a while in natural language processing Automatic methods for personality trait recognition have been studied for a while in natural language processing [32], [33], [34], [35], [36], [37].']],\n",
       "  [['Numerous works on the topic have been published based on the results of the shared Author Profiling Tasks at digital text forensics events by PAN initiative [2,5,7,[27][28][29][30].']],\n",
       "  [['Majority of approaches at PAN-AP 2013 [18] and PAN-AP 2014 [19] used combinations of style-based features such as frequency of punctuation marks, capital letters, quotations, and so on, together with POS tags and content-based features such as bag of words, dictionary-based words, topic-based words, entropy-based words, etc.']],\n",
       "  [['[9] has a consolidated list of 21 candidate models and classified the authors of English or Spanish texts based on their gender and age.']],\n",
       "  [['In 2013 In 2013 [10], 2014 [11] and 2015 [12] PAN competition age and gender profiling was done on the English and Spanish datasets with the traditional supervised machine learning approaches: Logistic Regression, Random Forest, SVMs, etc.']],\n",
       "  [['The other source was conversations taken from the PAN13 data set which was originally proposed for predicting age and gender [10].'],\n",
       "   ['In this work, the data was collected from two sources: child grooming (Internet source) In this work, the data was collected from two sources: child grooming (Internet source) [9] and non-grooming (dataset source) [10] conversations.']],\n",
       "  [[', infer the sociolinguistic characteristics of the author of the given text [Rangel et al. 2014]).'],\n",
       "   [', infer the sociolinguistic characteristics of the author of the given text [Rangel et al. 2014]).'],\n",
       "   ['We inherit the same set of baselines as in the previous experiment on the authorship verification problem, except for those studies reported in PAN2014 [Rangel et al. 2014] since we do not have the available result for direct comparison.']],\n",
       "  [['We use the following datasets for these experiments: PAN14 (Rangel et al., 2014) 4 -we include the English data from PAN14 for the following domains: BLOGS, hotel REVIEWS, and Social Media (SOME).']],\n",
       "  [['Age classes included a gap in between: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27), 30s (33-48).'],\n",
       "   ['The best results (above 80% accuracy) were obtained on English data [24].']],\n",
       "  [['is currently receiving a growing interest in the Computational Linguistics community as it is also testified by the first shared task organized in 2013 on Author Profiling at PAN 2013 (Rangel et al., 2013).']],\n",
       "  [['While the authors claim their experiments to be the first implementation of a text categorization system on Apache Spark in Python using the NLTK framework, our experiments are performed with Spark on six corpora, including approximately 150 times larger PAN-AP-13 corpus [13] with up to 8 464 237 features and The Blog Authorship Corpus with up to 11 334 188 features.']],\n",
       "  [['Unlike Facebook Unlike Facebook [10] and Twitter [11][12][13][14], there is only a handful of studies dissecting age demographics across cQA services [2,15].'],\n",
       "   ['html (accessed on 1 February 2021)) in Twitter, blogs and social media [11][12][13][14].']],\n",
       "  [['As part of the PAN at CLEF initiative [1] there have been multiple author profiling challenges in the past years, which generated a substantial body of research in this field, summarized in [18], [20] and [19].']],\n",
       "  [[', , 2014;;Rangel et al., 2015).']],\n",
       "  [['AP has a wide applicability to many problems from different fields, such as forensics or marketing (Rangel et al., 2013).']],\n",
       "  [['Text stylometry was initially popularized in the area of forensic linguistics, specifically to the problems of author profiling and author attribution (Juola, 2006;Rangel et al., 2013).']],\n",
       "  [['[125] used the PJ dataset and the dataset for author profiling at PAN 2013.']],\n",
       "  [['We recommend [39] for more information about the data.']],\n",
       "  [['Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .'],\n",
       "   [', 119 , while three 34,49,66 used data sets that were created for the PAN-CLEF author profiling shared tasks [103][104][105] .'],\n",
       "   ['A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .']],\n",
       "  [['This relates to tasks such as author profiling (Rangel et al., 2013).']],\n",
       "  [['It should be noted that the Developer Agreements of these platforms forbid the usage of their data for the purpose of surveillance or in order to perform discriminatory actions, as exemplary outlined in Pardo et al. (2013).']],\n",
       "  [[\"Acknowledging that artificial agents could benefit from understanding that different people have different perspectives could lead to a type of author profiling task, where a model is used to predict someone's opinion of a conflict or type of conflict (Rangel et al., 2013).\"]],\n",
       "  [['[10] proposed word level unigram, bigram and character level 1 to 5 -g as features and it has outperformed the other models.'],\n",
       "   ['ùë§ ùëò ‚ààùëÅ(ùë§ ùëó ) ùëö ùëó=ùëñ ùë§ ùëò ‚ààùëÅ(ùë§ ùëó ) ùëö ùëó=ùëñ (10) Equation ( 10) computes propagated ICF scores for each word in the embedded neighborhood.']],\n",
       "  [['Majority of approaches at PAN-AP 2013 Majority of approaches at PAN-AP 2013 [35] used combinations of style-based features such as frequency of punctuation marks, capital letters, quotations, and so on, together with POS tags and content-based features such as bag of words, TF-IDF, dictionary-based words, topic-based words, entropy-based words, or content-based features obtained with LSA.'],\n",
       "   ['As described in As described in [35], this corpus was collected from public repositories, retrieving posts labelled with author demographics such as gender and age 22 .'],\n",
       "   ['For age detection, we followed what was previously done in [37] and three classes where considered: 10s (13-17), 20s (23-27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).']]],\n",
       " [[['The GeoCLEF search task examined geographic search in text corpus [18].']],\n",
       "  [['Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.'],\n",
       "   ['This task was organized by Microsoft Research Asia (Mandl et al., 2007).'],\n",
       "   ['Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.']],\n",
       "  [['To detect cities, it combines a heuristic method inspired by the schema introduced in [34] with a rank-reciprocity algorithm that helps it detect misspellings (e.']],\n",
       "  [['To date, the most important large-scale evaluation is represented by the four GeoCLEF challenges, run from 2005 to 2008 [39,77].']],\n",
       "  [['But spatial evaluation campaigns like GeoCLEF (Mandl et al., 2007) do not give accurate resources (like polygons) and do not handle French documents.']]],\n",
       " [[['Li et al., 2007c).'],\n",
       "   ['Li et al., 2007c) 0.'],\n",
       "   ['Li et al., 2007c) 0.'],\n",
       "   ['Li et al., 2007c) 0.']]],\n",
       " [[['ImageCLEFPhoto used the IAPR TC-12 collection for the past three years [10,11,12], and it was extended to allow diversity measurement, by grouping the relevance judgments of existing topics into clusters that reflect relationships between relevant images in the collection.'],\n",
       "   ['Detail on the processes of topic selection and cluster assessment can be found in [12].']],\n",
       "  [['The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks.']]],\n",
       " [[['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It can be used for the evaluation of closely related tasks, for example for visual concept detection and object retrieval [54].'],\n",
       "   ['The same applies for the object retrieval task proposed in ImageCLEF2007 [54], with the difference that now images from the same collection could be used for training and testing [54].'],\n",
       "   ['The same applies for the object retrieval task proposed in ImageCLEF2007 [54], with the difference that now images from the same collection could be used for training and testing [54].']],\n",
       "  [['In the ImageClef 2007 medical image classification competition In the ImageClef 2007 medical image classification competition [56], a database of 12,000 categorized radiograph images is used.']]],\n",
       " [[['As an example of its importance, the ImageCLEF challenge has an entire task dedicated to the annotation and retrieval of medical images [124], and participants achieve impressive results for this task, in particular when combining visual and textual information together.']],\n",
       "  [['Many articles have been published on CBIR for general images [10,39] as well as medical images [21,29,30,32,33].']],\n",
       "  [['Ela √© um subconjunto da base do ImageCLEFmed 2007 (Henning M√ºller, 2008).']],\n",
       "  [[', 2007), (M√ºller et al., 2008), (M√ºller et al.']]],\n",
       " [[['2004), oft-recorded in noisy environments, mean WER for the 2006 ASR transcripts is reported as 25% (Pecina et al. 2008).']],\n",
       "  [['For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) [22] and mean average segment precision (MASP) [10].']],\n",
       "  [['Since the RSR 2011 task was a known-item search, one useful evaluation metric is the Mean Reciprocal Rank (MRR); additionally we apply a metric that evaluates the ranking and takes account of the distance between the predicted and actual jump-in point (mean Generalized Average Precision (mGAP)) [15].']],\n",
       "  [['Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.']],\n",
       "  [['The focus then shifted towards spoken content that is produced spontaneously such as interviews, lectures and TV shows [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['We used the retrieval model PL2 and QE model BO1 described in the previous section (see Equations 1,3) to calculate the results shown in Table III, which shows performance in terms of Mean Average Precision (MAP), Recall and Precision for top 10 documents (P@10).']]],\n",
       " [[['The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections [244,245].'],\n",
       "   ['First, we note that the results obtained for the query likelihood model are comparable to or better than the mean results of all the participating groups in the respective TREC Genomics [129][130][131] and CLEF Domain-specific tracks [244,245].']]],\n",
       " [],\n",
       " [[['For example, Morpho Challenge 2007 (Kurimo et al., 2007) was evaluations of unsupervised segmentation for English, Finnish, German and Turkish.']],\n",
       "  [['The correct morphological segmentations for Finnish data used in Morphochal-lenge2007 (Kurimo et al., 2007) provide a rich and varied set of words, and are readily analyzable by our sampler.']]],\n",
       " [],\n",
       " [[['We used data from CLEF-2007 and CLEF-2008 [12,18].']],\n",
       "  [['Similarly, in the CLEF 2007 Multilingual Question Answering Track Similarly, in the CLEF 2007 Multilingual Question Answering Track (Giampiccolo et al., 2007), topicrelated questions track consisted of clusters of questions, which were related to the same topic.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['For example, the inter-annotator agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-Task dataset (Agirre et al., 2007) was approximately 72.']],\n",
       "  [['We performed the experiment following the instructions for SemEval-2007 task 1 We performed the experiment following the instructions for SemEval-2007 task 1 (Agirre et al., 2007).']]],\n",
       " [[['The recent ARQMath Lab 3 (Mansouri et al., 2022) included several teams that applied models pre-trained on math: MIRMU used mathBERTa, a model based on roberta-base (Novotn·ª≥ and ≈†tef√°nik, 2022;Geletka et al.']],\n",
       "  [['We argue that this is a common size for mathematical information retrieval database, as evidenced by the ARQMath1, ARQMath2, and ARQMath3 databases from the MIR field, which contain 77, 71, and 78 queries, respectively, for their answer retrieval tasks [16].'],\n",
       "   ['During the process of collecting relevance labels for query-document pairs, we adopt an approach similar to ARQMath During the process of collecting relevance labels for query-document pairs, we adopt an approach similar to ARQMath [16].']]],\n",
       " [],\n",
       " [[[', only entities of a specific family of diseases [4][5][6][7][8][9].']],\n",
       "  [['Our query revealed a number of medical NER challenges for the Spanish language (Table 3), including CLEF eHealth (2020-21) [92][93][94][95][96][97][98][99][100][101], Iber-LEF (2020-22) [17,70,[102][103][104][105][106][107][108][109][110][111][112][113][114][115], and CLEF BioASQ (2022) [116][117][118][119].'],\n",
       "   ['Our query revealed a number of medical NER challenges for the Spanish language (Table 3), including CLEF eHealth (2020-21) [92][93][94][95][96][97][98][99][100][101], Iber-LEF (2020-22) [17,70,[102][103][104][105][106][107][108][109][110][111][112][113][114][115], and CLEF BioASQ (2022) [116][117][118][119].'],\n",
       "   ['For example, the Spanish Medical RoBERTa model, released in 2021 [48], was tested and validated in the IberLEF 2022 [17], BioASQ 2022 [116], and SocialDisNER 2022 [87,[126][127][128][129][130] challenges.']]],\n",
       " [[['Checkworthiness captures whether the claim can be factually verifed and if the content of the claim is potentially harmful or benefcial warranting a professional fact checking efort [12,25].'],\n",
       "   ['2022 competition [25].'],\n",
       "   ['The annotation task for acquiring CW-CURE was set up on the crowdsourcing platform Amazon Mechanical Turk (AMT) following closely-related previous works on claim detection The annotation task for acquiring CW-CURE was set up on the crowdsourcing platform Amazon Mechanical Turk (AMT) following closely-related previous works on claim detection [25,39].'],\n",
       "   ['competitions in CLEF [25] and vary them slightly to suit CURE content.'],\n",
       "   ['The classifcation performance is measured using precision, recall, and F1 values on the positive class as is the standard practice for claim detection evaluation The classifcation performance is measured using precision, recall, and F1 values on the positive class as is the standard practice for claim detection evaluation [25].'],\n",
       "   ['We applied the comprehensive annotation scheme proposed for claims [18] and combined it with the notion of checkworthiness [25] in our resource contribution.']],\n",
       "  [[', 2020;Nakov et al., 2022) aim at detecting \"check-worthy\" claims while Konstantinovskiy et al.'],\n",
       "   [', 2021Nakov et al., , 2022;;Stammbach et al.'],\n",
       "   [', 2021Nakov et al., , 2022;;Barr√≥n-Cede√±o et al.']],\n",
       "  [['A variety of methods, including multi-class classification (Patwari, Goldwasser, and Bagchi 2017), Distant Supervision (Vlachos and Riedel 2015) and Transformer-based models (Nakov et al. 2022) have been employed for claim identification task.']],\n",
       "  [['More sophisticated representation techniques, such as LIWC [10] and ELMo [11], have also been investigated.']]],\n",
       " [[['A key functionality of these systems is the retrieval of already debunked narratives for misinformation claims, which essentially means retrieving previously fact-checked similar claims [2][3][4].'],\n",
       "   ['Many existing methods for training debunked-narrative retrieval models rely on supervised learning techniques which typically leverage annotated pairs of misinformation claims and fact-checking articles as training data Many existing methods for training debunked-narrative retrieval models rely on supervised learning techniques which typically leverage annotated pairs of misinformation claims and fact-checking articles as training data [3,[14][15][16][17][18].'],\n",
       "   ['Lab shared task 2020, 2021 and 2022 [2,3,14,23] focus on debunked-narrative retrieval task and release different datasets for training and testing.'],\n",
       "   ['Lab task datasets which include CLEF 22 2A [3], CLEF 21 2A [14] and CLEF 20 2A [2].'],\n",
       "   ['Lab task datasets which include CLEF 22 2A [3], CLEF 21 2A [14] and CLEF 20 2A [2].'],\n",
       "   ['We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers [2,3,14,16].'],\n",
       "   ['For evaluation, we employ two widely used ranking metrics For evaluation, we employ two widely used ranking metrics [3,14]: Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP).'],\n",
       "   ['com/sshaar/That-is-a-Known-Lie (2) CLEF 22 2A-EN and 2B-EN [3] https://sites.']],\n",
       "  [['Task 2 \"asks to detect previously fact-checked claims (in two languages)\" (Nakov et al., 2022c).']]],\n",
       " [[['The issue of consumables (materials) has become a common problem in conducting effective practical (laboratory work and practical exams) and in the case of defective equipment to carry out practical work [2].']]],\n",
       " [],\n",
       " [],\n",
       " [[['The recently organised HIPE shared tasks on named entity recognition and linking in multilingual historical documents are essential first step towards alleviating this situation: both editions, HIPE-2020 [57,60] and HIPE-2022 [58,63,65], have produced significant datasets for the evaluation of NE processing systems on historical material.']],\n",
       "  [['However, using NER on historical documents can be challenging because the available pretrained models were trained on contemporary datasets [31,32].']],\n",
       "  [[', 2019), historical English (Ehrmann et al., 2022), or music recommendation terminology (Epure and Hennequin, 2023), still using American and British English.']],\n",
       "  [['3) was published and used in the context of the 2022 edition of the shared task HIPE -Identifying Historical People, Places and other Entities (Ehrmann, Romanello, Najem-Meyer, Doucet, & Clematide, 2022); the version of the corpus described in this paper (v.'],\n",
       "   ['The dataset was featured in two of the challenges into which the HIPE-2022 shared task The dataset was featured in two of the challenges into which the HIPE-2022 shared task (Ehrmann et al., 2022) was organised: the Multilingual Classical Commentary Challenge and the Global Adaptation Challenge.']],\n",
       "  [[', 2020c(Ehrmann et al., , 2022)).']],\n",
       "  [['This speeds up the process of comprehending historical content and helps to extract insightful information and connections that could have been difficult or time-consuming to obtain through manual analysis alone [14].'],\n",
       "   ['The last HIPE competition (named HIPE-2022) The last HIPE competition (named HIPE-2022) [14] was held in 2022, with the primary goal of providing new perspectives on the transferability of NER.']],\n",
       "  [['In Natural Language Processing tasks such as part-of-speech tagging, where only the tagging itself is evaluated, metrics like Precision, Recall, and F1-score can be computed directly without requiring alignment between ground truth and predicted texts In Natural Language Processing tasks such as part-of-speech tagging, where only the tagging itself is evaluated, metrics like Precision, Recall, and F1-score can be computed directly without requiring alignment between ground truth and predicted texts [7,8,16].']]],\n",
       " [[['The iDPP@CLEF 2024 competition is an initiative aimed at leveraging sensor data The iDPP@CLEF 2024 competition is an initiative aimed at leveraging sensor data [1] [2] and machine learning techniques to predict ALS progression.']]],\n",
       " [[['The pre-training is performed on the ROCO[Rad+21], MedICaT[Sub+20], and Image Retrieval in Cross-Language Evaluation Forum (ImageCLEF) caption[R√ºc+22] datasets.']],\n",
       "  [['The ROCOv2 The ROCOv2 26 dataset is based on the dataset used in the medical caption task 29 at the ImageCLEF 2023 30 , where participants had access to the training and validation sets after signing a user agreement.'],\n",
       "   ['All results are also described in detail in the overview paper All results are also described in detail in the overview paper 29 .'],\n",
       "   ['The full results are shown in the overview paper 29 .'],\n",
       "   ['The full results are shown in the overview paper 29 .']]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[[', 2019, Hopkins and King, 2010, Moreo and Sebastiani, 2022, Dias et al.'],\n",
       "   ['[Moreo and Sebastiani, 2022] or what is the percentage of plankton organisms belonging to each of the phytoplankton species in this water sample?'],\n",
       "   [', 2021] in the LeQua [Esuli et al., 2022] competition, the only competition entirely devoted to quantification held to date, ii) HistNetQ proves competitive also under the asymmetric approach too, that is, when a set of training bags is not available and must be generated from D via sampling.'],\n",
       "   ['3 The most important one was based on the datasets4 provided for the LeQua 2022 quantification competition [Esuli et al., 2022].'],\n",
       "   [', 2020] and because it was the winner of both subtasks in the competition [Esuli et al., 2022].']]],\n",
       " [[['PlantCLEF2022 PlantCLEF2022 (Go√´au et al., 2022) is an extensive dataset comprising over 4 million images and includes a wide range of 80,000 plant species.']],\n",
       "  [['The images in (C-H) stem from a species, Aralia nudicaulis, in the PlantCLEF2022 dataset (Go√´au et al., 2022).'],\n",
       "   ['Embracing this idea, PlantCLEF2022 (Go√´au et al., 2022;Xu et al.']],\n",
       "  [['Moreover, we harnessed a ViT model that was pretrained on the plant-relevant dataset PlantCLEF2022 [21].'],\n",
       "   ['Moreover, we opted for a ViT model that was pretrained on the plant-relevant dataset PlantCLEF2022 [21].']],\n",
       "  [['Powered by the high precision achieved in recent years by ML models, they focus instead on the generation of global models based on extreme datasets, consisting of millions of images representing tens of thousands of individual species (for comparison, it is estimated that the world has ‚àº300K plant species), as exemplified by the iNat Challenge [26] and PlantCLEF/LifeCLEF [27,28].'],\n",
       "   [\"We then complement these baseline results with those obtained using two other test sets: a subset of the images in PlantCLEF'22-23 [40,28] and a set of plant images automatically collected from Wikipedia.\"],\n",
       "   [\"We consider two additional test sets: a random sample of 10,000 labeled images from the PlantCLEF'22-23 [40,28] competition, and a sample of close to 1,500 images obtained from Wikipedia.\"]]],\n",
       " [[['Many bioacoustic benchmarks, tailored for machine learning applications and species detection/recognition, focus on bird sounds to address various challenges within the field, such as BirdCLEF for species classification [7], MetaAudio for few-shot classifcation [8], DCASE 2022 and 2023 for bioacoustic event detection [9], [10], or BIRB [4], a large scale generalization benchmark.']],\n",
       "  [['There are prestigious competitions like the Detection and Classification of Acoustic Scenes and Events (DCASE) focusing on audio-driven machine learning There are prestigious competitions like the Detection and Classification of Acoustic Scenes and Events (DCASE) focusing on audio-driven machine learning [7], and numerous large-scale audio datasets with high-level labeling available in the literature, including Audioset [8], Urban80k [9], and Librispeech [10], along with unique concepts such as bird identification [11], and music genre detection [12].']],\n",
       "  [[\"In In [23,24], a NN was used for bird recognition based on birds' voices.\"]],\n",
       "  [['One of the main challenges this year is the 4400 minutes of test soundscapes that must be predicted in 120 minutes of CPU time instead of 2000 minutes in BirdCLEF 2023 [3].'],\n",
       "   ['We also consider the macro-F1 score as a secondary metric, which was utilized in the 2022 edition of the BirdCLEF competition [3].']]],\n",
       " [[['It was shown during the previous edition of the challenge (see Lorieul et al., 2022) that the information on climatic conditions was essential for the spatial prediction of plant (and animal) species (see also Leblanc et al.']],\n",
       "  [['Therefore, the evaluation of species distribution models on PO data induces important evaluation biases only due to the sampling patterns, as pointed out by the previous GeoLifeCLEF campaign ( [9]).'],\n",
       "   ['For the first time in the GeoLifeCLEF series, we assembled a large test set of standardized presence-absence data, allowing to avoid the many evaluation biases due to the sampling issues of the more commonly available presence-only data, as noted the previous year For the first time in the GeoLifeCLEF series, we assembled a large test set of standardized presence-absence data, allowing to avoid the many evaluation biases due to the sampling issues of the more commonly available presence-only data, as noted the previous year [9].']],\n",
       "  [['In agreement with recent comparisons on plants 23,26 and other taxa 44,45 , we found multispecies DNNs to perform better than SSDMs in all comparisons.']],\n",
       "  [['Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification.']]],\n",
       " [],\n",
       " [[['2021), including fungi (Picek et al. 2022;Bartlett et al.']]],\n",
       " [[['The current state-of-theart AV approach, based on PAN 2021 results (Kestemont et al. 2021) is AdHominem by Boenninghoff et al.']],\n",
       "  [['[104] concluded that \"all submissions, despite their increased level of sophistication in most of the cases, were outperformed by a naive baseline based on character n-grams and cosine similarity\" thus noticing a situation of stall in the progress on AV.'],\n",
       "   ['[104] concluded that \"all submissions, despite their increased level of sophistication in most of the cases, were outperformed by a naive baseline based on character n-grams and cosine similarity\" thus noticing a situation of stall in the progress on AV.']],\n",
       "  [['AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks [31,6,7,53,8] offer a very good overview of the most recent trends in the field.'],\n",
       "   ['As a result, NNs methods are nowadays becoming more and more prevalent at PAN [6,7,53].']],\n",
       "  [['Following the framework of Authorship Verification Following the framework of Authorship Verification (Stamatatos et al., 2022;Wegmann et al.']]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[['Another notion is claimed by Bondarenko et al. (2022) in that argumentative refers to a series of words forming in sentence collection which is crawled from debate, a large collection of text passages, representing public opinion on controversial topics.'],\n",
       "   ['As in spoken discourse, which is well-known as a debate topic (Bondarenko et al., 2022), this kind of argument is very common.']],\n",
       "  [['He is an initiator of the CLEF shared task series Touch√© on argument retrieval (Bondarenko et al., 2022), and co-chaired SemEval tasks on argument reasoning comprehension (Habernal et al.']],\n",
       "  [[', 2022) and Touch√© (Bondarenko et al., 2022), who want to help participants explore the datasets without forcing them to download large volumes of data or giving participants full access to the data: it is indeed possible to host the index privately on the Hugging Face Hub and only expose access to it through a search interface.']],\n",
       "  [['In contrast, boosting interventions aim at fostering user competencies that facilitate navigating online environments and, unlike nudging, offer the advantage of upholding user autonomy, as well as remaining effective over an extended period of time [25,38].']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']],\n",
       "  [['me corpus was created for studying argument retrieval and was utilized for the shared tasks at Touch√© @ CLEF 2022 [16].']]],\n",
       " [[['For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].'],\n",
       "   ['As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.'],\n",
       "   ['The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].']],\n",
       "  [['It was then used for producing reports and overview graphs about the submitted experiments [1,3].']],\n",
       "  [['To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are [12,13].'],\n",
       "   ['This is the case, for example, of the CLEF 2005 multilingual merging track [12], which provided participants with some of the CLEF 2003 multilingual experiments as list of results to be used as input to their merging algorithms.']],\n",
       "  [[', 2005;Nunzio et al., 2005).']]],\n",
       " [[['To deal with these issues, a lot of research has been done in recent years, and the output has been presented in different styles, including research papers [7], books [8,9], doctoral dissertations [10,11], test collections [12], retrieval evaluation events [13], etc.'],\n",
       "   ['Procedure-1 computes the BM25 score of all the papers against the initial search query, which is combined with the citation analysis score to compute the final base weight of the candidate papers and ranks the initial search results in steps [6][7][8][9][10][11][12][13][14].']]],\n",
       " [],\n",
       " [[['Several QA reports [6,14] indicate that the translation errors cause an important drop in accuracy for cross-language tasks with respect to the monolingual exercises.']],\n",
       "  [['Recently there has been intensive research in this area, fostered by evaluation-based conferences such as the Text REtrieval Conference (TREC) (Voorhees, 2001b), the Cross-Lingual Evaluation Forum (CLEF) (Vallin et al., 2005), and the NII-NACSIS Test Collection for Information Retrieval Systems workshops (NTCIR) (Kando, 2005).']],\n",
       "  [['QA can be open-domain QA can be open-domain [7,8] or closed-domain [9].']],\n",
       "  [['Voorhees, 2005;Vallin, 2005).']],\n",
       "  [['We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [[[\"Enfin, les r√©sultats de 2005 soulignent le besoin d'offrir une interface donnant plus de contr√¥le √† l'individu pour la formulation et la reformulation des requ√™tes (Clough et al, 2005).\"]],\n",
       "  [['The test collection ImageCLEFMed 2005 contains Casimage (9000 images), MIR (2000 images), PEIR (33000 images), and PathoPic (9000 images) (Clough et al., 2005).']],\n",
       "  [[\"Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval (Clough, 2003), confirming Flank's conclusion that image CLIR can be made to work.\"],\n",
       "   ['As part of preparations for the formation of the imageCLEF collection As part of preparations for the formation of the imageCLEF collection (Clough and Sanderson, 2003), a preliminary evaluation of image CLIR was conducted on the St.']],\n",
       "  [['The IRMA 10000 databaseThe IRMA 10000 database1 was used in the automatic annotation task of the 2005 ImageCLEF evaluation [17].'],\n",
       "   ['Table 1 gives an overview of the best results obtained for the IRMA tasks from the ImageCLEF 2005 evaluation [17] along with the results we obtained using sparse patch histograms with and without position information.']],\n",
       "  [['The St Andrews collection has been used for the past three years at ImageCLEFThe St Andrews collection has been used for the past three years at ImageCLEF5 , the cross-language image retrieval task (Clough, M√ºller, Hersh, Deselaers, Lehmann, Grubinger, 2005;Clough, M√ºller, Sanderson, 2005;Clough and Sanderson, 2003).']],\n",
       "  [['Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.']],\n",
       "  [['ImageCLEF is another image benchmarking competition that is part of the Cross Language Evaluation Framework (CLEF) competition (Clough et al. 2004).']],\n",
       "  [['Another recent example is where users complement their traditional keyword query with additional information, such as example documents [24], tags [73], images [76,91], categories [338], or their search history [20].']],\n",
       "  [['Accordingly, this technique is less time-consuming compared to the technique that depends on texts for the purposes of indexing and retrieving [5].']],\n",
       "  [['The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['Spreading from textual search evaluations [25], evaluation campaigns have been established for image retrieval [9], as well as for video content search [69].']]],\n",
       " [[['At other times, corpora is automatically obtained (for instance, English and Czech interview recordings of Survivors of the Shoah Visual History Foundation using in CL-SDR 2006 [11] were transcribed using a ASR system with the consequent increase of transcription errors).']],\n",
       "  [['2004), oft-recorded in noisy environments, mean WER for the 2006 ASR transcripts is reported as 25% (Pecina et al. 2008).']],\n",
       "  [['These results are confirmed for a very different retrieval task of unstructured oral testimonies in the speech retrieval task introduced at CLEF 2005 [6].'],\n",
       "   ['We are currently exploring the use of our document correction method for the CLEF speech retrieval task based on oral testimonies [6].']],\n",
       "  [['While best retrieval accuracy was achieved using a monolingual evaluation task where the queries were English, our results for the cross language task were the best among those making formal submissions to the CLEF 2007 CL-SR task, showing the lowest decrease relative to monolingual performance when queries were translated from their source language to English [12].']],\n",
       "  [['The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) [11,13].']],\n",
       "  [['For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) [22] and mean average segment precision (MASP) [10].']],\n",
       "  [['The data is drawn from the Cross-Language Speech Retrieval Track of the Crosslanguage Evaluation Forum (CLEF CL-SR) (Pecina et al., 2007) collection.']],\n",
       "  [['The transcripts produced with Automatic Speech Recognition (ASR) systems tend to contain many recognition errors, leading to low Information Retrieval (IR) performance [1] unlike the retrieval from broadcast speech, where the lower word error rate did not harm the retrieval [2].']],\n",
       "  [['Since the RSR 2011 task was a known-item search, one useful evaluation metric is the Mean Reciprocal Rank (MRR); additionally we apply a metric that evaluates the ranking and takes account of the distance between the predicted and actual jump-in point (mean Generalized Average Precision (mGAP)) [15].']],\n",
       "  [['mGAP [24] awards runs that not only find the relevant items earlier in the ranked output list, but also are closer to the jump-in point of the relevant content.']],\n",
       "  [['More recently in 2005 the Cross-Language Evaluation Forum (CLEF) has started a speech retrieval track on spoken interviews [33].']],\n",
       "  [['Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.']],\n",
       "  [[', 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007;Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b).']],\n",
       "  [['Moreover, user studies have repeatedly revealed that in interactive applications real users would often willingly trade some potential of retrieval effectiveness if doing so would lead to more understandable and predictable system behavior [Zhang et al. 2007].'],\n",
       "   ['Evaluating search systems using predefined passages is at best an imperfect model of the real task faced by a speech retrieval system, but work on alternative evaluation designs has started only recently [Pecina et al. 2007].']],\n",
       "  [['In this paper we report on work carried out for the Cross-Language Evaluation Forum (CLEF) 2005 Cross-Language Speech Retrieval (CL-SR) track (White et al, 2005).'],\n",
       "   ['See (Oard et al, 2004) and (White et al, 2005) for details.']],\n",
       "  [['In all experiments, we used the training dataset from the CLEF CL-SR 2007 task [18].']],\n",
       "  [['The focus then shifted towards spoken content that is produced spontaneously such as interviews, lectures and TV shows [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['We used the retrieval model PL2 and QE model BO1 described in the previous section (see Equations 1,3) to calculate the results shown in Table III, which shows performance in terms of Mean Average Precision (MAP), Recall and Precision for top 10 documents (P@10).']],\n",
       "  [['Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].']],\n",
       "  [['(2) spoken document retrieval: podcasts can be represented by their transcripts of their spoken content -in this way podcast search is related to spoken document retrieval [28], [3], [62].']],\n",
       "  [['Retrieval from an archive of oral history has also been well-studied [15], but lacks the multispeaker, multi-genre aspect of podcasts.']]],\n",
       " [],\n",
       " [[['GeoCLEF 1 is the cross-language geographic retrieval track run as part of the Cross Language Evaluation Forum (CLEF), and has been operating since 2005 GeoCLEF 1 is the cross-language geographic retrieval track run as part of the Cross Language Evaluation Forum (CLEF), and has been operating since 2005 (Gey et al., 2005).']],\n",
       "  [['In GeoCLEF tracks (2005-2008) In GeoCLEF tracks (2005-2008) [47], the focus is to test and evaluate cross-language geographic IR (GIR), which consists of retrieval for topics with a geographic speci cation.']],\n",
       "  [['To date, the most important large-scale evaluation is represented by the four GeoCLEF challenges, run from 2005 to 2008 [39,77].']]],\n",
       " [[['For a better comparison, we calculated the nDCG p ‚Ä≤ (nDCG-prime) score, as the task organizers did [29].'],\n",
       "   ['This is significantly higher than the reported score by the ARQMath result paper [29].']],\n",
       "  [['More recently, the ARQMath labs at CLEF 2020 and 2021 have had a formula retrieval task [6,7].']]],\n",
       " [[['We evaluate Poly-DPR on BioASQ8 We evaluate Poly-DPR on BioASQ8 (Nentidis et al., 2020) dataset to see how effective the model is.']],\n",
       "  [['2019(Nentidis et al. , 2020(Nentidis et al.'],\n",
       "   ['2019(Nentidis et al. , 2020(Nentidis et al.']]],\n",
       " [[[', 2020;Shaar et al., 2020) of claims in political debates and in social media.']],\n",
       "  [[', 2019;Barr√≥n-Cedeno et al., 2020;Hidey et al.']],\n",
       "  [['Lab task 21,22 , claim matching is one of the pivotal stages during automated claim verification to find previously fact-checked claims.']],\n",
       "  [[', , 2020;;Shaar et al., 2020;Nakov et al.']],\n",
       "  [['[34] to differentiate between three core (sequential) tasks: (1) Check-worthiness, which aims to spot factual claims that are worthy of fact-checking [11,31,39,45], (2) Evidence retrieval of potential evidence for identified claims [21,49,56,60,66,70,74] , and (3) verdict prediction, which aims to establish the veracity of a claim [60,63,74].'],\n",
       "   ['[1,11,31,39,45]).'],\n",
       "   [', an annual misinformation prediction conference, has provided a dataset that contains 629 labeled English tweets pertaining to COVID-19 as a benchmark dataset [11].'],\n",
       "   ['[11] for crowd-sourcing labels to detect claims made on social media, except instead of aggregating the label, they implemented a multi-task learning approach, in which the correlated survey questions used to determine check-worthiness were modelled jointly [1].']],\n",
       "  [['Given a topic and a stream of potentially-related tweets, rank the tweets by check-worthiness for the topic [35,73].'],\n",
       "   ['Given a check-worthy claim and a collection of previously verified claims, rank these verified claims, so that those that verify the input claim (or a sub-claim in it) are ranked on top [73].'],\n",
       "   ['Given a debate segmented into sentences, together with speaker information, prioritize sentences for fact-checking [73].']],\n",
       "  [['To date, the majority of work on fact-checking algorithms used to identify \"check-worthy\" claims circulating online has focused on designing models that have high accuracy on test sets of previously fact-checked claims To date, the majority of work on fact-checking algorithms used to identify \"check-worthy\" claims circulating online has focused on designing models that have high accuracy on test sets of previously fact-checked claims [5,21,29,34].'],\n",
       "   ['[3,5,21,29,30,34]).'],\n",
       "   ['5, ùë†1 ‚àº ùêµùëíùë°ùëé (1,17), and ùë†2 ‚àº ùêµùëíùë°ùëé (5,17) produces the desired behavior in claim virality such that, across the entire distribution, false utterances travel deeper, wider, and are seen by more people than true utterances.']],\n",
       "  [[', 2018;Barron-Cedeno et al., 2020).']],\n",
       "  [[', 2020;Shaar et al., 2020;Hasanain et al.']]],\n",
       " [[['To reduce the time and effort needed for information extraction from chemical literature and patents, datasets and text mining approaches have been developed on a wide range of information extraction tasks, including named entity recognition and relation extraction To reduce the time and effort needed for information extraction from chemical literature and patents, datasets and text mining approaches have been developed on a wide range of information extraction tasks, including named entity recognition and relation extraction [7,8,9,10,11,12].']],\n",
       "  [[', 2015;He et al., 2020;Watanabe et al.']],\n",
       "  [[', 2018;He et al., 2020).']]],\n",
       " [[['The three most common information extraction tasks -named entity recognition The three most common information extraction tasks -named entity recognition (38,39,40), concept normalization (41,42), and relation extraction (Section 7) -are still active areas of research.']]],\n",
       " [[[\"As our ultimate goal is to find the connection between Moments of Change (MoC) in individuals' longitudinal online data (Task A) and other information regarding the individuals' level of risk (Task B), we wanted to repurpose as much as possible existing mental health datasets As our ultimate goal is to find the connection between Moments of Change (MoC) in individuals' longitudinal online data (Task A) and other information regarding the individuals' level of risk (Task B), we wanted to repurpose as much as possible existing mental health datasets (Losada and Crestani, 2016;Losada et al., 2020;Shing et al.\"],\n",
       "   ['We obtained the eRisk dataset (Losada and Crestani, 2016;Losada et al., 2020) upon signing a data use agreement.']],\n",
       "  [['Subtask A data can be found on (Losada and Crestani, 2016;Losada et al., 2020;Shing et al.']]],\n",
       " [[['[143] leverage a recently proposed EL toolkit REL [181] for mining historical newspapers for people, places, and other entities in the CLEF HIPE 2020 evaluation campaign [37].']],\n",
       "  [[', 2018;Ehrmann et al., 2020).']],\n",
       "  [['The HIPE-2020 corpus The HIPE-2020 corpus [62] is a medium-sized, historical news corpus in French, German and English, created as part of the HIPE-2020 shared task.'],\n",
       "   ['They were trained on newspaper materials in French, German, and English, and cover roughly 18C-21C (full details in [62] and [59]).']],\n",
       "  [['To this scope, the CLEF HIPE 2020 evaluation [11,12] gathered 13 teams on shared tasks dedicated to the evaluation of named entity processing on historical newspapers in French, German and English.']],\n",
       "  [[', PERSON, ORGANIZATION, LOCATION, PRODUCT, and DATE), as proposed by the first edition of HIPE (Identifying Historical People, Places and other Entities) [13], employing a group of eight annotators, two for each region.'],\n",
       "   ['Additionally, to enable the performance comparison of NER on historical texts and to improve the robustness of current NER systems on non-standard inputs, the first edition of the HIPE competition (named HIPE-2020) [13] was proposed in 2020.']]],\n",
       " [[['Automatically generated reports are prone to errors and low-quality textual descriptions (among other issues) [4].']]],\n",
       " [[['Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition [66] .']],\n",
       "  [['Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].']],\n",
       "  [[', 2018) and 2020 (Joly et al., 2020) improved the species identification process by including location information.']],\n",
       "  [['Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity, with several tens of thousands of recordings covering up to 1,500 species [9,10].']]],\n",
       " [[['It is important to note that similar tasks often arise also in patent retrieval It is important to note that similar tasks often arise also in patent retrieval [15,34,37,39,40] and academic document retrieval domains [23,24,38,44].']]],\n",
       " [[['html, accessed on 3 June 2021) [114] was a shared task organized within CLEF 2020 in the context of PAN, a series of scientific events, and shared tasks on digital text forensics and stylometry.']],\n",
       "  [['Up until the very recent PAN 2018 and PAN 2020 Authorship event [3,4], the most popular and effective approaches still largely relies on n-gram features and traditional machine learning classifiers, such as support vector machines (SVM) [5] and trees [6].']],\n",
       "  [['Bevendorff et al., 2020;Boenninghoff et al.']],\n",
       "  [['The PAN workshop series (Bevendorff et al. 2020;Daelemans et al.']],\n",
       "  [[', 2020), and Fanfiction (Bevendorff et al., 2020(Bevendorff et al.'],\n",
       "   ['We also use the test data from PAN2020 (Bevendorff et al., 2020) as the validation data for our Fanfiction experiments.']],\n",
       "  [['For instance, models trained on the Reddit comments [4] exhibited significantly better transfer than those trained on the corpus of Amazon Reviews [29] and the Fanfiction dataset [5].']],\n",
       "  [['AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks [31,6,7,53,8] offer a very good overview of the most recent trends in the field.'],\n",
       "   ['As a result, NNs methods are nowadays becoming more and more prevalent at PAN [6,7,53].']]],\n",
       " [[[', 2020b(Bondarenko et al., , 2021) ) featured a related track.']],\n",
       "  [['‚Ä¢ Controversial arguments: Touch√© ‚Ä¢ Controversial arguments: Touch√© (Bondarenko et al., 2020) and ArguAna (Wachsmuth et al.']],\n",
       "  [[', 2019;Bondarenko et al., 2021).']],\n",
       "  [['0 5183 300 Quora Not reported 522931 10000 Touch√©-2020 (Bondarenko et al., 2020) CC BY 4.']],\n",
       "  [[', 2020), Touch√©-2020 (Bondarenko et al., 2020), and SciFact (Wadden et al.']],\n",
       "  [['To evaluate the robustness of our approach in an out-of-domain setting we take a subset of the BEIR benchmark sets [41], due to the greater computational expense of current list-wise rankers multiple sparsely annotated test sets would be infeasible under our compute budget, as such we choose test sets with a smaller set of densely annotated topics namely, TREC Covid [42] and Touche [43].']],\n",
       "  [['Touch√©-2020 dataset (Bondarenko et al., 2020)  and categorized questions and answers across various domains.']],\n",
       "  [['We experiment with all of the following 15 MTEB retrieval datasets: Arguana We experiment with all of the following 15 MTEB retrieval datasets: Arguana [44], ClimateFEVER [10], CQADupstackRetrieval [20], DBPedia [19], FEVER [42], FiQA2018 [32], HotpotQA[49], MSMARCO [3], NFCorpus [7], NQ [28], QuoraRetrieval [41] SCIDOCS [9], SciFact [45], Touche2020 [6], TRECCOVID [43].'],\n",
       "   ['6 from https://huggingface.']],\n",
       "  [['‚Ä¢ Text Retrieval -BEIR ‚Ä¢ Text Retrieval -BEIR [53] (ArguAna [56], FEVER [54], FIQA [40], MS MARCO [7], NQ [29], Quora, SciFact [57], Touch√©-2020 [11], HotPotQA [63]): https://github.']]],\n",
       " [[['As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 (Gonzalo et al., 2010), we have used several measures related to question reformulations: (i) correctness (the fraction of questions for which there is at least one right reformulation offered by our system), obtaining a 75.']]],\n",
       " [[['To assess performance, we use the standard cluster recall at a cutoff at X images (CR@X) To assess performance, we use the standard cluster recall at a cutoff at X images (CR@X) [11], a measure of how many clusters from the ground truth are represented among the top X results provided by the retrieval system; the precision  [6] relevance GLRLM 0.']],\n",
       "  [['Relevance was more thoroughly studied in existing literature than diversification [11,31,36] and even though a considerable amount of diversification literature exists (mainly in the text-retrieval community), the topic remains important, especially in multimedia [27,33,35,39,43].'],\n",
       "   ['Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task [27] that proposes a dataset consisting of 498,920 news photographs (images and caption text) classified into sub-topics (e.'],\n",
       "   [', one of the state-of-the-art platforms) and addresses in particular the social dimension reflected in the nature of the data and methods devised to account for it; -while smaller in size than the ImageCLEF collections [27,41], the proposed dataset contains images that are already associated to topics by Flickr.']],\n",
       "  [['Relevance was more thoroughly studied in existing literature than diversification [1,2,3] and even though a considerable amount of diversification literature exists, the topic remains an important one, especially in social media [4,5,6,7,8].'],\n",
       "   ['(2) while smaller in size than ImageCLEF collections (2) while smaller in size than ImageCLEF collections [5,26], Div400 contains images that are already associated to topics by Flickr.'],\n",
       "   ['System performance was assessed on the testset using Cluster Recall at X (CR@X) -a measure assessing how many clusters from the ground truth are represented among the top X results (only relevant images considered) System performance was assessed on the testset using Cluster Recall at X (CR@X) -a measure assessing how many clusters from the ground truth are represented among the top X results (only relevant images considered) [5] and Precision at X (P@X) -measures the number of relevant photos among the top X results.']],\n",
       "  [['The increasing importance of diversity is reflected in a number of benchmarks, such as ImageCLEF 2009 [16] or MediaEval Diverse Social Images [9].']],\n",
       "  [['In terms of image search there are many ways of diversifying results; often these are implicit and involve removing near duplicates [3].']],\n",
       "  [['To effectively leverage the massive explosion of multimedia content, a large number of approaches have been proposed in the areas such as information retrieval, multimedia retrieval and computer vision [2,3,5,11,16].']],\n",
       "  [['The problems of image and text retrieval have been the subject of extensive research in the fields of information retrieval, computer vision, and multimedia The problems of image and text retrieval have been the subject of extensive research in the fields of information retrieval, computer vision, and multimedia [2], [10], [12], [27], [28].'],\n",
       "   ['These manual annotations are typically in the form of a few keywords, a small caption, or a brief image description [12], [13], [27].'],\n",
       "   ['In parallel with these developments, advances have been reported in multimodal retrieval systems [8], [9], [10], [11], [12], [13], [27].']],\n",
       "  [['[36,73,52,55,56,69]), the problem is not solved yet.']],\n",
       "  [['Recently, the idea of diversi cation for image search results has been studied by many researchers Recently, the idea of diversi cation for image search results has been studied by many researchers [26,52], and some international challenges have been also proposed to address this issue (ImageCLEF [30] and MediaEval Retrieving Diverse Social Images Task [17]).']],\n",
       "  [['image search diversification [23][24][25][26][27].'],\n",
       "   ['Another well-known example is the ImageCLEF 2009 Photo Task, which revolved around image diversification [25].']],\n",
       "  [['An effective retrieval system should also take into account the diversification of the results [8].'],\n",
       "   ['With this concept, cluster recall was introduced as a measure for diversity in image retrieval [8].'],\n",
       "   ['To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task (2008)(2009)  [8], [26], ImageCLEF Lifelogging Task (2017-2019) [27], TREC Web Track: Diversity Task (2009-2012) [28], and MediaEval Retrieving Diverse Social Images Task (2013-2017) 1 .'],\n",
       "   ['To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task To this date, several benchmark initiatives promoted the development and comparability of approaches in the context of diversification: the ImageCLEF Photo Task (2008)(2009)  [8], [26], ImageCLEF Lifelogging Task (2017-2019) [27], TREC Web Track: Diversity Task (2009-2012) [28], and MediaEval Retrieving Diverse Social Images Task (2013-2017) 1 .']]],\n",
       " [[['The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach [1].']],\n",
       "  [[\"Afin d'√©valuer notre mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF Afin d'√©valuer notre mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF (Tsikrika et al.,, 2008 ;Tsikrika et al.\"],\n",
       "   ['Les principales caract√©ristiques de la collection qui a √©t√© utilis√©e dans le cadre de la comp√©tition ImageCLEF 2008 et 2009 (Tsikrika et al.,, 2008 ;Tsikrika et al.']],\n",
       "  [['In order to experiment our model, we have used the IR collection ImageCLEF In order to experiment our model, we have used the IR collection ImageCLEF [15].']],\n",
       "  [['This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['The titles of these topics can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['More detailed per topic analyses can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).']],\n",
       "  [['To address the cross media retrieval problem, advances have been reported over the last decades [7,26,28].']]],\n",
       " [[['[6] and by Akg√ºl et al.']],\n",
       "  [['Benchmarking of content-based image classification systems focusing on medical images has been tracked at the Image Cross-Language Evaluation Forum (ImageCLEF) [44].']],\n",
       "  [['In the ad-hoc retrieval task [20], the participants were given a set of 16 textual queries with 2-3 sample images for each query.']],\n",
       "  [[', in the machine vision and image analysis communities [8], [9], where the consensus is that it promotes comparison between methods and pushes forward progress.']],\n",
       "  [['This fact is supported by the high accuracy that text query-based systems can get compared to visual query-based systems [16].']],\n",
       "  [['However, visual-based retrieval systems have generally perform poorly compared to textual approaches [18] for general medical images acquired under different modalities and orientations, and depicting different anatomical structures.']],\n",
       "  [['[7,8] This tight bound on scope necessitated the combination of many techniques, often resulting in a complex framework.']],\n",
       "  [['The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].'],\n",
       "   ['Similar to retrieval, it has shown that the classification results have better accuracy by combining text and images, in most cases, than the results using either text or image features alone [10][11][12].'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.'],\n",
       "   ['By observing the retrieval results of the ImageCLEF track during the past several years [10,11], we concluded that the text-based retrieval systems overwhelmingly outperformed visual systems in terms of precision and other measures.']],\n",
       "  [['A detailed account on imageCLEF 2009 and 2010 with the results of the official runs from all the participants and conclusions can be found in [1,2].']],\n",
       "  [['The collection is a subset of a larger collection of 77,000 images made available by the medical image retrieval track in 2010 [9] of ImageCLEF1 evaluation.']],\n",
       "  [['43 Due to the effectiveness of CEDD and FCTH features in ImageCLEF benchmark evaluation by several groups, 23,44 we selected these for comparative evaluation with the proposed concept-based image representation approaches.']],\n",
       "  [[', in the machine vision and image analysis communities [9,10], where it promoted the comparison between methods and it pushed progress.']],\n",
       "  [[', 2008), (M√ºller et al., 2010), (M√ºller et al.']],\n",
       "  [['The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 [1,[11][12][13].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['The Idiap research team [4] combined local binary patterns (LBP) with modSIFT [5], which is used to characterize image textures.']],\n",
       "  [['Depending on the corpus, documents are represented in a variety of formsincluding text (the most common form at TREC), images [19,48], or videos [25,42].']],\n",
       "  [['The Idiap research team [4] coupled LBP and modSIFT [5].']],\n",
       "  [['The collections are shown in Table 1 and consist of two relatively small data sets: 74,902 and 77,495 images for the 2009 [48] and 2010 [49] data sets, respectively.']]],\n",
       " [[['Aachen University of Technology provided both of the manually IRMA coded datasets [35].']],\n",
       "  [['To evaluate the CBIR system, both the mean average precision (MAP) [36] and the error score proposed by the ImageCLEF campaign [37] are used as the evaluation criteria to measure the retrieval performance of in our work.'],\n",
       "   ['Idiap Idiap [37] 178.']],\n",
       "  [['The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing [25], [26], [27].'],\n",
       "   ['Table Table II provides an overview of reported results in literature [25].']],\n",
       "  [['Image Test Data -The Image Retrieval in Medical Applications (IRMA) databaseImage Test Data -The Image Retrieval in Medical Applications (IRMA) database2 is a collection of more than 14,000 x-ray images (radiographs) randomly collected from daily routine work at the Department of Diagnostic Radiology of the RWTH Aachen University3  [8,17].']],\n",
       "  [['IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks [25].']],\n",
       "  [['developed a multi-cue approach based on the support vector machine (SVM) algorithm to annotate medical images automatically by combining global and local features [6]  [7], and achieved very good results in the Im-ageCLEF 2009 medical image annotation task [8].'],\n",
       "   ['All images were classified according to the IRMA code, which is a string of 13 characters and consists of four mono-hierarchical axes: the technical code T (imaging modality); directional code D (body orientations); anatomical code A (the body region); and biological code B (the biological system examined) [8].'],\n",
       "   ['Table Table 2 shows the resulting errors by comparing the method proposed in this paper with other published results [8]  [26].']],\n",
       "  [['Indicated by our experimental results on the benchmark dataset IRMA from ImageCLEFmed09 Indicated by our experimental results on the benchmark dataset IRMA from ImageCLEFmed09 [20], the new unsupervised training scheme introduced by us had a significant impact on the retrieval performance, decreasing the total error by 21.'],\n",
       "   ['To evaluate the retrieval performance, we used a benchmark dataset called IRMA (Image Retrieval Medical Applications) dataset as part of ImageCLEFmed09 initiative To evaluate the retrieval performance, we used a benchmark dataset called IRMA (Image Retrieval Medical Applications) dataset as part of ImageCLEFmed09 initiative [20] which has 12677 images for training and 1733 images for testing, all images classified using IRMA codes.']],\n",
       "  [['The repository contains 12, 677 training and 1, 733 testing x-ray images composed of cases of patients of different ages, genders, captured at different angular positions, and depicting various pathologies [17].']],\n",
       "  [['These techniques had very good results for several years until more elaborate machine learning approaches such as support vector machines (SVMs) really improved outcomes for all classification tasks (Tommasi et al, 2010).']]],\n",
       " [[['The large-scale visual concept detection and annotation task (LS-VCDT) in ImageCLEF 2009 The large-scale visual concept detection and annotation task (LS-VCDT) in ImageCLEF 2009 [13] used the MIR Flickr collection [14] as the benchmarking dataset.']],\n",
       "  [['In particular, the Photo Annotation Task [27] is a subtask inside ImageCLEF.']],\n",
       "  [['A full description of how these images were classified can be found in [17].']],\n",
       "  [['Altogether 19 research groups participated and submitted 73 runs [22].']],\n",
       "  [['Techniques that could identify non-real world scenes for removal [76] would contribute to reducing the unbalance in the datasets classes, and reduce false positive error.']]],\n",
       " [[['The RobotVision@ImageCLEF task The RobotVision@ImageCLEF task [18], [16] addresses the problem of visual place classification.'],\n",
       "   ['Seven groups registered and submitted at least one run for the 2009 edition of the Robot Vision task Seven groups registered and submitted at least one run for the 2009 edition of the Robot Vision task [18], with a total of 27 submitted runs.']],\n",
       "  [['This database is used as a benchmark in indoor scene recognition (Pronobis et al., 2010).']],\n",
       "  [['According to the official webpage, ImageCLEF is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign starting from 2003 According to the official webpage, ImageCLEF is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign starting from 2003 [19].'],\n",
       "   ['0 by the IDIAP team [19].'],\n",
       "   ['Results of the best 4 systems and their scores [19,32] Team/System SIMD Proposed CVIU IDIAP Score 916.']],\n",
       "  [['The first edition of the competition The first edition of the competition [13] included room annotations, but also pose annotations.'],\n",
       "   ['Twenty-nine different groups registered to the first edition of the Robot Vision Challenge Twenty-nine different groups registered to the first edition of the Robot Vision Challenge [13], organized in 2009.']],\n",
       "  [['localization of mobile robot using visual information) in the RobotVision task [1] based on language model [2].'],\n",
       "   ['This year is the first year of RobotVision track This year is the first year of RobotVision track [1] and of LIG participation in this track.']]],\n",
       " [[['The VideoCLEF 2009 tasks included a multimedia hyperlinking task which required participants to find related resources across languages This was based on linking videos to material on the same subject in a different language The VideoCLEF 2009 tasks included a multimedia hyperlinking task which required participants to find related resources across languages This was based on linking videos to material on the same subject in a different language [12].']],\n",
       "  [['Evaluation of video retrieval is also very active and standardized, with important contributions from TRECVID,5 videoCLEF [81,82], and MultimediaEval.']]],\n",
       " [],\n",
       " [[['To deal with these issues, a lot of research has been done in recent years, and the output has been presented in different styles, including research papers [7], books [8,9], doctoral dissertations [10,11], test collections [12], retrieval evaluation events [13], etc.'],\n",
       "   ['Procedure-1 computes the BM25 score of all the papers against the initial search query, which is combined with the citation analysis score to compute the final base weight of the candidate papers and ranks the initial search results in steps [6][7][8][9][10][11][12][13][14].']]],\n",
       " [],\n",
       " [[[\"Pour le moment, les syst√®mes de QR bilingues sont beaucoup moins performants que les syst√®mes de QR monolingues, mais les chercheurs se disent encourag√©s par les r√©sultats obtenus jusqu'√† maintenant (Vallin et al, 2005).\"]],\n",
       "  [['Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 [6].'],\n",
       "   ['Besides, section 5 presents and discusses the results obtained using all official English questions of QA CLEF 2004 [8] and 2006 [6].'],\n",
       "   ['This fact has been confirmed in the last edition of CLEF 2006 [6].'],\n",
       "   ['Nowadays, at CLEF 2006 [6], three different approaches are used by CL-QA systems in order to solve the bilingual task.'],\n",
       "   ['Furthermore, this affirmation is corroborated checking the official results on the last edition of CLEF 2006 [6] where our method [3] has being ranked first at the bilingual English-Spanish QA task.'],\n",
       "   ['07% at CLEF 2006) and than other current bilingual QA systems [6].']],\n",
       "  [['BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006).'],\n",
       "   ['For this purpose we have used the CLEF 2006 set of questions, the EFE corpora, the evaluation measures34 proposed by the CLEF organization (Magnini et al, 2006) and our official results in this competition.']],\n",
       "  [['The Cross-Language Evaluation Forum or CLEFThe Cross-Language Evaluation Forum or CLEF3 , established in 2000, promotes multilingual question answering, where the question is posed in a different language than the language of the documents in the repository [73].']],\n",
       "  [['We compared the results of the best systems of both EQueR and CLEF QA task in 2004 (Valin et al., 2004) and found them to be consistent.']],\n",
       "  [['We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [[['Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.']]],\n",
       " [[['Research on SCR initially investigated IR for planned speech content such as news broadcasts and documentaries [1], [2].']],\n",
       "  [['Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].']]],\n",
       " [[['The Early risk prediction on the Internet (eRisk) (Parapar et al., 2022) corpus focuses on detecting various text-related indicators of mental health conditions in Reddit posts.']]],\n",
       " [[[', 2022;Joly et al., 2022), improving on traditional methods using only environmental data.']]],\n",
       " [[[', 2022) are initialized from BERTbase and RoBERTa-base, and further pretrained on Math StackExchange19 with extra LaTeX tokens to better tokenize math formulas for ARQMath-3 tasks (Mansouri et al., 2022).']]],\n",
       " [[['Another notion is claimed by Bondarenko et al. (2022) in that argumentative refers to a series of words forming in sentence collection which is crawled from debate, a large collection of text passages, representing public opinion on controversial topics.'],\n",
       "   ['As in spoken discourse, which is well-known as a debate topic (Bondarenko et al., 2022), this kind of argument is very common.']],\n",
       "  [['He is an initiator of the CLEF shared task series Touch√© on argument retrieval (Bondarenko et al., 2022), and co-chaired SemEval tasks on argument reasoning comprehension (Habernal et al.']],\n",
       "  [[', 2022) and Touch√© (Bondarenko et al., 2022), who want to help participants explore the datasets without forcing them to download large volumes of data or giving participants full access to the data: it is indeed possible to host the index privately on the Hugging Face Hub and only expose access to it through a search interface.']],\n",
       "  [['In contrast, boosting interventions aim at fostering user competencies that facilitate navigating online environments and, unlike nudging, offer the advantage of upholding user autonomy, as well as remaining effective over an extended period of time [25,38].']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']],\n",
       "  [['me corpus was created for studying argument retrieval and was utilized for the shared tasks at Touch√© @ CLEF 2022 [16].']]],\n",
       " [[[', 2022;Nentidis et al., 2022).']]],\n",
       " [[['The LeQua shared task (Esuli et al. 2022) involved creating quantification systems for sentiment and the topic of product reviews.']]],\n",
       " [[['GRAM2VEC was developed on the publicly available PAN22 corpus (Bevendorff et al., 2022), which we refer to throughout the paper.']]],\n",
       " [[['14 These ML techniques produce an average C-index of around 0.']]],\n",
       " [[['Other datasets also focus on historical [52] or music [53] domains.']]],\n",
       " [],\n",
       " [[['In the first year of running SimpleText as a track at CLEF 2022, its counted a total of 62 registered teams In the first year of running SimpleText as a track at CLEF 2022, its counted a total of 62 registered teams [11].'],\n",
       "   ['However, several runs for Task 3 inserted some context or definition for difficult terms in additional to language simplification [25,11].'],\n",
       "   ['However, we observed that direct application of large pre-trained models often keeps sentences unchanged as in case of the runs of PortLinguE with 36% of unchanged sentences [11,17].'],\n",
       "   [\"In the last year's edition, 40 articles, 20 of each source, were made available [11].\"],\n",
       "   ['Evaluation Topical relevance was only evaluated last year with a 0-5 score on the relevance degree towards the content of the original article Evaluation Topical relevance was only evaluated last year with a 0-5 score on the relevance degree towards the content of the original article [11].'],\n",
       "   ['retrieving difficult terms, we will use the train data collected in 2022 [11].'],\n",
       "   ['Evaluation As in 2022, we will evaluate complex concept spotting in terms of their complexity and the detected concept spans Evaluation As in 2022, we will evaluate complex concept spotting in terms of their complexity and the detected concept spans [11].'],\n",
       "   ['Our training data is a truly parallel corpus of directly simplified sentences (648 sentences for now) coming from scientific abstracts from the DBLP Citation Network Dataset for Computer Science and Google Scholar and PubMed articles on Health and Medicine [7,8,11,10].'],\n",
       "   ['Our training data is a truly parallel corpus of directly simplified sentences (648 sentences for now) coming from scientific abstracts from the DBLP Citation Network Dataset for Computer Science and Google Scholar and PubMed articles on Health and Medicine [7,8,11,10].'],\n",
       "   ['The SimpleText corpus [11] contains directly simplified sentences, and is not much smaller than existing high-quality corpora like NEWSELA [30] (2,259 sentences).'],\n",
       "   ['As in 2022, we evaluate the complexity of the provided simplifications in terms of vocabulary and syntax as well as the errors (Incorrect syntax; Unresolved anaphora due to simplification; Unnecessary repetition/iteration; Spelling, typographic or punctuation errors) [11].']]],\n",
       " [[['lab has focused on developing technology to assist the journalist fact-checker during the main steps of verification [7,8,18,19,[47][48][49]51,52].'],\n",
       "   ['The Arabic data consists of 3k tweets on topics such as COVID-19 and politics [1,47].']],\n",
       "  [[', 2021bNakov et al., , 2022)).']],\n",
       "  [['prepared three different tasks, aiming to solve different problems in the factchecking pipeline (Nakov et al., 2022b).']],\n",
       "  [['@CLEF, which, for example, in the 2022 edition dealt with the infodemic phenomenon developed during the Covid-19 pandemic, launching tasks on recognizing tweets worth checking based on their veracity, and useful for fact checking others [22].']]],\n",
       " [],\n",
       " [[['Next, tasks related datasets (Ionescu et al., 2022) can help to strengthen datasets presented (Section 2.']],\n",
       "  [['In the imageCLEFmedical Caption Task 2022, organized by Cross-Language Education and Function (CLEF), a medical image captioning challenge was held using datasets consisting of single CT images, MRI images, X-ray images, and their corresponding captions [7].'],\n",
       "   ['20 [7].']],\n",
       "  [['The data originates from a large-scale collection of images taken from coral reefs around the world as part of a coral reef  monitoring project with the Marine Technology Research Unit and is hosted by the ImageCLEF The data originates from a large-scale collection of images taken from coral reefs around the world as part of a coral reef  monitoring project with the Marine Technology Research Unit and is hosted by the ImageCLEF [11] conference.']]],\n",
       " [[['Indeed, statistical analysis of the results in the multilingual tracks of the 2001 and 2002 campaigns has shown that performance changes need to be rather large to find statistically significant differences between experiments (Braschler 2002(Braschler , 2003)).']],\n",
       "  [['We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign [3][4][5][6] for bilingual ad-hoc retrieval tracks 6 .']]],\n",
       " [[[\"El objetivo del iCLEF'2002 El objetivo del iCLEF'2002 (Gonzalo and Oard, 2002) fue proporcionar un marco de referencia com√∫n para realizar experimentos comparando dos sistemas de recuperaci√≥n de informaci√≥n transling√ºe que permitan a un usuario que desconoce el idioma de los documentos realizar una expansi√≥n interactiva de la consulta, una selecci√≥n interactiva de documentos (al igual que el a√±o anterior), o ambas opciones a la vez.\"]],\n",
       "  [['(Oard & Gonzalo, 2002)).']],\n",
       "  [['The interactive CLEF experimental framework The interactive CLEF experimental framework (Gonzalo & Oard, 2002) was followed, but additional measurements (both objective and subjective) were recorded.']]],\n",
       " [[['The results of this pilot investigation were first presented at the CLEF 2002 Workshop and are reported in Jones and Federico (2003).']]],\n",
       " [[['This task has been developed within the CLEF 2016 eHealth Evaluation Lab, which aims to foster the development of approaches to support patients, their next-of-kin, and clinical sta‚Üµ in understanding, accessing and authoring health information [15].']],\n",
       "  [['Systems developed to face this challenge varied greatly [65,66].']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [['The organizers shared a dataset of 2,309 unannotated drug 4 Although CLEF eHealth is organized every year [13,14], its main focus is multi-linguality and information retrieval rather than clinical NLP Table 1 Clinical NLP Challenges, the tasks they posed, and the number of participating teams, since 2015, ordered by data sensitivity.']],\n",
       "  [['Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks [16,41,42].']],\n",
       "  [[\"In 2013-2017, these tasks were patient-centric, with clinicians also considered from 2015, but in 2017 a pilot task on Technology Assisted Reviews (TAR) to support health scientists and policymakers' information access was also introduced (Suominen et al, 2013;Kelly et al, 2014;Goeuriot et al, 2015;Kelly et al, 2016;Goeuriot et al, 2017).\"]],\n",
       "  [['In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 [6], 2010 [7], 2012 [8] and 2014 [9], the NLP challenges organized by SemEval in 2014 [10], 2015 [11] and 2016 [12], and the NLP challenges organized by ShARe/CLEF in 2013 [13] and 2014 [14].']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4,5,8,10,16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German.']],\n",
       "  [['The methods mentioned above have also been applied for clinical entity recognition and RE, such as the NLP challenges organized by i2b2 in 2009 [9], 2010 [10], 2012 [11], and 2014 [12], the NLP challenges organized by SemEval in 2015 [13] and 2016 [14], the NLP challenges organized by ShARe/CLEF in 2013 [15] and 2014 [16], and the NLP challenges organized by BioCreative/OHNLP in 2018 [17].']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']]],\n",
       " [[['While many different types of medical images are collected While many different types of medical images are collected [4,5], the source of the images are not always identified appropriately [6,7].'],\n",
       "   ['We used the medical Subfigure Classification dataset used in the ImageCLEF 2016 competition We used the medical Subfigure Classification dataset used in the ImageCLEF 2016 competition [7].']]],\n",
       " [[['Official results of the evaluation (based on the score S detailed above) are synthesized in the overview paper of LifeCLEF lab [32] and further developed in the working note of the plant task [19].']],\n",
       "  [['Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014.'],\n",
       "   ['This was also the case of the PlantCLEF challenges [37,38,43], where the deep learning submissions [41,42,65,66] outperformed combinations of hand-crafted methods significantly.']],\n",
       "  [['Over the last decade, there have been increasingly impressive performance gains achieved by the paradigm shift to deep learning, including bioacoustics [29].'],\n",
       "   ['69 the following year when deep learning was applied, outperforming the closest scoring handcrafted method by 19% [29].']],\n",
       "  [['In the LifeCLEF Bird (Audio) Identification Task 2016/2017 algorithm benchmarking competition, the top algorithms were a variation of fully supervised deep learning CNN architecture [35], [36].']],\n",
       "  [['405 used a pretrained networks AlexNet, GoogLeNet, and VGGNet on the LifeCLEF 2015 plant task data set 406 and MalayaKew data set 407 for plant identification.']]],\n",
       " [[['Living labs are available in information retrieval and for many recommender-system domains, particularly news Living labs are available in information retrieval and for many recommender-system domains, particularly news [4][5][6], and they attracted dedicated workshops [7].']]],\n",
       " [[['Aside from extrinsic and intrinsic plagiarism detection, the methods described in this article have numerous other applications such as machine translation Aside from extrinsic and intrinsic plagiarism detection, the methods described in this article have numerous other applications such as machine translation [67], author profiling for marketing applications [211], spam detection [248], law enforcement [127,211], identifying duplicate accounts in internet fora [4], identifying journalistic text reuse [47], patent analysis [1], event recognition based on tweet similarity [24,130], short answer scoring based on paraphrase identification [242], or native language identification [119].'],\n",
       "   ['Aside from extrinsic and intrinsic plagiarism detection, the methods described in this article have numerous other applications such as machine translation Aside from extrinsic and intrinsic plagiarism detection, the methods described in this article have numerous other applications such as machine translation [67], author profiling for marketing applications [211], spam detection [248], law enforcement [127,211], identifying duplicate accounts in internet fora [4], identifying journalistic text reuse [47], patent analysis [1], event recognition based on tweet similarity [24,130], short answer scoring based on paraphrase identification [242], or native language identification [119].']],\n",
       "  [[', grouping documents by authorship) [16,23].'],\n",
       "   ['Bagnall introduced an AA methodBagnall introduced an AA method1  [1] and obtained top positions in shared tasks in authorship verification and authorship clustering [16,23].']]],\n",
       " [[['The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 [1] and 2015 [6] was to investigate how professional metadata (title, authors, .']],\n",
       "  [['The workshop builds on the results of the earlier discussion, and through the CLEF Social Book Search Lab The workshop builds on the results of the earlier discussion, and through the CLEF Social Book Search Lab [10] has already been pushing this line of research with a range of user studies, novel user interfaces, and analysis of large scale social data.']],\n",
       "  [['Figure 1 shows an explanatory excerpt of four of the modalities of the documents in the social book search (SBS) collection used in the SBS lab at the CLEF evaluation forum (Koolen et al. 2016).'],\n",
       "   ['For the experiments using the ratings as an additional modality, we use the social book search (SBS) 2016 lab task For the experiments using the ratings as an additional modality, we use the social book search (SBS) 2016 lab task (Koolen et al. 2016).']],\n",
       "  [['For the construction of this dataset, 40 book records were selected randomly from the Amazon/LibraryThing corpus in English language provided by the Social Book Search Lab For the construction of this dataset, 40 book records were selected randomly from the Amazon/LibraryThing corpus in English language provided by the Social Book Search Lab [16], a track which is part of the CLEF (Conference and Labs of the Evaluation Forum).']],\n",
       "  [['The interactive track of CLEF Social Book Search The interactive track of CLEF Social Book Search [82] 11 was started in 2014.']],\n",
       "  [['Related to this task, the goal of The Social Book Search (SBS) Lab [1], a track belonging to the CLEF (Conference and Labs of the Evaluation Forum) from 2007 to 2016, has been to research and develop techniques to support readers in complex book search tasks, providing a common evaluation dataset.'],\n",
       "   ['We randomly selected 300 reviews, associated to 40 different fiction books from the Amazon/LibraryThing corpus for English language provided by the SBS Lab We randomly selected 300 reviews, associated to 40 different fiction books from the Amazon/LibraryThing corpus for English language provided by the SBS Lab [1].']],\n",
       "  [['CLEF had a series of competitions on book recommendations [21], but this relied on posts, tags, reviews and ratings by many users in the LibraryThing community and the Amazon shop.']],\n",
       "  [[\"However, in the context of book search, where the goal is to rank books given a query [8], the usual application of PRF [4,18] may not be appropriate, considering that the book's descriptions may not adequately convey the experience and emotion attached to the story line that a reader may be looking to enjoy.\"],\n",
       "   ['The document collection used in this study is provided by the Suggestion Track 5 of the Social Book Search Lab (SBS) at CLEF The document collection used in this study is provided by the Suggestion Track 5 of the Social Book Search Lab (SBS) at CLEF [8].']],\n",
       "  [['In the book domain, The CLEF Social Book Search dataset [11] provides 120 natural language information needs.'],\n",
       "   ['The CLEF Social Book Search dataset [11] contains a collection of 120 natural language information needs of books extracted from an online discussion forum.']],\n",
       "  [['The Social Book Search Lab CLEF The Social Book Search Lab CLEF [30] that ran from 2011 to 20163 enabled a number of studies in complex search for the book domain [13,14,17,58,59].']]],\n",
       " [[['They cover a period ranging from May 2015 to November 2016 and are composed by 134 different languages [5,4].']],\n",
       "  [['More specifically, it focuses on location extraction from tweets, which is vital to geo spatial applications as well as applications linked with events (Goeuriot, Mothe, Mulhem, Murtagh, & Sanjuan, 2016).'],\n",
       "   ['We manually analyzed some tweets from the festival tweet collection used in CLEF 2015 We manually analyzed some tweets from the festival tweet collection used in CLEF 2015 (Goeuriot et al., 2016) to detect clues that could be used to predict whether a location occurs in a tweet or not.']],\n",
       "  [['This follows a previous task initiated in [7] about cultural microblog contextualization.']],\n",
       "  [['The CLEF 2016 Cultural Microblog Contextualization (MC2) lab The CLEF 2016 Cultural Microblog Contextualization (MC2) lab (Goeuriot et al. 2016) was the first edition run as independent CLEF lab, and sixth edition as part of the INEX family.']],\n",
       "  [[\"La t√¢che CLEF a √©volu√© r√©cemment pour prendre en compte diff√©rents besoins qui peuvent √™tre utiles aux utilisateurs dans le cadre d'√©v√©nements comme les festivals (Goeuriot et al., 2016;Ermakova et al.\"]],\n",
       "  [[', 2003] ou le PLSA (Probabilistic Latent Semantic Analysis) [Hofmann, 1999], et les m√©thodes pr√©dictives √† base de r√©seaux de neurones.'],\n",
       "   [\"Le cadre probabiliste de l'analyse s√©mantique latente (PLSA) a √©t√© propos√© pour la premi√®re fois, dans [Hofmann and Puzicha, 1998], puis dans [Hofmann, 1999] pour son application √† l'indexation.\"],\n",
       "   ['ionCorpus de test de requ√™ Nous avons test√© nos propositions sur le corpus CLEF CMC 2016 sur la recherche de tweets intitul√©e \"Timeline Illustration\" (sous-t√¢che 3, [Goeuriot et al., 2016]).']],\n",
       "  [['We manually analyzed some tweets from the festival tweet collection used in CLEF 2015 We manually analyzed some tweets from the festival tweet collection used in CLEF 2015 [9] in order to detect clues that could be used to predict whether a location occurs in the tweet or not.']]],\n",
       " [[['To this end, various IR test collections have been created for diversity track such as TREC [7], NT-CIR [16], and CLEF [4].'],\n",
       "   ['NCTIR [16], CLEF [4]) are similar and omitted for brevity sake.']],\n",
       "  [['Indeed, statistical analysis of the results in the multilingual tracks of the 2001 and 2002 campaigns has shown that performance changes need to be rather large to find statistically significant differences between experiments (Braschler 2002(Braschler , 2003)).']],\n",
       "  [['We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign [3][4][5][6] for bilingual ad-hoc retrieval tracks 6 .']],\n",
       "  [['We create our training and evaluation data from the Cross-Language Evaluation Forum (CLEF) 2000-2008 campaign for bilingual ad-hoc retrieval tracks [3][4][5][6][27][28][29][30][31].']]],\n",
       " [[['Most CLIR research has been focused on the cross-lingual matching function although the recent CLEF conferences have initiated some work on the second function: (partial) translation for document selection (Oard & Gonzalo, 2002).']],\n",
       "  [['The interactive CLEF experimental framework The interactive CLEF experimental framework (Gonzalo & Oard, 2002) was followed, but additional measurements (both objective and subjective) were recorded.']],\n",
       "  [[\"El objetivo del iCLEF'2002 El objetivo del iCLEF'2002 (Gonzalo and Oard, 2002) fue proporcionar un marco de referencia com√∫n para realizar experimentos comparando dos sistemas de recuperaci√≥n de informaci√≥n transling√ºe que permitan a un usuario que desconoce el idioma de los documentos realizar una expansi√≥n interactiva de la consulta, una selecci√≥n interactiva de documentos (al igual que el a√±o anterior), o ambas opciones a la vez.\"]],\n",
       "  [['(Oard & Gonzalo, 2002)).']],\n",
       "  [['A special task was designed this year for interactive user experiments A special task was designed this year for interactive user experiments (Oard and Gonzalo 2001).']]],\n",
       " [],\n",
       " [[['The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections [244,245].'],\n",
       "   ['First, we note that the results obtained for the query likelihood model are comparable to or better than the mean results of all the participating groups in the respective TREC Genomics [129][130][131] and CLEF Domain-specific tracks [244,245].']]],\n",
       " [[['We used data from CLEF-2007 and CLEF-2008 [12,18].']],\n",
       "  [['Similarly, in the CLEF 2007 Multilingual Question Answering Track Similarly, in the CLEF 2007 Multilingual Question Answering Track (Giampiccolo et al., 2007), topicrelated questions track consisted of clusters of questions, which were related to the same topic.']],\n",
       "  [[', , 2004;;Vallin et al., 2005;Magnini et al.']]],\n",
       " [[['The test set is constituted from the AVE 2006 The test set is constituted from the AVE 2006 [11] campaign data for French.']],\n",
       "  [['Today, there is also experimental evidence for the claim that question answering can profit from the use of logical subsystems: A comparison of answer validators in the Answer Validation Exercise 2006 found that systems reported to use logic generally outperformed those without logical reasoning [2].']],\n",
       "  [['In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus (Pe√±as et al., 2007).'],\n",
       "   ['(2006); Pe√±as et al. (2007).'],\n",
       "   ['These values are comparable to performance of state-of-the-art RTE systems Pe√±as et al. (2007); Giampiccolo et al.']],\n",
       "  [['This result pushes us to use this system for Answer Validation (Pe√±as et al., 2007).']],\n",
       "  [['We performed the same experiment joining the Answer Validation Exercise4 (AVE) 2006 English data set (Pe√±as et al., 2006) and the Microsoft Research Paraphrase Corpus5 (MSRPC) (Dolan et al.']],\n",
       "  [['The aim of the AVE task was to automatically assess the validity of the answers given by QA systems [9].'],\n",
       "   ['The task of answer validation (AVE) The task of answer validation (AVE) [9] at CLEF was dedicated to validate answers to questions in relation with a justification passage, both provided by QA systems.']],\n",
       "  [['( [5]) give an overview of the first AVE launched during QA@CLEF 2006 campaign.']],\n",
       "  [[', 2008, Rodrigo et al., 2009).']],\n",
       "  [['Many evaluation campaigns and benchmarks are related to textual entailment, as well as paraphrase detection in general, among which PASCAL challenge [13], Answer Validation Exercise [71], the MSRP paraphrase corpus [15] or the SNLI corpus [10].']],\n",
       "  [['Les r√©sultats ont √©t√© √©valu√©s par la pr√©cision, le rappel et la f-mesure qui ont √©t√© calcul√©s de la fa√ßon suivante : pr√©cision = #paires jug√©es OU I correctement #jug√©es comme OU I , rappel = #paires jug√©es comme OU I correctement #paires OU I et f -mesure = 2 * pr√©cision * rappel pr√©cision+rappel 3 Travaux en validation de r√©ponses Les r√©sultats ont √©t√© √©valu√©s par la pr√©cision, le rappel et la f-mesure qui ont √©t√© calcul√©s de la fa√ßon suivante : pr√©cision = #paires jug√©es OU I correctement #jug√©es comme OU I , rappel = #paires jug√©es comme OU I correctement #paires OU I et f -mesure = 2 * pr√©cision * rappel pr√©cision+rappel 3 Travaux en validation de r√©ponses (Pe√±as et al., 2006) pr√©sentent le d√©roulement de la premi√®re campagne AVE.']],\n",
       "  [['Entailment has been also used for zero-shot and few-shot relation extraction [37], answer validation [35], event argument extraction [36], and claim verification in fact checking [16].']]],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['ImageCLEFPhoto used the IAPR TC-12 collection for the past three years [10,11,12], and it was extended to allow diversity measurement, by grouping the relevance judgments of existing topics into clusters that reflect relationships between relevant images in the collection.'],\n",
       "   ['Detail on the processes of topic selection and cluster assessment can be found in [12].']],\n",
       "  [['The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks.']]],\n",
       " [[['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It has also been used for object retrieval [54], and for measuring word association with application to region-level AIA [22].'],\n",
       "   ['It can be used for the evaluation of closely related tasks, for example for visual concept detection and object retrieval [54].'],\n",
       "   ['The same applies for the object retrieval task proposed in ImageCLEF2007 [54], with the difference that now images from the same collection could be used for training and testing [54].'],\n",
       "   ['The same applies for the object retrieval task proposed in ImageCLEF2007 [54], with the difference that now images from the same collection could be used for training and testing [54].']],\n",
       "  [['In the ImageClef 2007 medical image classification competition In the ImageClef 2007 medical image classification competition [56], a database of 12,000 categorized radiograph images is used.']]],\n",
       " [[['As an example of its importance, the ImageCLEF challenge has an entire task dedicated to the annotation and retrieval of medical images [124], and participants achieve impressive results for this task, in particular when combining visual and textual information together.']],\n",
       "  [['Many articles have been published on CBIR for general images [10,39] as well as medical images [21,29,30,32,33].']],\n",
       "  [['Medical image-CLEF task : In 2004, CLEF organizers launched the Medical image-CLEF task (Muller et al., 2008) which focuses on the use of multimodal information (text and image) in the medical domain.'],\n",
       "   ['The used document collection is provided by the Radiological Society of North America (RNSA) and constitutes an important body of medical knowledge issued from peer-reviewed scientific literature (Muller et al., 2008).']],\n",
       "  [['Ela √© um subconjunto da base do ImageCLEFmed 2007 (Henning M√ºller, 2008).']],\n",
       "  [[', 2007), (M√ºller et al., 2008), (M√ºller et al.']]],\n",
       " [[['2004), oft-recorded in noisy environments, mean WER for the 2006 ASR transcripts is reported as 25% (Pecina et al. 2008).']],\n",
       "  [['For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) [22] and mean average segment precision (MASP) [10].']],\n",
       "  [['Since the RSR 2011 task was a known-item search, one useful evaluation metric is the Mean Reciprocal Rank (MRR); additionally we apply a metric that evaluates the ranking and takes account of the distance between the predicted and actual jump-in point (mean Generalized Average Precision (mGAP)) [15].']],\n",
       "  [['Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.']],\n",
       "  [['The focus then shifted towards spoken content that is produced spontaneously such as interviews, lectures and TV shows [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['These videos were provided by the CLEF 2 speech retrieval tracks [3].'],\n",
       "   ['We used the retrieval model PL2 and QE model BO1 described in the previous section (see Equations 1,3) to calculate the results shown in Table III, which shows performance in terms of Mean Average Precision (MAP), Recall and Precision for top 10 documents (P@10).']]],\n",
       " [],\n",
       " [[['The GeoCLEF search task examined geographic search in text corpus [18].']],\n",
       "  [['Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.'],\n",
       "   ['This task was organized by Microsoft Research Asia (Mandl et al., 2007).'],\n",
       "   ['Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.']],\n",
       "  [['To detect cities, it combines a heuristic method inspired by the schema introduced in [34] with a rank-reciprocity algorithm that helps it detect misspellings (e.']],\n",
       "  [['To date, the most important large-scale evaluation is represented by the four GeoCLEF challenges, run from 2005 to 2008 [39,77].']],\n",
       "  [['But spatial evaluation campaigns like GeoCLEF (Mandl et al., 2007) do not give accurate resources (like polygons) and do not handle French documents.']]],\n",
       " [[['The main corpus in our experiments is the one used in the Morpho Challenge 2007 competition The main corpus in our experiments is the one used in the Morpho Challenge 2007 competition [8].']],\n",
       "  [['Three hundred words were randomly selected from the Morpho Challenge corpus (Kurimo, Creutz, & Varjokallio, 2008) including over 2.']]],\n",
       " [[['For example, the inter-annotator agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-Task dataset (Agirre et al., 2007) was approximately 72.']],\n",
       "  [['We performed the experiment following the instructions for SemEval-2007 task 1 We performed the experiment following the instructions for SemEval-2007 task 1 (Agirre et al., 2007).']]],\n",
       " [],\n",
       " [[['The Fact-checking Lab at CLEF [3,9] looks at this problem by defining the task of ranking sentences according to their need to be fact-checked.']],\n",
       "  [['2 Whereas the 2019 edition [9,10] also focused on political debates, isolated claims were considered as well, in conjunction with a closed set of Web documents to retrieve evidence from.'],\n",
       "   ['The 2019 edition featured two tasks The 2019 edition featured two tasks [10]: Task 1 2019 .']],\n",
       "  [['lab on identification and verification of claims (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [['In the last few years, the research community has been looking at automatic check-worthiness predictions In the last few years, the research community has been looking at automatic check-worthiness predictions [15,49], at truthfulness detection/credibility assessments [5,12,24,33,34,39], and at developing fact-checking URL recommender systems and text generation models to mitigate the impact of fake news in social media [51,52,56].'],\n",
       "   ['Related to this, the Fact Checking Lab at CLEF [12,34] addressed the task of ranking sentences according to their need to be fact-checked.'],\n",
       "   ['[34] retrieve evaluations of different articles at factcheck.']],\n",
       "  [['Recent work has focused on the automatic classification of truthfulness or fact checking Recent work has focused on the automatic classification of truthfulness or fact checking [2,9,15,20,22,24].'],\n",
       "   ['CLEF developed a Fact-Checking Lab [9,22] to address the issue of ranking sentences according to some fact-checking property.']],\n",
       "  [['Recent work has focused on the automatic classification of truthfulness or fact checking Recent work has focused on the automatic classification of truthfulness or fact checking [2,9,15,20,22,24].'],\n",
       "   ['CLEF developed a Fact-Checking Lab [9,22] to address the issue of ranking sentences according to some fact-checking property.']],\n",
       "  [['lab (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [['Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [[', [1,14,24]), it is often still necessary to involve humans in the fact-checking process.']],\n",
       "  [['lab at CLEF2019 (Elsayed et al., 2019;Hasanain et al.'],\n",
       "   ['lab (Elsayed et al., 2019;Hasanain et al.']],\n",
       "  [['The 2019 edition covered the various modules necessary to verify a claim: from check-worthiness, to ranking and classification of evidence in the form of Web pages, to actual fact-checking of claims against specific text snippets [24,25].']],\n",
       "  [[', 2018) and fact-checking (Elsayed et al., 2021) prediction is driven towards a particular community.']],\n",
       "  [['In the last few years, the research community has been looking at automatic check-worthiness predictions In the last few years, the research community has been looking at automatic check-worthiness predictions [15,49], at truthfulness detection/credibility assessments [5,12,24,33,34,39], and at developing fact-checking URL recommender systems and text generation models to mitigate the impact of fake news in social media [51,52,56].'],\n",
       "   ['Related to this, the Fact Checking Lab at CLEF [12,34] addressed the task of ranking sentences according to their need to be fact-checked.'],\n",
       "   ['[34] retrieve evaluations of different articles at factcheck.']],\n",
       "  [['lab has focused on developing technology to assist the journalist fact-checker during the main steps of verification [7,8,18,19,[47][48][49]51,52].']],\n",
       "  [['shared task has played a significant role in recent years [5].']]],\n",
       " [[['Indeed, death certificates are standardized documents filled by physicians to report the death of a patient but the content of each document contains heterogeneous and noisy data that participants had to deal with [9].']],\n",
       "  [['Systems developed to face this challenge varied greatly [65,66].']],\n",
       "  [['Each year CLEF eHealth offers IR, information extraction (IE) and information management tasks to volunteer task participants which aim to evaluate systems that support laypeople in searching for and understanding health information [15,27,28,57].']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [['As health information retrieval systems are continuously used to improve the excellence of medical services in hospitals, the size and diversity of information is increasing and becoming compound As health information retrieval systems are continuously used to improve the excellence of medical services in hospitals, the size and diversity of information is increasing and becoming compound [1,4,8].'],\n",
       "   ['Search engines are widely used by information seekers as a means to access health information available on the internet [8,9,10].']],\n",
       "  [['Rece‚ô™tly the CLEF eHealth track o‚ô™ Tech‚ô™ology Assisted Reviews i‚ô™ Empirical ‚Ñßedici‚ô™e [9,20] developed datasets co‚ô™tai‚ô™i‚ô™g 72 topics created from diag‚ô™ostic test accuracy systematic reviews produced by the Cochra‚ô™e Col-laboratio‚ô™.'],\n",
       "   ['[5,9,20].']],\n",
       "  [[\"In 2013-2017, these tasks were patient-centric, with clinicians also considered from 2015, but in 2017 a pilot task on Technology Assisted Reviews (TAR) to support health scientists and policymakers' information access was also introduced (Suominen et al, 2013;Kelly et al, 2014;Goeuriot et al, 2015;Kelly et al, 2016;Goeuriot et al, 2017).\"]],\n",
       "  [['Moreover, it is also critical to provide search solutions for non-English content as well as cross-language or multilingual IR solutions [4,10,16].']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4,5,8,10,16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German.']],\n",
       "  [['In 2019 [6], the task involved the automatic annotation of German non-technical summaries of animal experiments with ICD-10 codes.']],\n",
       "  [['The CLEF (Conference and Labs of the Evaluation Forum) The CLEF (Conference and Labs of the Evaluation Forum) (Kelly et al., 2019) eHealth dataset is a curated collection of non-technical summaries (NTS) of animal experiments from Germany, which was used to organize the Multilingual Information Extraction Task (Task 1) in the CLEF eHealth Challenge 2019 (D√∂rendahl et al.']]],\n",
       " [[['In this respect, the Early Risk Prediction on the Internet (eRisk) [14,15], as well as the Computational Linguistics and Clinical Psychology (CLPsych) [9] workshops were the first to propose benchmarks to bring together many researchers to address the automatic detection of mental disorders in online social media.'],\n",
       "   ['Here, we study various collections released at different editions of the eRisk workshop Here, we study various collections released at different editions of the eRisk workshop [14,15].']],\n",
       "  [['eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection [4][5][6].'],\n",
       "   [\"A full description of this ranking-based evaluation approach can be found in the eRisk 2019's overview report [6].\"],\n",
       "   ['full description and analysis of the results can be found in the lab overviews full description and analysis of the results can be found in the lab overviews [4][5][6] and working note proceedings.']],\n",
       "  [['This model was chosen because it achieved comparable results to the best performing model at the recent eRisk shared task [17,22], and is based on an end-to-end architecture, which makes the reasoning behind its decision more easily explainable.'],\n",
       "   ['The dataset used is from the first sub-task of the eRisk 2019 shared task The dataset used is from the first sub-task of the eRisk 2019 shared task [17], whose focus is the early risk detection of anorexia.'],\n",
       "   ['[17], the dataset was collected following the extraction and annotation method, proposed by Coppersmith et al.'],\n",
       "   ['In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.']],\n",
       "  [[', depression, eating disorders) [30,58] from the social media content of users.']],\n",
       "  [['Sentiment and emotional distress identi ed from those digital content can be used for early detection of individuals who are in need and provide in-time interventions 57 .']]],\n",
       " [[[\"Still, some datasets exist, and existing lifelogging datasets [12] usually contain a person's daily life activities automatically captured and recorded using smartphone applications, wearable devices, and other sensors.\"]],\n",
       "  [['It is also noteworthy to mention that 3D datasets exist which are annotated at the slice level, where it is justified to use 2D CNN approaches [7] but it is not the case when the data is annotated at the volume level [4,17].'],\n",
       "   ['The dataset is provided by ImageCLEF Tuberculosis 2019 The dataset is provided by ImageCLEF Tuberculosis 2019 [4]  [17], intended for the task of severity scoring (SVR).']],\n",
       "  [['A recent alternative is the Visual Question Answering (VQA) concept A recent alternative is the Visual Question Answering (VQA) concept [15], [13], whereby a model is queried regarding the content of an image by means of an explicit question.']]],\n",
       " [[['However, such efforts have proven more challenging in tropical ecosystems [50], where identifications made by expert botanists are more accurate.'],\n",
       "   ['Large online image libraries can be rapidly developed; for example, several hundred thousand images of 10,000 Amazonian plant species [50] have been collected.']]],\n",
       " [[['The handcrafted linguistic features included surface and syntactic features (Daelemans et al. 2019;Rangel & Rosso 2019) for the traditional machine learning models.']]],\n",
       " [[['Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models [7][8] [9].']]],\n",
       " [[[', 2019a, H√ºrriyetoƒülu et al. 2019b).'],\n",
       "   ['The crosscontext generalizability of the automated systems is already a challenging task (H√ºrriyetoƒülu et al. 2019b).']],\n",
       "  [['We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab (H√ºrriyetoglu et al., 2019).']],\n",
       "  [['Specifically, the shared tasks CLEF 2019 Protest News (H√ºrriyetoglu et al., 2019), AESPEN 2020 (H√ºrriyetoglu et al.']],\n",
       "  [['Various models have been developed for text classification in general and also for this particular task (H√ºrriyetoglu et al., 2019).']],\n",
       "  [['The detailed description of the subtasks can be found in The detailed description of the subtasks can be found in H√ºrriyetoglu et al. (2019) and H√ºrriyetoglu et al.']]],\n",
       " [[['While in the early years much CLIR research focused on the development of translation resources and methods specifically focused on the CLIR task, the rapid advances in Statistical Machine Translation (SMT) techniques in recent years means that it has played an increasing role in the improvement of CLIR systems [10], [4], [8].']],\n",
       "  [['In the following we consider four public experimental collections, whose characteristics are reported in Table In the following we consider four public experimental collections, whose characteristics are reported in Table 3: (i) CLEF 2003, Multilingual-4, Ad-Hoc Track [1]; (ii) TREC 13, 2004, Robust Track [15]; (iii) CLEF 2009, bilingual X2EN, The European Library (TEL) Track [7]; and, (iv) TREC 21, 2012, Web Track [6].']],\n",
       "  [['We evaluate the final retrieval models on HC4 We evaluate the final retrieval models on HC4 [26], a newly constructed evaluation collection for CLIR, for Chinese and Persian, NTCIR [31] for Chinese, CLEF 08-09 for Persian [1,14], and CLEF 03 [4] for French and German.'],\n",
       "   ['While this effect deserves further investigation, we note that queries for this collection were originally created in Persian and then translated into English, possibly initially by nonnative speakers [1,14].']]],\n",
       " [],\n",
       " [],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['Cabral, 2009;D. Santos and L. Cabral, 2010), GeoTime in 2010 and 2011 (F.']],\n",
       "  [['The Giki corpus includes 97 questions from the GikiP 2008 The Giki corpus includes 97 questions from the GikiP 2008 (Santos and Cardoso, 2008) and GikiCLEF 2009 (Santos and Cabral, 2009) tasks for answering questions requiring spatial reasoning with information from Wikipedia.']]],\n",
       " [],\n",
       " [[['The data comes from the CLEF-IP 2010 task [ 5], where 134 French patent topics are used to search a collection of 1.'],\n",
       "   ['Queries were constructed from the translated patent topics based on the best runs submitted to the CLEF-IP 2010 Queries were constructed from the translated patent topics based on the best runs submitted to the CLEF-IP 2010 [ 5], where most of sections in the patent topics were used to formulate the query as described in [ 3].']],\n",
       "  [['Relevant prior-art patents have common technical aspects with a patent application, and include patents that can invalidate the novelty of the invention and patents that describe the state-of-the-art in the field of the invention on which the patent application is building [ 4,5].'],\n",
       "   ['Patent prior-art search task was addressed in the CLEF-IP task in both 2009 Patent prior-art search task was addressed in the CLEF-IP task in both 2009 [ 5] and 2010 [ 4].']],\n",
       "  [['In prior-art Task defined in Clef-IP 2010, participants were asked to find the prior-art for a given patent application [16].']],\n",
       "  [['For our experiments, we used used the Lucene IR System For our experiments, we used used the Lucene IR System 6 to index the English subset of CLEF-IP 2010 and CLEF-IP 2011 datasets7  [19,21] with the default settings for stemming and stop-word removal.']],\n",
       "  [['For patent search in compounding languages, the CLIR effectiveness is usually lower than for other language pairs [3,7].'],\n",
       "   ['The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments [7].'],\n",
       "   ['Translated patent topics were processed to form queries by adding terms occurring more than twice in the title, abstract, description, and claims sections combined and all bigrams that occur more than three times, using the term frequency as a weight for these terms Translated patent topics were processed to form queries by adding terms occurring more than twice in the title, abstract, description, and claims sections combined and all bigrams that occur more than three times, using the term frequency as a weight for these terms [7].']],\n",
       "  [['Different participating teams experimented with term distribution analysis in a language modeling setting employing the document structure of the patent documents in various ways (Piroi and Tait 2010).']],\n",
       "  [['Although a query is very long in patent prior art search, a significant term mismatch between queries and relevant documents has been reported earlier Although a query is very long in patent prior art search, a significant term mismatch between queries and relevant documents has been reported earlier [Roda et al., 2009;Magdy et al.'],\n",
       "   ['Therefore an important topic in patent retrieval is Cross-Language Information Retrieval (CLIR), where the topic is a patent application in one language and the objective is to find relevant prior-art patents in another language [Roda et al., 2009;Joho et al.'],\n",
       "   ['A significant term mismatch between the patent query and relevant patents has been mentioned the main cause for low effective patent prior art search in previous studies [Roda et al., 2009;Magdy, 2012].']],\n",
       "  [['In later years, different workshops such as the Japanese NTCIR [27] and more recently the CLEF-IP evaluation track [28] added patent categorization tasks.']],\n",
       "  [['Most of the approaches to this task use standard information retrieval (IR) processes [7], [8].'],\n",
       "   ['26 [7].']],\n",
       "  [['The recent advancement in patent search is driven by the \"Intellectual Property\" task initialized by CLEF [PT10].']],\n",
       "  [['A passage retrieval task was revisited in CLEF-IP in 2012 (Piroi et al. 2012) and 2013 (Piroi et al.'],\n",
       "   ['2011), flowchart/structure recognition (Piroi et al. 2012(Piroi et al.'],\n",
       "   [', 2013)), and chemical structure recognition (Piroi et al. 2012).']],\n",
       "  [['Research labs have the opportunity to test their methods on multiple shared tasks such as PR, patent classification, image-based PR, image classification, flowchart recognition, and structure recognition (Roda et al (2010); Piroi et al (2010Piroi et al ( , 2011Piroi et al ( , 2012Piroi et al ( , 2013))).'],\n",
       "   ['CLEF-IP 2012 Collection: this dataset was created as a test collection for three tasks: passage retrieval starting from claims, chemical structure recognition, and flowchart recognition CLEF-IP 2012 Collection: this dataset was created as a test collection for three tasks: passage retrieval starting from claims, chemical structure recognition, and flowchart recognition Piroi et al (2012).']],\n",
       "  [['The CLEF-IP tracks included a patent classification task [46], [47], which provided a dataset of more than 1 million patents as the training set to classify 3,000 patents into their IPC subclasses.']],\n",
       "  [[\"Furthermore we see recent advances in neural retrieval remain neglected for document-to-document retrieval despite the task's importance in several, mainly professional, domains [24,28,29,30].\"]],\n",
       "  [[', by the CLEF-IP labs [25,26,27,28,29].']],\n",
       "  [['Although multilingual patent datasets, such as MAREC/IREC Although multilingual patent datasets, such as MAREC/IREC (Piroi, 2021) and (Roda et al., 2009;Piroi, 2010;Piroi et al.']]],\n",
       " [[['A first attempt to release a collection of log data with the aim of verifiability and repeatability was done within the Cross-Language Evaluation Forum (CLEF) in 2009 in a track named LogCLEF which is an evaluation initiative for the analysis of queries and other user activities A first attempt to release a collection of log data with the aim of verifiability and repeatability was done within the Cross-Language Evaluation Forum (CLEF) in 2009 in a track named LogCLEF which is an evaluation initiative for the analysis of queries and other user activities [3].']]],\n",
       " [[['Similarly, Ferro and Harman [14] describes the GridCLEF track at CLEF, an initiative to investigate the effects of various system components for multilingual information access systems with respect to language (e.'],\n",
       "   ['This may require more evaluation exercises, such as GridCLEF [14], to determine relationships and effects between (sub-) components.']],\n",
       "  [['Component-based evaluation methodologies Ferro and Harman [22], Hanbury and M√ºller [30] mixed different components in order to avoid to build an IR system from scratch; but, even though these approaches allowed researchers to focus on the components of their own interest, they have not delivered estimates of the performance figures of each component.'],\n",
       "   ['The idea of creating all the possible combinations of components has been proposed by Ferro and Harman The idea of creating all the possible combinations of components has been proposed by Ferro and Harman [22], who noted that a systematic series of experiments on standard collections would have created a GoP, where (ideally) all the combinations of retrieval methods and components are represented, allowing us to gain more insights about the effectiveness of the different components and their interaction; this would have called also for the identification of suitable baselines with respect to which all the comparisons have to be made.']],\n",
       "  [['GLMM and ANOVA were applied in [14] and [15] to the composition of the run components and their effect, using a Grid of Points (GoP) setting, borrowed from [11], to study all the possible run configurations.']]],\n",
       " [[['Although unsupervised learning of morphological segmenters does not reach the detail and accuracy of hand-built analyzers, it has proven useful for many NLP applications, including speech recognition Although unsupervised learning of morphological segmenters does not reach the detail and accuracy of hand-built analyzers, it has proven useful for many NLP applications, including speech recognition [3], information retrieval [4], and machine translation [5].']],\n",
       "  [['4806 for German [24].']],\n",
       "  [['The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP [34], which has become a standard for unsupervised morphological segmentation [46].']],\n",
       "  [['4806 for German [33].']],\n",
       "  [[', , 2008(Kurimo et al., , 2009) ) or \"Multilingual parsing\" (Zeman et al.']]],\n",
       " [[['For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].'],\n",
       "   ['As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.'],\n",
       "   ['The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].']],\n",
       "  [['It was then used for producing reports and overview graphs about the submitted experiments [1,3].']],\n",
       "  [['To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are [12,13].'],\n",
       "   ['This is the case, for example, of the CLEF 2005 multilingual merging track [12], which provided participants with some of the CLEF 2003 multilingual experiments as list of results to be used as input to their merging algorithms.']],\n",
       "  [['Please refer to (Agirre et al., 2008) for a more detailed discussion about this data.']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['After ten year of CLEF, the best bilingual systems went up to about 85%-95% of the best monolingual ones (Agirre et al., 2009;Ferro and Silvello, 2014a) for most language pairs.']],\n",
       "  [['We evaluate the final retrieval models on HC4 We evaluate the final retrieval models on HC4 [26], a newly constructed evaluation collection for CLIR, for Chinese and Persian, NTCIR [31] for Chinese, CLEF 08-09 for Persian [1,14], and CLEF 03 [4] for French and German.'],\n",
       "   ['While this effect deserves further investigation, we note that queries for this collection were originally created in Persian and then translated into English, possibly initially by nonnative speakers [1,14].']]],\n",
       " [[['The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections The CLEF domain-specific track evaluates retrieval on structured scientific documents, using bibliographic databases from the social sciences domain as document collections [244,245].'],\n",
       "   ['First, we note that the results obtained for the query likelihood model are comparable to or better than the mean results of all the participating groups in the respective TREC Genomics [129][130][131] and CLEF Domain-specific tracks [244,245].']]],\n",
       " [],\n",
       " [[[', , 2004;;Vallin et al., 2005;Magnini et al.']],\n",
       "  [['In the QA and information retrieval domains progress has been assessed via evaluation campaigns (Voorhees and Harman, 2005;Forner et al., 2008;Ayache et al.']],\n",
       "  [['Finally, the question answering field has been developed significantly [2].']]],\n",
       " [[['The aim of the AVE task was to automatically assess the validity of the answers given by QA systems [9].'],\n",
       "   ['The task of answer validation (AVE) The task of answer validation (AVE) [9] at CLEF was dedicated to validate answers to questions in relation with a justification passage, both provided by QA systems.']]],\n",
       " [[['CLEF holds annual competitions on various QA tasks, including an spoken document task [1,2,3] named QAst (Question Answering on Speech Transcripts).']]],\n",
       " [[['To this end, the goal was to provide a collection of more than 150,000 images; such a collection would be, for instance, much larger than the IAPR TC-12 image collection (Grubinger et al, 2006) that consists of 20,000 photographs and that was, at the time, employed in the ImageCLEF 2008 photo retrieval task (Arni et al, 2009).']],\n",
       "  [['(2008), which was the winning entry of the 2008 Im-ageClef Retrieval challenge (Arni et al., 2008).'],\n",
       "   [', 2008) 35  , 2008), the winners of the challenge (Arni et al., 2008).']],\n",
       "  [[\"La deuxi√®me appel√©e ImageCLEFphoto (Arni et al., 2008) est une t√¢che de recherche d'images bas√©e sur les informations textuelles et visuelles, et propose d'√©tudier les probl√®mes soulev√©s par la diversit√©.\"]],\n",
       "  [['ImageCLEFPhoto used the IAPR TC-12 collection for the past three years [10,11,12], and it was extended to allow diversity measurement, by grouping the relevance judgments of existing topics into clusters that reflect relationships between relevant images in the collection.'],\n",
       "   ['Detail on the processes of topic selection and cluster assessment can be found in [12].']],\n",
       "  [['ImageClef Retrieval evaluations [41,4,2].'],\n",
       "   ['However, methods that combine different modalities can improve retrieval performance on multimodal databases [41,4,2].'],\n",
       "   ['With our re-implementation of [1], using the same features and their weighting w = [1, 2] ‚ä§ and k = 2, we obtain results slightly below the ones reported by ImageCLEF in [4].']],\n",
       "  [['We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval [6], 2008 Geographic IR [15], 2008 Image Retrieval [2], and 2008 Robust IR [1].']],\n",
       "  [['We used six different multimedia corpora, five of which came from TRECVID We used six different multimedia corpora, five of which came from TRECVID [17] and one from ImageCLEF [4].']],\n",
       "  [['This work is a follow up to the study by Villa and Halvey [15], which looked at the effort involved in judging the relevance of text documents.'],\n",
       "   ['The latter three variables were based on the topic classifications defined in the ImageCLEF 2007 [4,5] and allow us to investigate how the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.']],\n",
       "  [['the visual content of images as well as semantic information are available), standard information retrieval techniques have reported very good results for the task of image retrieval [15,16,17].'],\n",
       "   ['At the moment, the image collection has been used for evaluating cross-language TBIR methods, CBIR methods, and methods that combine information from both text and images [16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].']],\n",
       "  [['Thirdly, a search engine should maximize results precision and cover different aspects of the query in the same time [4] but these two measures are often difficult to maximize simultaneously [1].'],\n",
       "   ['They implement a dynamic programming algorithm applied on top of a greedy selection and test their approach on a heterogeneous test database (ImageCLEF 2008 photo retrieval task [1]).']],\n",
       "  [['The IAPR TC-12 photographic collection consists of 60 topics and 20,000 still natural images taken from locations all around the world and including an assorted cross-section of still natural images [11].']],\n",
       "  [['In these challenges, our image and trans-media similarity based retrieval systems demonstrated very good performance (see [14]).']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks.']],\n",
       "  [['A base de imagens utilizada foi a da ImageCLEF Photographic Retrieval Task [Arni et al. 2009], composta por 20.'],\n",
       "   ['2014], para isso avaliou-se os rankings nas primeiras 20 posic ¬∏√µes dentre as 1000 retornadas como proposto pelo desafio em [Arni et al. 2009] al√©m do ranking total.']],\n",
       "  [['Nesse trabalho foi utilizada a ImageCLEF Photographic Retrieval Task collection Nesse trabalho foi utilizada a ImageCLEF Photographic Retrieval Task collection [Arni et al. 2009].']],\n",
       "  [[\"La deuxi√®me appel√©e ImageCLEFphoto (Arni et al., 2008) est une t√¢che de recherche d'images bas√©e sur les informations textuelles et visuelles, et propose d'√©tudier les probl√®mes soulev√©s par la diversit√©.\"]]],\n",
       " [[['More interestingly, the ImageCLEF medical image retrieval task targets modality classification of medical images and have attracted wide attention from the area in recent years [27].']],\n",
       "  [['In addition to images, this collection contains image captions, URLs to the full-text scientific publications from which the images were extracted, their PubMed identifiers from the GoldMiner collection7 , 30 information need requests expressed as image retrieval questions (topics containing text and images), and judgments about the relevance of images retrieved by teams participating in the evaluation [12,13].'],\n",
       "   ['12, which is within the range of visual retrieval results reported for the ImageCLEFmed 2008 medical image retrieval task, and is consistent with the observation that visual retrieval techniques can degrade the overall performance [12].'],\n",
       "   ['The difference in classification precision cannot be explained by the nature of the questions, as the better and worse performing questions were distributed evenly over question categories, complexity levels, and difficulty for retrieval measured by the average Mean Average Precision obtained for these topics in the ImageCLEFmed 2008 evaluation The difference in classification precision cannot be explained by the nature of the questions, as the better and worse performing questions were distributed evenly over question categories, complexity levels, and difficulty for retrieval measured by the average Mean Average Precision obtained for these topics in the ImageCLEFmed 2008 evaluation [12].']],\n",
       "  [['Particularly, the medical image retrieval task of ImageCLEF, presented in (M√ºller et al., 2008), proposes a publicly-available benchmark for the evaluation of several multimodal retrieval systems.']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['In this way, the research study likewise considers comparative label circulations to lift transfer learning, where the comparability of two name disseminations is processed utilizing Jensen Shannon divergence (JSD) [5,18,35,36].'],\n",
       "   [\"There is a trade-off between ''too much capacity'' and ''not enough capacity'' There is a trade-off between ''too much capacity'' and ''not enough capacity'' [36].\"]],\n",
       "  [['Figure 2 shows the scores of our Ambiguous Proximity Distribution Kernel (Ambiguous PDK), along with visual retrieval algorithms submitted by additional groups [62].']]],\n",
       " [[['The obtained leave-one-out error depends on Œ≤ , so for each known model it is possible to find the best Œ≤ producing the lowest criterion error ERR (5).'],\n",
       "   ['Let us identify with Adapt-W the adaptation method described in the previous Section with ERR (5) substituted by WERR (9).'],\n",
       "   ['The experiments were run on two subsets of two different object category databases: the Caltech-256 [9] and the IRMA database used in the CLEF challenge 2008 [5].'],\n",
       "   ['We decided to work on the 2008 IRMA database version [5], just considering the third axis of the code: it describes the anatomy, namely which part of the body is depicted, independently to the used acquisition technique or direction.']],\n",
       "  [['In this section we show empirically the effectiveness of our transfer algorithm 1 on three datasets: Caltech-256 In this section we show empirically the effectiveness of our transfer algorithm 1 on three datasets: Caltech-256 [37], Animals with Attributes (AwA) [24] and IRMA [38].'],\n",
       "   ['We decided to work on the 2008 IRMA database version [38], just considering the third axis of the code: it describes the anatomy, namely which part of the body is depicted, independently to the used acquisition technique or direction.']],\n",
       "  [['In particular, in the last editions of the benchmark (2008 and 2009), a special scoring scheme was defined for comparing performances of the different methods, such as to really exploit this labeling hierarchy, by penalizing wrong classification in the highest hierarchy positions over the lowest ones, as well as penalizing the false category associations over the assignment of unknown codes [DD09].'],\n",
       "   ['SVM classification is the most widely used state-of-theart learning tool for medical image classification, as it generally provides the best performances [DD09].'],\n",
       "   ['Independently on the kind of image descriptors used, most state-ofthe-art approaches to medical image classification rely on discriminative learning tools like support vector machines (SVM) Independently on the kind of image descriptors used, most state-ofthe-art approaches to medical image classification rely on discriminative learning tools like support vector machines (SVM) [DD09].']]],\n",
       " [[['La premi√®re t√¢che appel√©e Visual Concept Detection Task (VCDT) (Deselaers et al., 2008) est une t√¢che de d√©tection de concepts visuels.']],\n",
       "  [['In this section, we compare our MMSVMs with a single SVM on the ImageCLEF2009 Photo Annotation dataset In this section, we compare our MMSVMs with a single SVM on the ImageCLEF2009 Photo Annotation dataset [13].']],\n",
       "  [['La premi√®re t√¢che appel√©e Visual Concept Detection Task (VCDT) (Deselaers et al., 2008) est une t√¢che de d√©tection de concepts visuels.']]],\n",
       " [[['The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach [1].']],\n",
       "  [['This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns This chapter presents an overview of the Wikipedia image retrieval task in the Im-ageCLEF 2008 and 2009 evaluation campaigns (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['The titles of these topics can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).'],\n",
       "   ['More detailed per topic analyses can be found in the overview papers of the task (Tsikrika andKludas, 2009, 2010).']],\n",
       "  [['To address the cross media retrieval problem, advances have been reported over the last decades [7,26,28].']],\n",
       "  [[\"Afin d'√©valuer notre m√©thode d'apprentissage des param√®tres du mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF Afin d'√©valuer notre m√©thode d'apprentissage des param√®tres du mod√®le de repr√©sentation de documents multim√©dia, nous avons effectu√© plusieurs exp√©rimentations sur la collection ImageCLEF (Tsikrika et Kludas (2008, 2009)).\"],\n",
       "   ['Elle est notamment utilis√©e pour classer les participants de la comp√©tition ImageCLEF (Tsikrika et Kludas (2008, 2009)).']]],\n",
       " [],\n",
       " [[['We decided to test the BoC representation in the Geographic Information Retrieval (GIR) environment using the CLEF collection for evaluating the Geo-CLEF task [10,11].']],\n",
       "  [['It operated from 2005 to 2008 [11,12,21,22].']],\n",
       "  [['Recent development on GIR systems Recent development on GIR systems [6] evidence that: i) traditional IR systems are able to retrieve the majority of the relevant documents for most queries, but that, ii) they have severe difficulties to generate a pertinent ranking of them.'],\n",
       "   ['GIR has been evaluated at the CLEF forum GIR has been evaluated at the CLEF forum [3] since year 2005, under the name of the GeoCLEF task [6].']],\n",
       "  [['To date, most GIR systems focus on individual map features, with particular emphasis on text-based retrieval [4].']],\n",
       "  [['We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval We used four data sets corresponding to the following CLEF tracks: 2005 Ad-hoc English retrieval [6], 2008 Geographic IR [15], 2008 Image Retrieval [2], and 2008 Robust IR [1].']],\n",
       "  [['Another witness of this trend are geographic information retrieval systems [5,12,17,19] and in particular local search services, such as Google Maps1 , Yahoo!']],\n",
       "  [['We used three data sets from the CLEF-2008: one for evaluating Geographic IR We used three data sets from the CLEF-2008: one for evaluating Geographic IR [14], other for evaluating Image Retrieval [2], and another for evaluating Robust IR [1].']],\n",
       "  [['For example, the GeoCLEF 2007 corpus consisted of three sub-corpora in English, German, and Portuguese, each with around 200,000 documents [41].'],\n",
       "   ['28 in GeoCLEF 2007 [41] and from 0.'],\n",
       "   [', [21,41,53]).']],\n",
       "  [['The GeoCLEF search task examined geographic search in text corpus [18].']],\n",
       "  [[', 2006), -GeoClef : Geographic Cross Language Evaluation Forum (Mandl et al., 2009), -ACE : Automatic Content Extraction program (Strassel et al.']],\n",
       "  [[', 2006), -GeoClef : Geographic Cross Language Evaluation Forum (Mandl et al., 2009), -ACE : Automatic Content Extraction program (Strassel et al.']],\n",
       "  [['2015), locations in geographical IR (Mandl et al. 2009) or timestamps in time-aware IR (Li and Croft 2003).'],\n",
       "   ['2005;Hashemi and Kamps 2014;Macdonald et al. 2015).'],\n",
       "   ['A lot of other retrieval research sub-fields such as geographical IR (Mandl et al. 2009), image retrieval (Villegas et al.'],\n",
       "   ['Let us explore multimodal document collections such as used in GeoCLEF Let us explore multimodal document collections such as used in GeoCLEF (Mandl et al. 2009) or in the social book search lab (Bogers et al.'],\n",
       "   ['For the experiments with the geographical modality, we use the topics and collection of the GeoCLEF 2008 For the experiments with the geographical modality, we use the topics and collection of the GeoCLEF 2008 (Mandl et al. 2009) monolingual English search task.']],\n",
       "  [['Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.'],\n",
       "   ['This task was organized by Microsoft Research Asia (Mandl et al., 2007).'],\n",
       "   ['Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.']],\n",
       "  [[', Geo-CLEF [4]) and they often lack in rich annotations as would be required for the examples above.']],\n",
       "  [['This includes seminal GIR research based on heuristics [5,28,32], combining BM25 ranking together with geo-spatial criteria derived from gazetteers or general knowledge bases, and also more recent GIR/GeoQA methods based on machine learning and, more recently, neural networks.'],\n",
       "   ['As for results, the authors performed tests on data from a previous CLEF competition [28] and noted that using both textual and geographical features yielded the best results in terms of MAP.']],\n",
       "  [['Second, our geoparsing methods to extract place mentions and geocode them to their correct location on Earth would need to be adapted (Mandl et al. 2009).']],\n",
       "  [['But spatial evaluation campaigns like GeoCLEF (Mandl et al., 2007) do not give accurate resources (like polygons) and do not handle French documents.']]],\n",
       " [[['Evaluation of video retrieval is also very active and standardized, with important contributions from TRECVID,5 videoCLEF [81,82], and MultimediaEval.']],\n",
       "  [['Sound and Vision makes it possible for the wider multimedia community to work on tasks related to the important issue of keyword recommendation through its support of the VideoCLEF video analysis and retrieval benchmark evaluation Sound and Vision makes it possible for the wider multimedia community to work on tasks related to the important issue of keyword recommendation through its support of the VideoCLEF video analysis and retrieval benchmark evaluation [17].']]],\n",
       " [[[\"The TREC Filtering Track [31] and CLEF INFILE [6] focused on the multiple filtering tasks including adaptive filtering, where systems aim to select relevant documents from a stream of incoming documents based on a user's profile.\"]]],\n",
       " [[[', 2007(Kurimo et al., , 2008(Kurimo et al.']]],\n",
       " [[['the visual content of images as well as semantic information are available), standard information retrieval techniques have reported very good results for the task of image retrieval [15,16,17].'],\n",
       "   ['At the moment, the image collection has been used for evaluating cross-language TBIR methods, CBIR methods, and methods that combine information from both text and images [16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].'],\n",
       "   ['Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].']],\n",
       "  [['The cross-language image retrieval campaign Image-CLEF The cross-language image retrieval campaign Image-CLEF [5,4] aims to evaluate different approaches of text and content based retrieval methods.']],\n",
       "  [['The International Association of Pattern Recognition (IAPR) TC-12 benchmark dataset The International Association of Pattern Recognition (IAPR) TC-12 benchmark dataset (Grubinger et al, 2006) was created for the cross-language image retrieval track of the CLEF evaluation campaign (ImageCLEF 2006) (Clough et al, 2006).']]],\n",
       " [[['However, these tasks used highly subsampled 2D version of medical images (32 √ó 32 pixels) [15].']],\n",
       "  [['nine of the ten best models in ImageCLEFmed 2006 competition were based on SVM [10].']],\n",
       "  [['According to According to M√ºller et al., 2007 the majority of images in the ImageCLEFMed collection contain annotations and tags which relate to age, gender, background information, magnification, technical information and date of production and so it might be possible to create annotation tasks that address at least some of these criteria.']],\n",
       "  [['Many articles have been published on CBIR for general images [10,39] as well as medical images [21,29,30,32,33].']],\n",
       "  [['For further information on the annotation task of ImageCLEF 2007 we refer the reader to M√ºller et al. (2007).']],\n",
       "  [['The database used in this study is ImageCLEF 2007 [3] which was provided by the IRMA group from RWTH University Hospital of Aachen, Germany.']],\n",
       "  [[', 2007), (M√ºller et al., 2008), (M√ºller et al.']],\n",
       "  [['In the past ten years research projects such as Assert [7], IRMA 1 (Image Retrieval in Medical Applications [8]), and MedGIFT2 (Medical GNU Image Finding Tool, [9]) have advanced the field through a fairly large number of publications and explorations of several sub-domains of medical image re-trieval such as varying feature spaces, interaction with the user [10], evaluation of real user needs [11], and evaluations of image retrieval systems [12].'],\n",
       "   ['A gold standard defining expected responses for a retrieval system, and a comparison of the performance of participating systems [12,36] are also distributed.'],\n",
       "   ['In image retrieval such community building has started around the ImageCLEFmed [12,36] medical image retrieval benchmark.']]],\n",
       " [[[', i2b2 [5,7,43,44] and the ShARe/CLEF eHealth Evaluation Labs [45][46][47].']],\n",
       "  [['This task has been developed within the CLEF 2016 eHealth Evaluation Lab, which aims to foster the development of approaches to support patients, their next-of-kin, and clinical sta‚Üµ in understanding, accessing and authoring health information [15].']],\n",
       "  [['Systems developed to face this challenge varied greatly [65,66].']],\n",
       "  [['Each year CLEF eHealth offers IR, information extraction (IE) and information management tasks to volunteer task participants which aim to evaluate systems that support laypeople in searching for and understanding health information [15,27,28,57].']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [['The organizers shared a dataset of 2,309 unannotated drug 4 Although CLEF eHealth is organized every year [13,14], its main focus is multi-linguality and information retrieval rather than clinical NLP Table 1 Clinical NLP Challenges, the tasks they posed, and the number of participating teams, since 2015, ordered by data sensitivity.']],\n",
       "  [['Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks [16,41,42].']],\n",
       "  [['As health information retrieval systems are continuously used to improve the excellence of medical services in hospitals, the size and diversity of information is increasing and becoming compound As health information retrieval systems are continuously used to improve the excellence of medical services in hospitals, the size and diversity of information is increasing and becoming compound [1,4,8].'],\n",
       "   ['Search engines are widely used by information seekers as a means to access health information available on the internet [8,9,10].']],\n",
       "  [[\"In 2013-2017, these tasks were patient-centric, with clinicians also considered from 2015, but in 2017 a pilot task on Technology Assisted Reviews (TAR) to support health scientists and policymakers' information access was also introduced (Suominen et al, 2013;Kelly et al, 2014;Goeuriot et al, 2015;Kelly et al, 2016;Goeuriot et al, 2017).\"]],\n",
       "  [['Moreover, it is also critical to provide search solutions for non-English content as well as cross-language or multilingual IR solutions [4,10,16].']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4,5,8,10,16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German.']]],\n",
       " [[['Today, we have to deal with increasingly complex document collections and queries Today, we have to deal with increasingly complex document collections and queries (Imhof and Braschler 2015) that no longer just consist of textual modalities but also of a large set of non-textual modalities such as visual words in image retrieval (Villegas et al. 2015), locations in geographical IR (Mandl et al.'],\n",
       "   ['2009), image retrieval (Villegas et al. 2015), XML retrieval (Kamps et al.']],\n",
       "  [['While many different types of medical images are collected While many different types of medical images are collected [4,5], the source of the images are not always identified appropriately [6,7].'],\n",
       "   ['We used the medical Subfigure Classification dataset used in the ImageCLEF 2016 competition We used the medical Subfigure Classification dataset used in the ImageCLEF 2016 competition [7].']]],\n",
       " [[['There have been recent international competitions which have examined this topic, for instance the recent ImageCLEF challenge for plant classification (including leaf images) [11].']],\n",
       "  [['The LifeCLEF Bird task proposes to evaluate one of these challenges The LifeCLEF Bird task proposes to evaluate one of these challenges [12] based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios.']],\n",
       "  [['This paper presents the participation of Inria ZENITH team to the 2015-edition of this challenge [9,19].']],\n",
       "  [['The LifeCLEF [11] lab proposes to evaluate this challenge in the continuity of the image-based plant identification task that was run within Im-ageCLEF the past years but with a broader scope (i.']],\n",
       "  [['In the 2015 SeaClef contest In the 2015 SeaClef contest [23], the best results have shown that Deep Learning can achieve a better classification for fish detection than SVM or other classical methods.']],\n",
       "  [['[16,42,22] ).']],\n",
       "  [['This was followed by other applications, such as Pl@ntNet (iOS, Android, and web) and Folia (iOS) dedicated to the European flora [70].']],\n",
       "  [['We used the dataset released for the LIFE-CLEF2015 [4] contest.']],\n",
       "  [['[13][14][15][16][17][18][19]).']],\n",
       "  [['Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014.'],\n",
       "   ['This was also the case of the PlantCLEF challenges [37,38,43], where the deep learning submissions [41,42,65,66] outperformed combinations of hand-crafted methods significantly.']],\n",
       "  [['The first test set (called Plant10) contains 485 images, which were selected from the training data of PlantCLEF [28][20] [49] competition published by the LifeCLEF campaign under ImageCLEF.']],\n",
       "  [['In addition, we demonstrate that the ensemble model combining the enhanced HGO-CNN and Plant-StructNet architectures outperforms the state-of-the-art (SOTA) on the Plant-Clef2015 [16] dataset (Sec.'],\n",
       "   ['Two evaluation metrics are employed: the image-centered and the observation score Two evaluation metrics are employed: the image-centered and the observation score [16].'],\n",
       "   ['This is similar to the results reported in [16].'],\n",
       "   ['t = Œª T t Œ¥ t t = Œª T t Œ¥ t (16) In this case, the attention term Œª t controls the contribution of each convolutional feature at the t-th state.']],\n",
       "  [['Over the last decade, there have been increasingly impressive performance gains achieved by the paradigm shift to deep learning, including bioacoustics [29].'],\n",
       "   ['69 the following year when deep learning was applied, outperforming the closest scoring handcrafted method by 19% [29].']],\n",
       "  [['Some of these were selected from the Xeno-canto1 database [24,33,34], others were taken from the Birds of Argentina & Uruguay: A Field Guide Total Edition corpus [12,27,30], and finally, several recordings were taken from The Internet Bird Collection2  [1].']],\n",
       "  [['The LifeCLEF2015 Fish classification challenge dataset LCF-15 The LifeCLEF2015 Fish classification challenge dataset LCF-15 [24], [25] contained 20,000 labeled images, 93 videos and 15 fish species.'],\n",
       "   ['73% on the LCF-15 [24], [25] dataset.'],\n",
       "   ['For the weak positive supervision, two specialized fishdomain datasets were utilized: the LCF-15 classification challenge dataset For the weak positive supervision, two specialized fishdomain datasets were utilized: the LCF-15 classification challenge dataset [24], [25] with 22.'],\n",
       "   ['Clearly, the additional positive weak supervision could not improve the false-negative rate (F N ) significantly, where the external fish images (in LCF-15 Clearly, the additional positive weak supervision could not improve the false-negative rate (F N ) significantly, where the external fish images (in LCF-15 [24], [25] and QUT2014 [43], [44]) were very different from the project-domain fish images.']],\n",
       "  [['405 used a pretrained networks AlexNet, GoogLeNet, and VGGNet on the LifeCLEF 2015 plant task data set 406 and MalayaKew data set 407 for plant identification.']],\n",
       "  [['Initiatives such as the FishCLEF 15 challenge have expanded the body of literature on computer vision techniques by challenging participants to create a high performing model for fish identification, importantly in unconstraint underwater habitats which is critical for applying this technology to real-world scenarios (Joly et al., 2015).']],\n",
       "  [['A hierarchical classification level was proposed in A hierarchical classification level was proposed in [15] that uses the Balance Guaranteed Optimized Tree (BGOT) algorithm for fish classification.'],\n",
       "   ['A hierarchical classification level was proposed in A hierarchical classification level was proposed in [15] that uses the Balance Guaranteed Optimized Tree (BGOT) algorithm for fish classification.']],\n",
       "  [['High-quality photodocumentation presents an additional benefit in the form of providing taxonomic machine learning algorithms with feature-rich source imagery High-quality photodocumentation presents an additional benefit in the form of providing taxonomic machine learning algorithms with feature-rich source imagery (Joly et al. 2014;W√§ldchen and M√§der 2018), whether used in tandem with other parameters or in isolation.']]],\n",
       " [[[\"Du point de vue de l'√©valuation de ces mod√®les, une premi√®re initiative dans le cadre de la campagne ¬´ Living Labs for IR ¬ª (LL4IR) Du point de vue de l'√©valuation de ces mod√®les, une premi√®re initiative dans le cadre de la campagne ¬´ Living Labs for IR ¬ª (LL4IR) (Schuth et al., 2015) √† CLEF 2015 (Mothe et al.\"],\n",
       "   ['La campagne d\\'√©valuation \"Living Labs for Information Retrieval\" (LL4IR) La campagne d\\'√©valuation \"Living Labs for Information Retrieval\" (LL4IR) (Schuth et al., 2015 ;Cappellato et al.'],\n",
       "   [\"Ce syst√®me est bas√© sur l'historique des clics (Schuth et al., 2015).\"],\n",
       "   [\"489) qui d√©passe les valeurs des m√©triques obtenues pour l'ensemble des participants ainsi que la Baseline fournie par les organisateurs de la campagne d'√©valuation (Schuth et al., 2015).\"]],\n",
       "  [['The living lab approach has been applied in Information Retrieval (IR) research to primarily involve users in IR evaluation The living lab approach has been applied in Information Retrieval (IR) research to primarily involve users in IR evaluation [3,16].']],\n",
       "  [['‚Ä¢ How to run online LTR experiments at home ‚Ä¢ How to run online LTR experiments at home [25] ‚Ä¢ CLEF LL4IR: Living Labs for IR Evaluation [26] ‚Ä¢ TREC OpenSearch -Academic Search [32] [15 minutes] Discussion and conclusion.']],\n",
       "  [['To tackle this gap, two main user-centered initiatives, namely the LL4IR [2] and the NewsREEL [1] benchmarks running for CLEF, have been launched.']],\n",
       "  [['2004) and living labs (Schuth et al. 2015) provide and use a large range of different modalities in order to optimize the retrieval results.']],\n",
       "  [['Further, it might happen (and as we show in (Schuth et al, 2015a) it indeed did happen) during the test period that new products arrive; experimental systems were unable to include these in their ranking (this was the same for all participants), while the production system might return them.'],\n",
       "   ['A more in-depth analysis of the results is provided in the LL4IR extended lab overview paper (Schuth et al, 2015a).'],\n",
       "   ['Again, we refer to the LL4IR extended lab overview paper (Schuth et al, 2015a) for full details.'],\n",
       "   ['7 A more detailed analysis of the use-cases, including results from a second unofficial evaluation round, and a discussion of ideas and opportunities for future development is provided in the LL4IR extended lab overview paper (Schuth et al, 2015a).']],\n",
       "  [['The Living Labs for Information Retrieval Evaluation (LL4IR) held as part of CLEF 2015 presented a unique collaboration between industry and academia [Schuth et al., 2015, Balog et al.']],\n",
       "  [['The CLEF LL4IR track [11] featured product search and web search as use cases.']],\n",
       "  [['The CLEF LL4IR track [11] featured product search and web search as use cases.']]],\n",
       " [[['Idomaar is a novel reference framework that was developed in the context of the European project CrowdRec [9].']],\n",
       "  [['The offline task is described in more detail in [18].']],\n",
       "  [['To tackle this gap, two main user-centered initiatives, namely the LL4IR [2] and the NewsREEL [1] benchmarks running for CLEF, have been launched.'],\n",
       "   ['Unlike NewsReel framework [1], the proposed extension does not require any infrastructure to be deployed by participants.']],\n",
       "  [['Kille et al (2015) describe the offline task in greater detail.']],\n",
       "  [['As a point of reference, we consider the upper limit of 100 ms per recommendation prescribed by the CLEF NewsReel challenge [10].']]],\n",
       " [[['Authorship analysis traditionally used for the resolution of literary disputes is now found to be useful for solving pragmatic issues such as forensic inquiries, plagiarism detection, and various forms of social misconduct (Stamatatos, Potthast, Rangel, Rosso, & Stein, 2015).'],\n",
       "   ['For more information on trends in authorship analysis, please refer to the PAN/CLEF evaluation lab (Stamatatos et al., 2015).']],\n",
       "  [['PAN is a network for the digital text forensics [Stamatatos et al. 2015].']],\n",
       "  [[', [1,215,246]).']],\n",
       "  [['PAN has spurred widespread interest in this task among the research community, obtaining rather high participation figures in verification tasks from 2013 to 2015 [11,26,34].'],\n",
       "   ['[32]: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).'],\n",
       "   ['In 2015 In 2015 [34], besides age and gender identification, we introduced the task of personality recognition in Twitter.']],\n",
       "  [['For evaluation purposes, we consider standard AV corpora with minimal amount of training data, PAN-2013 [13], PAN-2014E and PAN-2014N [28], PAN-2015 [29], a collection of scientific papers mined from the web [3], and Amazon Reviews dataset [9].']],\n",
       "  [['Given a suspicious document, the goal of style-breach detection is identifying passages that exhibit different stylometric characteristics [233].'],\n",
       "   [\"In intrinsic plagiarism detection, combining feature analysis methods is a standard approach In intrinsic plagiarism detection, combining feature analysis methods is a standard approach [233], since an author's writing style always comprises of a multitude of stylometric features [127].\"],\n",
       "   ['Supervised machine-learning methods, specifically random forests, were the best-performing approach at the intrinsic detection task of the PAN 2015 competition [233].'],\n",
       "   ['They adopted the extrinsic verification paradigm by using texts from other authors to identify features that are characteristic of the writing style of the suspected author [233].']],\n",
       "  [[', grouping documents by authorship) [16,23].'],\n",
       "   ['Bagnall introduced an AA methodBagnall introduced an AA method1  [1] and obtained top positions in shared tasks in authorship verification and authorship clustering [16,23].']],\n",
       "  [['For this purpose, we use authorship verification datasets released by PAN in 2013 [18], 2014 [19] and 2015 [20].']],\n",
       "  [['While personality and emotion have been investigated in author profiling studies (Stamatatos et al., 2015;Rangel and Rosso, 2016a, i.'],\n",
       "   ['While personality and emotion have been investigated in author profiling studies (Stamatatos et al., 2015;Rangel and Rosso, 2016a, i.']]],\n",
       " [[['For text-oriented QA, the TREC [2,32] and CLEF [21] conference series offer a wealth of benchmark questions, but there is no design consideration on harnessing structured data at all.']]],\n",
       " [[['The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 [1] and 2015 [6] was to investigate how professional metadata (title, authors, .']],\n",
       "  [['The workshop builds on the results of the earlier discussion, and through the CLEF Social Book Search Lab The workshop builds on the results of the earlier discussion, and through the CLEF Social Book Search Lab [10] has already been pushing this line of research with a range of user studies, novel user interfaces, and analysis of large scale social data.']]],\n",
       " [[['Several authors Several authors (Lommatzsch et al (2016); Domann et al ( 2016); Beck et al ( 2017)) have used APACHE SPARK and APACHE MAHOUT.'],\n",
       "   ['A survey amongst participants (Lommatzsch et al, 2017) has revealed that one of the main motivations for them to participate was to acquire new skills that are currently in high demand in industry.']],\n",
       "  [['A newer version of this dataset is published by the CLEF 2017 NewsREEL [126] task, and a competition is held based on this data to train and evaluate news recommender systems.']]],\n",
       " [[['For significantly more complex tasks-where the photos are nearly unconstrained (depicting different plant organs or the whole plant in its natural environment), with complex background, and much higher numbers of classes (10,000 in the case of LifeCLEF 2017 For significantly more complex tasks-where the photos are nearly unconstrained (depicting different plant organs or the whole plant in its natural environment), with complex background, and much higher numbers of classes (10,000 in the case of LifeCLEF 2017 [81]), we choose a deep learning approach and utilize state-of-theart deep convolutional neural networks, which succeeded in a number of computer vision tasks, especially those related to complex recognition and detection of objects.']],\n",
       "  [['The 2017-th edition of the LifeCLEF plant identification challenge The 2017-th edition of the LifeCLEF plant identification challenge (Joly et al. 2017) is an important milestone towards automated plant identification systems working at the scale of continental floras with 10.']],\n",
       "  [['In fact, all top methods from the LifeCLEF 2017 contest, an event that aims to evaluate the performance of state-of-the-art identification tools for biological data, were based on deep learning 21 .']],\n",
       "  [['In the scope of LifeCLEF 2017 [6] in particular, we measured impressive identification performance achieved thanks to recent deep learning models (e.']],\n",
       "  [['In fact, all top methods from the LifeCLEF 2017 contest, an event that aims to evaluate the performance of state-of-the-art identification tools for biological data, were based on deep learning 21 .']],\n",
       "  [['Another way that has emerged in the past years is to train Convolutional Neural Networks (CNN) to recognize plants directly from pictures, as done by many researchers in the PlantCLEF challenge [2].'],\n",
       "   ['Its performances are inferior to those of the most recent CNN (GoogLeNet, ResNet, VGGNet) [2] but its linear architecture allows to visualize more easily the different convolutional layers.']],\n",
       "  [['At 2017 SeaCLEF challenge [ 12 ], one of our annotated dataset (Igiugig) was listed as a detection challenge to detect unclear objects (salmon) in underwater videos.']],\n",
       "  [['PlantCLEF 2017 PlantCLEF 2017 [5] was a plant species recognition challenge organized as part of the LifeCLEF workshop [9].']],\n",
       "  [['The last generation of DLAs offer much promise for passing the bottleneck of image or video analysis through automated species identification The last generation of DLAs offer much promise for passing the bottleneck of image or video analysis through automated species identification [20][21][22][23] .']],\n",
       "  [['These problems raised many difficulties for humans before the introduction of machines [13].']],\n",
       "  [['[62] gathered the LifeCLEF dataset.']]],\n",
       " [],\n",
       " [[[\"This article describes the participation of a group of master's students to the CLEF eHealth 2017 This article describes the participation of a group of master's students to the CLEF eHealth 2017 [4] Lab which aims at gathering research on NLP techniques dedicated to improve information retrieval and extraction in biomedical texts.\"]],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [['The organizers shared a dataset of 2,309 unannotated drug 4 Although CLEF eHealth is organized every year [13,14], its main focus is multi-linguality and information retrieval rather than clinical NLP Table 1 Clinical NLP Challenges, the tasks they posed, and the number of participating teams, since 2015, ordered by data sensitivity.']],\n",
       "  [['However, the previous evaluation was of limited scope and new French benchmarks have since been published, which has motivated a more exhaustive evaluation of all the new capabilities mostly with the following corpora: (i) the Quaero corpus (from CLEF eHealth 2015 [15]) which includes French MEDLINE citations in (titles & abstracts) and drug labels from the European Medicines Agency, both annotated with UMLS Semantic Groups and Concept Unique Identifiers (CUIs); (ii) the C√©piDC corpus (from CLEF eHealth 2017 [16]) which gathers French death certificates annotated with ICD-10 codes produced by the French epidemiological center for medical causes of death (C√©piDC 1 ).'],\n",
       "   ['Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks Since 2015, the main venue for the evaluation of French biomedical annotation are the CLEF eHealth information extractions tasks [16,41,42].']],\n",
       "  [[\"In 2013-2017, these tasks were patient-centric, with clinicians also considered from 2015, but in 2017 a pilot task on Technology Assisted Reviews (TAR) to support health scientists and policymakers' information access was also introduced (Suominen et al, 2013;Kelly et al, 2014;Goeuriot et al, 2015;Kelly et al, 2016;Goeuriot et al, 2017).\"]],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4,5,8,10,16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German.']],\n",
       "  [['Closer to our work are prior efforts on creating datasets for medical entity recognition from diverse sources, including death certificates (Goeuriot et al., 2017), scientific publications (Verspoor et al.']]],\n",
       " [[['This dataset has been collected over a period of 18 months from May 2015 to November 2016 using a predefined set of keywords related to cultural festivals in the world [6].']],\n",
       "  [[', 2016;Ermakova et al., 2017;Goeuriot et al.']],\n",
       "  [['Searching for background knowledge is close to INEX/CLEF Tweet Contextualization track 2011-2014 [7] and CLEF Cultural micro-blog Contextualization 2016, 2017 Workshop [14], but SimpleText differs from them by making a focus on selection of notions to be explained and the helpfulness of the information provided rather than its relevance.']],\n",
       "  [['Searching for background knowledge is close to INEX/CLEF Tweet Contextualization track 2011-2014 [7] and CLEF Cultural micro-blog Contextualization 2016, 2017 Workshop [14], but SimpleText differs from them by making a focus on selection of notions to be explained and the helpfulness of the information provided rather than its relevance.']]],\n",
       " [[['This pilot task is part of the imageCLEF 2017 Labs This pilot task is part of the imageCLEF 2017 Labs [13] and was introduced this year.']],\n",
       "  [['As a preliminary step, we apply our preliminary version As a preliminary step, we apply our preliminary version [15] of the proposed baseline search engine to solve the problems in Image-CLEFlifelog 2017 [3] of ImageCLEF 2017 [10].']],\n",
       "  [['Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog Together with these, we organize rigorous comparative benchmarking initiatives: NTCIR 12 -Lifelog [25,26], NTCIR 13 -Lifelog 2, and the LifeLog task [13] at ImageCLEF 2017 [29], which aim to bring the attention of personal live archive analytics to a wide audience and to motivate research into some of the key challenges of the field.']],\n",
       "  [['The increase of interest in lifelogging has resulted in many dierent research challenges being developed, such as the NTCIR lifelog task Semantic Access Task (LSAT) of the the NTCIR-12 challenge The increase of interest in lifelogging has resulted in many dierent research challenges being developed, such as the NTCIR lifelog task Semantic Access Task (LSAT) of the the NTCIR-12 challenge [2] and the ImageCLEF [4] lifelog task.']],\n",
       "  [['However, the prerequisite is the fast and accurate extraction of weed distribution in the farmland to adjust the herbicide application rates [9].']],\n",
       "  [['For the computer assisted analysis, it was noted that no solution has sufficient prediction accuracy (10)(11)(12)(13)(14).']],\n",
       "  [['NLP escalates different fields; for example, information retrieval [6], information extraction (IE) [58], machine translation [89], question answering [90], social media [62,63,66], and so on.']]],\n",
       " [[['Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models [7][8] [9].']]],\n",
       " [[[', 2016 ;Losada et al., 2017).'],\n",
       "   ['ERDE (Early Risk Detection Error), defined in (Losada et Crestani, 2016 ;Losada et al., 2017), takes into account the accuracy of the decisions and the delay of these decisions (i.']],\n",
       "  [[', 2017), depression detection (Losada et al., 2017;Losada & Crestani, 2016) or terrorism detection (Iskandar, 2017).'],\n",
       "   ['Both tools, the dataset and the evaluation measure, were later used in the first pilot task of eRisk Both tools, the dataset and the evaluation measure, were later used in the first pilot task of eRisk (Losada et al., 2017) in which 8 different research groups submitted a total of 30 contributions.'],\n",
       "   ['As observed in As observed in (Losada et al., 2017), among the 30 contributions submitted to the eRisk task, a wide range of different document representations and classification models were used.']],\n",
       "  [['org) [10][11][12], where training data is released.']],\n",
       "  [['This scenario, known as \"early risk detection\" (ERD) has gained increasing interest in recent years with potential applications in rumor detection [4,13], sexual predator detection and aggressive text identification [6], depression detection [11,10] or terrorism detection [9].'],\n",
       "   [\"Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection [11,12] and eRisk 2018 early anorexia detection [12].\"],\n",
       "   ['[11], it was designed to take into account not only the correctness of decisions but also the delay taken to emit them.'],\n",
       "   ['As it is described in more detail in the overview of each task As it is described in more detail in the overview of each task [11,12] and the CLEF Working Notes, 8 a total of 180 models were submitted to these three eRisk tasks, ranging from simple to more advanced deep learning models.']],\n",
       "  [[', 2015;Chung & Pennebaker, 2007;Losada, Crestani, & Parapar, 2017;Pennebaker, 2011) (2017), Hung et al.']],\n",
       "  [['eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection [4][5][6].'],\n",
       "   ['full description and analysis of the results can be found in the lab overviews full description and analysis of the results can be found in the lab overviews [4][5][6] and working note proceedings.']],\n",
       "  [['Considering that the signs and symptoms of these disorders have been proven to be traceable on social media, scientists have started to work on the development of automated methods to detect signs and symptoms of these conditions [2][3][4][5] by addressing the importance of early detection [6,7].'],\n",
       "   ['Our main contributions are listed as follows: (1) we defined a methodology to generate a reliable Twitter dataset for suicide risk assessment, which is also the first user-level dataset of this type dedicated to posts in Spanish; (2) we presented a method to obtain a subset of the user tweets related to a specific topic, in this case, suicidal ideation; (3) we generated models that explore the impact of not just relational and behavioral factors but also elements identified by specialists during consultations, which have been mapped to social networks; (4) we developed image-based predictive models to detect suicidal ideation; (5) we integrated the previous elements into a method that combines multimodal data to build predictive models that address the detection of mental health issues using cross-platform information (Reddit, Instagram, and Twitter); and (6) we refined the evaluation process of predictive models for mental health issues by considering 2 different types of control groups within the social media context: users with posts that might not use terms related to mental conditions (generic control cases) and users who make use of terms related to mental disorders (focused control group).']],\n",
       "  [['In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.']],\n",
       "  [['One of the few exceptions are the datasets developed under the CLEF eRisk challenge [18,[26][27][28][29].'],\n",
       "   ['The interactions between SM activity and psychological traces have been the focus of attention of popular evaluation campaigns, such as the CLPsych shared tasks [32,33], organised within the Computational Linguistics and Clinical Psychology Workshop, or the eRisk tasks [26][27][28], oriented to early detection of signs of psychological problems and organised within the Conference and Labs of the Evaluation Forum (CLEF).']],\n",
       "  [['org/) at CLEF (Conference and Labs of the Evaluation Forum) proposed an author profiling task where the aim is to identify specific mental conditions such as depression [15].']],\n",
       "  [['The Conference and Labs of Evaluation Forum for Early Risk XSL ‚Ä¢ FO RenderX Prediction (CLEF eRISK) is a public competition about different areas such as health and safety [26].']],\n",
       "  [['In 2017, CLEF introduced the eRisk shared task, which invites entrants to create experimental models for predicting mental health risks using social media data sets supplied by the conference organizers In 2017, CLEF introduced the eRisk shared task, which invites entrants to create experimental models for predicting mental health risks using social media data sets supplied by the conference organizers [78].']],\n",
       "  [[\"One of the most known workshops is from the CLEF's (Conference and Labs of the Evaluation Forum) Early Risk Prediction on the Internet (eRisk) lab [23] wherein the task for early detection of depression consisted of processing Reddit posts sequentially and detecting depression signs as early as possible.\"],\n",
       "   ['Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 [23,40].']],\n",
       "  [[', 2018;Losada et al., 2017).']],\n",
       "  [['Erisk@CLEF Erisk@CLEF (Losada et al., 2017) served as a root for this task, which aims to detect depression from the social media data.']],\n",
       "  [['In (Losada et al., 2017), the organisers of eRisk 2017 develop a pilot project which main purpose is the identification of early risk detection of depression.']],\n",
       "  [['Therefore, early diagnosis of this problem is very important and is a challenge for individual and public health (Losada et al., 2017).']],\n",
       "  [['However, for social networks which update quickly, another setting, early risk detection (ERD) [Losada et al., 2017], may have more potential to detect and offer timely help to risky users.'],\n",
       "   ['We mainly use the eRisk2017 dataset We mainly use the eRisk2017 dataset [Losada and Crestani, 2016] in our experiment, which is adopted as the benchmark in the ERD task of CLEF 2017 [Losada et al.'],\n",
       "   ['We mainly use the eRisk2017 dataset We mainly use the eRisk2017 dataset [Losada and Crestani, 2016] in our experiment, which is adopted as the benchmark in the ERD task of CLEF 2017 [Losada et al., 2017].'],\n",
       "   ['We then test model performance in the ERD setting, using the official metrics ERDE 5 and ERDE 50 We then test model performance in the ERD setting, using the official metrics ERDE 5 and ERDE 50 [Losada et al., 2017].'],\n",
       "   ['Early attempts by Losada and Crestani[2016] used TF-IDF and Logistic Regression on all user posts for depression detection.']],\n",
       "  [[\"Core objectives of this work include optimizing psychiatric treatment, identifying early stages of mental illness, and measuring the effect of public policy on a population's well-being (Losada et al., 2017;Fine et al.\"]],\n",
       "  [['2015, Losada et al. 2017, Lynn et al.']],\n",
       "  [['To further add to our findings, we also include the eRisk 2018 dataset To further add to our findings, we also include the eRisk 2018 dataset (Losada et al., 2017(Losada et al.']]],\n",
       " [],\n",
       " [[[', 2021;Kordjamshidi et al., 2017).'],\n",
       "   ['This dataset is built on MSPRL (Kordjamshidi et al., 2017) corpus while we added human-generated spatial questions and Metaphoric usages and implicit meaning are not covered in this work.'],\n",
       "   ['The MSPRL (Kordjamshidi et al., 2017)   10a) for relation types.']]],\n",
       " [[['A further related corpus was released as part of the ShARe/CLEF 2013 NER task [32].'],\n",
       "   ['The portability of the models trained on PhenoCHF was furthermore demonstrated through their application to the corpus released for the NER task of ShARe/CLEF 2013 The portability of the models trained on PhenoCHF was furthermore demonstrated through their application to the corpus released for the NER task of ShARe/CLEF 2013 [32], whose annotations partially overlap with those in PhenoCHF.']],\n",
       "  [['CLEF 2013 [40] started a task for acronym/abbreviation normalization, with a focus on mapping acronyms and abbreviations to concepts in the Unified Medical Language System (UMLS) [41].']],\n",
       "  [['Following recommendations such as those for the evaluation campaign CLEF eHealth [13], we considered that if an anatomical concept was part of a disease, the query should not be labelled as anatomical.']],\n",
       "  [['BioCreative [37], I2B2 [35] and CLEF ShARE [34]).']],\n",
       "  [['Specific mentions of diseases and signs or symptoms were similarly annotated under the ShARe scheme [16,17] and additionally linked to terms in the SNOMED Clinical Terms vocabulary [18].']],\n",
       "  [['Textual annotations used the ShARE/CLEF 2013 corpus [18] which we describe later.']],\n",
       "  [['However, a new evaluation campaign introduced in 2013, CLEF eHealth [103], considers types of queries posed by laypeople searching the web for medical information.'],\n",
       "   ['The IR evaluation described in this article was carried out on the CLEF eHealth 2013 Task 3 test collection The IR evaluation described in this article was carried out on the CLEF eHealth 2013 Task 3 test collection [103].']],\n",
       "  [['In the domain of medicine, for example, several annotated corpora have been created [2][3][4][5][6][7].'],\n",
       "   [', [2][3][4][5][6][7]) are largely focussed on identifying medical disorders, signs, symptoms, treatments and tests, and/or relationships that link them together.']],\n",
       "  [[\"In the framework of CLEF eHealth 2014 [12], one of the proposed shared tasks (task 3) addresses this challenge [8]: queries have been defined from real patient cases issued from the clinical documents provided by the CLEF eHealth 2014's task 2.\"]],\n",
       "  [['CLEF 2013 (Suominen et al., 2013) started a task for acronym/abbreviation normalization, using the UMLS 8 as target terminology.']],\n",
       "  [[', 2014;Doƒüan & Lu, 2012a), and was also applied to clinical notes in the ShARe / CLEF eHealth task (Suominen et al., 2013), where it achieved the highest normalization performance out of 17 international teams (Leaman et al.']],\n",
       "  [['Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words (Chapman et al., 2013).'],\n",
       "   ['For entity encoding, all participating systems were evaluated using accuracy, in \"strict\" and \"relaxed\" modes, as defined in (Suominen et al., 2013).']],\n",
       "  [['The more recent 2013 CLEF-eHEALTH challenge (Suominen et al., 2013) corpus consists of EHRs annotated with named entities referring to disorders and acronyms/abbreviations, mapped to UMLS concept identifiers.']],\n",
       "  [['Nous travaillons sur des donn√©es ind√©pendantes fournies par les comp√©titions CLEF eHealth 2013 et 2014 (Kelly et al., 2013 ;Goeuriot et al.'],\n",
       "   ['Les donn√©es fournies par les comp√©titions CLEF eHealth 2013 et 2014 (Kelly et al., 2013 ;Goeuriot et al.']],\n",
       "  [['2014;Kelly et al. 2014).'],\n",
       "   ['As evaluation set we use the training and test collections from CLEF eHealth task 3a As evaluation set we use the training and test collections from CLEF eHealth task 3a (Kelly et al. 2014): the CLEF document collection and five train ?'],\n",
       "   ['As evaluation set we use the training and test collections from CLEF eHealth task 3a As evaluation set we use the training and test collections from CLEF eHealth task 3a (Kelly et al. 2014): the CLEF document collection and five train ?']],\n",
       "  [[', i2b2 [5,7,43,44] and the ShARe/CLEF eHealth Evaluation Labs [45][46][47].'],\n",
       "   [', i2b2 [5,7,43,44] and the ShARe/CLEF eHealth Evaluation Labs [45][46][47].'],\n",
       "   ['However, the increasing emergence of annotated corpora that include gold-standard links between entity mentions and concepts in terminological resources [33,34,45] has stimulated a large amount of research into the development of dedicated normalisation methods for both biomedical scientific text and narrative clinical text.']],\n",
       "  [[', 2010;Suominen et al., 2013;White and Horvitz, 2015).'],\n",
       "   ['To design reliable simulated clinical search tasks, we used the medical cases provided within major medical IR evaluation tracks namely the TREC 1 Filtering (Robertson and Hull, 2000) and the CLEF 2 e-Health (Suominen et al., 2013) with related search contexts.'],\n",
       "   ['All of the participants, experts or novices, aimed to solve the same healthrelated search tasks for which information needs were extracted from two clinical information datasets issued from major IR evaluation campaigns, namely TREC Filtering All of the participants, experts or novices, aimed to solve the same healthrelated search tasks for which information needs were extracted from two clinical information datasets issued from major IR evaluation campaigns, namely TREC Filtering (Robertson and Hull, 2000) and CLEF E-Health (Suominen et al., 2013).'],\n",
       "   ['The overall goal of the ShARe eHealth track is to evaluate systems that assist novices in understanding their health-related information Suominen et al. (2013).']],\n",
       "  [['Moreover, many biomedical shared tasks are devoted to disorder recognition, including i2b2 2010 15 , ShARe/ CLEF eHealth task 2013 16 , SemEval 2014 task 7 17 , and SemEval 2015 task 14 18 .']],\n",
       "  [['In the past several years, lots of machine learning-based clinical entity recognition systems have been proposed, may due to some publicly available corpora provided by organizers of some shared tasks, such as the Center for Informatics for Integrating Biology & the Beside (i2b2) 2009 [8], 2010 [9][10][11][12][13], 2012 [14][15][16][17][18] and 2014 track1 [19][20][21][22][23] datasets, ShARe/CLEF eHealth Evaluation Lab (SHEL) 2013 dataset [24], and SemEval (Semantic Evaluation) 2014 task 7 [25], 2015 task 6 [26] 2015 task 14 [27], and 2016 task 12 [28] datasets.']],\n",
       "  [['These results are consistent with previous investigations [7] on i2b2/VA 2010 and ShARe/CLEF 2013 [33] datasets.']],\n",
       "  [['Using the ShARe corpus, participants in the 2014 ShARe/CLEF Task 2 were tasked with normalizing semantic modifiers related to disease mentions in clinical texts [35].']],\n",
       "  [[', 2011) and ShARe/CLEF 2013 (Suominen et al., 2013).'],\n",
       "   [', 2011), the TREC Medical Records Track collection (Voorhees & Tong, 2011), and the ShARe/CLEF 2013 train set (Suominen et al., 2013) (for further technical details see (De Vine et al.']],\n",
       "  [['Communitywide shared task competitions have relied on such corpora to advance the start-of-the-art in biomedical NLP [19,[32][33][34][35].']],\n",
       "  [['Each year CLEF eHealth offers IR, information extraction (IE) and information management tasks to volunteer task participants which aim to evaluate systems that support laypeople in searching for and understanding health information [15,27,28,57].']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [['These are mainly built for the purposes of challenges (Kelly et al., 2013;Goeuriot et al.']],\n",
       "  [['Many researchers made important contributions to dataset construction including the GENIA corpus [36], the NCBI disease corpus [37], and the ShARe/CLEF eHealth evaluation [38].']],\n",
       "  [['In the past several years, as a number of manually annotated corpora have been publically available for clinical entity recognition in challenges such as the Center for Informatics for Integrating Biology & the Beside (i2b2) In the past several years, as a number of manually annotated corpora have been publically available for clinical entity recognition in challenges such as the Center for Informatics for Integrating Biology & the Beside (i2b2) [4,[9][10][11], ShARe/CLEF eHealth Evaluation Lab (SHEL) [12,13], SemEval (Semantic Evaluation) [14][15][16][17], etc.']],\n",
       "  [[\"In 2013-2017, these tasks were patient-centric, with clinicians also considered from 2015, but in 2017 a pilot task on Technology Assisted Reviews (TAR) to support health scientists and policymakers' information access was also introduced (Suominen et al, 2013;Kelly et al, 2014;Goeuriot et al, 2015;Kelly et al, 2016;Goeuriot et al, 2017).\"]],\n",
       "  [[\"Similar to the TREC CDS track, the 2013 to 2015 editions of CLEF's eHealth challenge [23] offered a patient-centric information retrieval task.\"]],\n",
       "  [['To promote the development of machine learning-based system, many publicly available corpora have been developed by organizers of some clinical NLP challenges such as the Informatics for Integrating Biology and the Bedside (i2b2) 2009 [8], 2010 [9][10][11][12][13], 2012 [14][15][16][17][18], 2014 [19][20][21][22][23], ShARe/CLEF eHealth Evaluation Lab 2013 dataset [24], and Semantic Evaluation 2014 task 7 [25], 2015 task 6 [26], 2015 task 14 [27], and 2016 task 12 [28] datasets.']],\n",
       "  [['The ShARe/CLEF 2014 [6] and SemEval 2015 [7] organized open challenges on detecting disorder mentions (subtask 1) and identifying various attributes (subtask 2) for a given disorder, including negation, severity, body location etc.']],\n",
       "  [['In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 [6], 2010 [7], 2012 [8] and 2014 [9], the NLP challenges organized by SemEval in 2014 [10], 2015 [11] and 2016 [12], and the NLP challenges organized by ShARe/CLEF in 2013 [13] and 2014 [14].']],\n",
       "  [['The ShARe / CLEF eHealth labs shared a set of annotated clinical notes for two shared-task editions [9,33] and three different NLP tasks consisting of: (1) named entity recognition (NER) and normalization of disorders, (2) normalization of acronyms and abbreviations, and (3) patient information retrieval.']],\n",
       "  [['The authors extended an annotation scheme from the CLEF eHealth 2014 Task 2 (Suominen et al., 2013).']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].']],\n",
       "  [['These are often built for the purposes of shared tasks [26,27].']],\n",
       "  [['We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30].']],\n",
       "  [['30 The clinical NLP community has organized a series of open challenges with a focus on clinical concept extraction, including Informatics for Integrating Biology & the Bedside (i2b2), 23,31,32 National NLP Clinical Challenges (n2c2), 25 SemEval, [33][34][35] and ShARe/CLEF 36,37 in the past decade.']],\n",
       "  [['The methods mentioned above have also been applied for clinical entity recognition and RE, such as the NLP challenges organized by i2b2 in 2009 [9], 2010 [10], 2012 [11], and 2014 [12], the NLP challenges organized by SemEval in 2015 [13] and 2016 [14], the NLP challenges organized by ShARe/CLEF in 2013 [15] and 2014 [16], and the NLP challenges organized by BioCreative/OHNLP in 2018 [17].']],\n",
       "  [['Most of these corpora come in the form of shared tasks such as i2b2 [14], ShARe/CLEF [15,16], and SemEval [17].']],\n",
       "  [['In the clinical domain, the ShARe/CLEF 2013 eHealth Eval-51 uation Lab and the i2b2/VA challenge methodologies have been applied in shared tasks 52 [11][12][13].']],\n",
       "  [['In particular, we use the top-k accuracy as an evaluation metric, following the previous works [42,36,47,35,41,43].']],\n",
       "  [[', 2015, Suominen et al., 2013, Bethard et al.']],\n",
       "  [['Research on concept normalization has grown thanks to shared tasks such as disorder normalization in the 2013 ShARe/CLEF Research on concept normalization has grown thanks to shared tasks such as disorder normalization in the 2013 ShARe/CLEF (Suominen et al., 2013), chemical and disease normalization in BioCreative V Chemical Disease Relation (CDR) Task (Wei et al.'],\n",
       "   [', 2019) and ShARe/CLEF (Suominen et al., 2013)  We take 40 clinical notes from the released data as training, consisting of 5,334 mentions, and the standard evaluation data with 6,925 mentions as our test set.']],\n",
       "  [['In the clinical domain, various NLP shared task challenges have been introduced for medical concept extraction, such as the i2b2 Challenge Shared Tasks In the clinical domain, various NLP shared task challenges have been introduced for medical concept extraction, such as the i2b2 Challenge Shared Tasks [4] and ShARe/CLEF e-health Shared Task [9,10].']],\n",
       "  [['Many researchers made important contributions to dataset construction including the GENIA corpus [36], the NCBI disease corpus [37], and the ShARe/CLEF eHealth evaluation [38].']],\n",
       "  [['In the biomedical domain, widely used datasets include 2010 i2b2/VA 17 and ShARe/CLEF 18 for clinical texts, NCBI disease corpus 19 for diseases, BioCreative II Gene Mention corpus (BC2GM) 20 and JNLPBA 21 for gene and proteins, BioCreative V Chemicals Disease Relationship (BC5CDR) 22 for diseases and chemicals, CHEMDNER 23 for drug and chemicals, LINNAEUS 24 and Species-800 25 for species, and the Plant corpus 14 for plant entities.']],\n",
       "  [['State-of-the-art models are increasingly successful in high-resource languages such as English or Spanish, where labeled datasets include ShARe/CLEF eHealth 2013 Task 1 (Suominen et al., 2013), SemEval-2014Task 7 (Prad-han et al.'],\n",
       "   ['Following previous works on entity linking (Suominen et al., 2013;Pradhan et al.']],\n",
       "  [['ShARe/CLEF [17] is one of the widely used NEN datasets for bioinformatics that is made up of clinical notes.']],\n",
       "  [['Several benchmarks have also been constructed, including for the biomedical area (Suominen et al., 2013) and the scientific area (SciAD, Veyseh et al.'],\n",
       "   ['Most current public datasets for acronym expansion are focused on a particular domain, such as the biomedical domain Most current public datasets for acronym expansion are focused on a particular domain, such as the biomedical domain (Suominen et al., 2013;Wen et al.']],\n",
       "  [['The representative clinical text datasets are MIMIC-III [25] and NLP community challenges, such as n2c2 NLP Research Data Sets [26], ShARe/CLEF eHealth [27], and CEGS N-GRID [19].']],\n",
       "  [[', 2003;Petersen and Ostendorf, 2007;De Belder and Moens, 2010;Suominen et al., 2013).']],\n",
       "  [['Assignment 3 is a medical-associated website document set, 5 evolution objections, a website document outcome set, and fifty analysis queries (Suominen et al. (2013); Kelly et al.']]],\n",
       " [[['Unreleased sequences from ViDRILO have been successfully used in the RobotVision at Image-CLEF competition [13] in 2013 [3] and 2014 [2].']],\n",
       "  [['ImageClef, the CLEF Cross Language Image Retrieval Track54 , is a benchmark for the evaluation of cross-language annotation and retrieval of images (Caputo et al, 2014).']],\n",
       "  [['ImageCLEF-DA ImageCLEF-DA [4] originally used for the ImageCLEF 2014 domain adaptation challenge consists of twelve common classes from three domains: ImageNet ILSVRC 2012 (I), Pascal VOC 2012 (P), and Caltech-256 (C).']],\n",
       "  [['Following the standard evaluation protocol in the unsupervised domain adaptation community, we evaluate our method on the digit classification task using MNIST Following the standard evaluation protocol in the unsupervised domain adaptation community, we evaluate our method on the digit classification task using MNIST [16], SVHN [17] and USPS [18] as well as the object recognition task using the Office-31 [19], Office-Home [20] and ImageCLEF-DA dataset [21], and demonstrate the superiority of the proposed method.'],\n",
       "   ['ImageCLEF-DA ImageCLEF-DA [21] is built for the ImageCLEF 2014 domain adaptation challenge with three domains, including Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P).']],\n",
       "  [['Theorem 1), we propose an amortization solution for minimizing the OT distance in (3).'],\n",
       "   ['The following theorem justifies solving the optimization problem in (4) rather than directly solving the one in The following theorem justifies solving the optimization problem in (4) rather than directly solving the one in (3).'],\n",
       "   ['Assuming that the transportation network T belongs to a family of models with infinite capacity, which means it has the ability to approximate any continuous function with arbitrary precision, then the optimization problem in ( 4) is equivalent to the optimization problem in (3).'],\n",
       "   ['The final dataset used in our experiments is the ImageCLEF-DA dataset The final dataset used in our experiments is the ImageCLEF-DA dataset [3].'],\n",
       "   ['Therefore, (G * , T * ) is the optimal solution of the OP in Therefore, (G * , T * ) is the optimal solution of the OP in (3).'],\n",
       "   ['One interesting study is the comparison between our amortization solution and the Sinkhorn algorithm One interesting study is the comparison between our amortization solution and the Sinkhorn algorithm [9] in solving the optimization problem in (3).']],\n",
       "  [['The importance of bridging the semantic gap is reflected by the emergence of benchmarks such as TRECVID [35] and ImageCLEF [8].']]],\n",
       " [],\n",
       " [[['In this context, using multimedia identification and collaborative data management tools is considered as one of the most promising solution to help bridging the taxonomic gap In this context, using multimedia identification and collaborative data management tools is considered as one of the most promising solution to help bridging the taxonomic gap [18,5,4,17,25,19].']],\n",
       "  [['Official results of the evaluation (based on the score S detailed above) are synthesized in the overview paper of LifeCLEF lab [32] and further developed in the working note of the plant task [19].']],\n",
       "  [['There have been recent international competitions which have examined this topic, for instance the recent ImageCLEF challenge for plant classification (including leaf images) [11].']],\n",
       "  [['The LifeCLEF Bird task proposes to evaluate one of these challenges The LifeCLEF Bird task proposes to evaluate one of these challenges [12] based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios.']],\n",
       "  [['This paper presents the participation of Inria ZENITH team to the 2015-edition of this challenge [9,19].']],\n",
       "  [['This was followed by other applications, such as Pl@ntNet (iOS, Android, and web) and Folia (iOS) dedicated to the European flora [70].']],\n",
       "  [['The LifeCLEF challenge The LifeCLEF challenge (Joly et al. 2014) uses a subset of our data for their challenge (Spampinato et al.'],\n",
       "   ['Finally, we indicate possible uses of this data, where both for marine ecology and computer vision/image processing new research areas and challenges can be explored using the data, where one of the important challenges created after the project is the LifeCLEF challenges (Joly et al. 2014;Spampinato et al.']],\n",
       "  [['The LifeCLEF2015 Fish classification challenge dataset LCF-15 The LifeCLEF2015 Fish classification challenge dataset LCF-15 [24], [25] contained 20,000 labeled images, 93 videos and 15 fish species.'],\n",
       "   ['73% on the LCF-15 [24], [25] dataset.'],\n",
       "   ['For the weak positive supervision, two specialized fishdomain datasets were utilized: the LCF-15 classification challenge dataset For the weak positive supervision, two specialized fishdomain datasets were utilized: the LCF-15 classification challenge dataset [24], [25] with 22.'],\n",
       "   ['Clearly, the additional positive weak supervision could not improve the false-negative rate (F N ) significantly, where the external fish images (in LCF-15 Clearly, the additional positive weak supervision could not improve the false-negative rate (F N ) significantly, where the external fish images (in LCF-15 [24], [25] and QUT2014 [43], [44]) were very different from the project-domain fish images.']],\n",
       "  [['Thus, considerable research efforts have been put towards developing systems for the task of understanding complex marine environments and distinguishing between a diverse set of fish species, which are based on publicly available fish datasets 1,3,8,15,35 .'],\n",
       "   ['Many datasets exist for fish analysis 3,8,9,15 .'],\n",
       "   ['We use accuracy to evaluate the models on this task which is a standard metric for binary classification problems 3,8,9,15,27 .']]],\n",
       " [[['An example living lab is CLEF NEWSREEL 1 An example living lab is CLEF NEWSREEL 1 [13], a campaign-style evaluation lab on news recommendation in realtime which is organized as part of CLEF 2014.'],\n",
       "   ['CLEF NEWSREEL CLEF NEWSREEL [13] is a campaign-style living lab that is organized as part of CLEF 2014.']],\n",
       "  [['As users visit selected news publishers, ORP randomly forwards recommendation requests to registered participants [8].']],\n",
       "  [[\"While last year's lab overview paper provided a detailed description of the online evaluation in a so-called living lab environment [14], this paper focuses on the simulation based evaluation that was applied in Task 2.\"],\n",
       "   ['[14], Section 3.']],\n",
       "  [['For further details about the evaluation scenario, the reader is referred to [9].']],\n",
       "  [['The living lab is described in detail in [12].']],\n",
       "  [['Since users are following their own agenda, laboratory biases on their behavior can be neglected [18].']],\n",
       "  [['Laboratory-based studies suffer from the standard critiques of artificial envi ronments and, often, unrepresentative participant sampling [15].']],\n",
       "  [['For a more detailed description of the recommendation scenario, the provided content and its users, the reader is referred to [9].'],\n",
       "   ['As described in [9], the vast majority of these users come from one of the Germanspeaking countries (Germany, Austria, Switzerland) in Central Europe.']],\n",
       "  [['It implements the idea of living laboratories where researchers gain access to the resources of a company to evaluate different information access techniques using A/B testing [7].']]],\n",
       " [],\n",
       " [],\n",
       " [[[\"Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab [7], SemEval Twitter SA [8], and aspect based SA tasks [9,10].\"]],\n",
       "  [['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter, see PAN [18] and RepLab [19] overviews for more details.'],\n",
       "   ['The RepLab 2014 \"Author Ranking\" task was specifically focused on influence The RepLab 2014 \"Author Ranking\" task was specifically focused on influence [19], as explained in more details in Section III.'],\n",
       "   ['The CLEF RepLab 2014 dataset The CLEF RepLab 2014 dataset [19] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum2 (CLEF).'],\n",
       "   ['The RepLab framework The RepLab framework [19] uses the Mean Average Precision (MAP) to evaluate the estimated rankings.'],\n",
       "   ['Actually, in RepLab 2014 [19], the organizers were not able to conclude on significant differences between certain participants due to the number of considered domains.']],\n",
       "  [['3 These tasks were organized as a CLEF evaluation task [8,9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['Keeping these different dimensions in view, the task of reputation dimensions classification was first introduced within RepLab 2014 [9].'],\n",
       "   ['9 In the month of June 2015.']],\n",
       "  [['More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter: PAN [59] and RepLab [3].'],\n",
       "   ['The RepLab Challenge 2014 dataset The RepLab Challenge 2014 dataset [3] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum 1 (CLEF).'],\n",
       "   ['The RepLab framework The RepLab framework [3] uses the traditional Mean Average Precision (MAP) to evaluate the estimated rankings.'],\n",
       "   ['In RepLab 2014 [3], the organizers were not able to conclude on significant differences between participants (and features or methods used) due to the small number of considered domains.']],\n",
       "  [[\"We use the context of RepLab We use the context of RepLab [2,3] tasks to evaluate our proposal that is to say: to propose an overview of 61 entity's (drawn in 4 domains: Automotive, Banking, Music and University) e-Reputation regarding experts taxonomies using provided set and pertaining of Micro-Blogs concerning each entity.\"],\n",
       "   ['It leads us to consider probability re-estimation of a document d in a class c using a smoothing as defined in (3).']],\n",
       "  [['Specifically, we evaluate the models for five applications: (1) predict whether the sentiment of tweet is positive, negative or neutral (SA) [26], (2) predict the entity the tweet belongs to (EI) [27], (3) predict the priority of the topic the tweet belongs to (TP) [ From the results, we find that Paragraph2Vec has poor performance for all the tasks compared to BOW.']],\n",
       "  [['There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [13], event and rumor detection [3,16], topic popularity and summarization [2,23], information diffusion [11], popularity prediction [18], and reputation monitoring [1].'],\n",
       "   [\"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\"]],\n",
       "  [['(1)[293] Other(8) [62,171,179,[197][198][199]214,294] Hotel Reviews(24) [[93][94][95][96][97][98][99][100]102,103,[127][128][129][130][131][132][133][135][136][137][138][139][140][141][142] https://doi.']],\n",
       "  [['In 2014, RepLab focused on two aspects of reputation analysis, which is Reputation Dimensions Classification and Author Profiling (Amigo et al., 2014).']],\n",
       "  [['Finally, the Twitter subcorpus was constructed in cooperation with RepLab [3] in order to address also the reputational perspective (e.']],\n",
       "  [[', , 2013(Amig√≥ et al., , 2014) ) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance.']],\n",
       "  [['There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [24], event and rumor detection [16,28], topic popularity and summarization [2,42], information diffusion [18], popularity prediction [34], and reputation monitoring [1].'],\n",
       "   [\"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\"]],\n",
       "  [['Authorities receive interactions in particular from hubs whereas hubs connect to a lot of authorities [21].']],\n",
       "  [['O gerenciamento da reputa√ß√£o digital √© uma importante an√°lise que serve para medir como √© a reputa√ß√£o de uma empresa em rela√ß√£o a certos grupos de interessados [1].'],\n",
       "   ['Para avaliar experimentalmente a proposta, ela foi experi-mentada usando o conjunto de publica√ß√µes do desafio do RepLab 2014 [1], que consiste de publica√ß√µes no Twitter extra√≠das em 2012 durante o per√≠odo de 1 o de Junho at√© 31 de Dezembro, com cerca de 48 mil tweets rotulados em 8 assuntos.'],\n",
       "   ['[11], vencedores da RepLab2014 [1], prop√µem resolver esta tarefa usando a Wikip√©dia para enriquecer as publica√ß√µes e treinando um classificador SVM (Support Vector Machines) para aprender a classificar as dimens√µes de reputa√ß√£o.'],\n",
       "   ['Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 [1] e as m√©tricas de avalia√ß√£o.']],\n",
       "  [['The main idea is then to predict For selecting relevant users we could make use of existing resources such as analytical platforms, like Socialbakers \\uf0d2 , or existing datasets, like RepLab 2014 [34].']],\n",
       "  [['[70] utilising the RepLab 2014 corpus [71] and Azzouza et al.'],\n",
       "   ['It was noticeable that the majority of studies sought to create their own training data, with only 6 studies using publicly available precollected data sets, and only 4 of these originated from gold-standard data sets produced as part of NLP community challenges like RepLab and SemEval, focused on reputational classification [71] and sentiment [69,63].']],\n",
       "  [['Other studies aimed at specific user profiles such as campaign promoters, bots, influencers, political position and polarity (Amig√≥ et al., 2014;Li et al.'],\n",
       "   ['In the RepLab 2014 edition (Amig√≥ et al., 2014), one of the objectives was author profiling in the automotive and banking domains.'],\n",
       "   ['We also evaluated our results against the RepLab dataset ( We also evaluated our results against the RepLab dataset ( Amig√≥ et al., 2014) with the revision of tags presented in (Nebot et al.'],\n",
       "   ['It must be noticed that the evaluation results are not comparable to those published in It must be noticed that the evaluation results are not comparable to those published in (Amig√≥ et al., 2014) and (Nebot et al.']],\n",
       "  [['Regarding reputation, most work has been focused on identifying the influential users in a specific domain Regarding reputation, most work has been focused on identifying the influential users in a specific domain (Amigo ¬¥et al. 2014).'],\n",
       "   [\"These categories are inspired in the RepLab 2014 dataset and designed according to the experts' criteria involved in the project (Amigo ¬¥et al. 2014).\"],\n",
       "   ['The first one is the RepLab 2014 dataset (Amigo ¬¥et al. 2014), which contains a track for the automotive domain.']],\n",
       "  [['Although significant advances have been made in RepLab 1 [1,2].'],\n",
       "   ['Most of the contributions on reputation monitoring to extract sets of tweets requiring a particular attention from a reputation manager have been proposed in the last editions of RepLab [1,2].'],\n",
       "   [\"RepLab'2014 [2] focused on the reputation dimension classification.\"],\n",
       "   [\"We perform a supervised classification over Replab'2013-14 dataset We perform a supervised classification over Replab'2013-14 dataset [1,2].\"]]],\n",
       " [[['We also witness the raise of new activities aimed at verifying the reproducibility of results: For example, the \"Reproducibility Track\" at ECIR since 2015 hosts papers that replicate, reproduce, and/or generalize previous research results while CLEF/NTCIR/TREC REproducibility6 (CENTRE) is a new joint evaluation activity, started in 2018, to assess and quantify the extent of replicability and reproducibility of our experimental results [7].']],\n",
       "  [['We also witness the rise of new activities aimed at verifying the reproducibility of results: for example, the \"Reproducibility Track\" at ECIR since 2015 hosts papers that replicate, reproduce, and/or generalize previous research results, while CLEF/NTCIR/TREC REproducibility6 (CENTRE) is a new joint evaluation activity, started in 2018, to assess and quantify the extent of replicability and reproducibility of our experimental results [7].']],\n",
       "  [['CENTRE@CLEF 2018 -CLEF/NTCIR/TREC Reproducibility5 aims to run a joint CLEF/NTCIR/TREC task on challenging participants: 1) to reproduce best results of best/most interesting systems in previous editions of CLEF/NTCIR/TREC by using standard open source IR systems; 2) to contribute back to the community the additional components and resources developed to reproduce the results in order to improve existing open source systems [4].']]],\n",
       " [[['This was the goal of the ExpertCLEF challenge, organized as part of the LifeCLEF 2018 campaign [5].']],\n",
       "  [['Through its biodiversity informatics related challenges, LifeCLEF is intended to push the boundaries of the state of the art in several research directions at the frontier of multimedia information retrieval, machine learning and knowledge engineering [8].']],\n",
       "  [['The evaluation measure used for the species detection task will be the classification mean Average Precision (c-mAP [12]).']],\n",
       "  [['To our knowledge, research on automated identification of organisms using deep learning, although an active research area ( [10][11][12][13][14][15]), has not addressed the issue when only small datasets are available.'],\n",
       "   ['In 2018 the accuracy of human experts and the best algorithms that competed was compared In 2018 the accuracy of human experts and the best algorithms that competed was compared [12].']],\n",
       "  [['For both tasks, the evaluation measure will be the classification mean average precision (c-mAP, [39]).']],\n",
       "  [['With the rapid developments of AI demonstrated by machines, AI technology can provide personalized product or service recommendations by investigating past consumption experiences and preferences of consumers [23,34].'],\n",
       "   ['However, with the development of computer technology, AI can also offer a high level of personalized recommendations in a manner similar to human experts [23,34].'],\n",
       "   [\"Thus, AI recommendation services offer a self-relevant recommendation to consumers [8,23,34], whereas the non-AI recommendation system may provide irrelevant information to consumers' tastes.\"],\n",
       "   [\"Thus, AI recommendation services offer a self-relevant recommendation to consumers [8,23,34], whereas the non-AI recommendation system may provide irrelevant information to consumers' tastes.\"]],\n",
       "  [['Also participants in the LifeCLEF challenges in 2018 (Joly et al., 2018) and 2020 (Joly et al.']]],\n",
       " [],\n",
       " [[['The tasks are: multilingual Information extraction; technologically assisted reviews in empirical medicine; and, patient-centred information retrieval [15].']],\n",
       "  [['Rece‚ô™tly the CLEF eHealth track o‚ô™ Tech‚ô™ology Assisted Reviews i‚ô™ Empirical ‚Ñßedici‚ô™e [9,20] developed datasets co‚ô™tai‚ô™i‚ô™g 72 topics created from diag‚ô™ostic test accuracy systematic reviews produced by the Cochra‚ô™e Col-laboratio‚ô™.'],\n",
       "   ['[5,9,20].']],\n",
       "  [['We consult multiple patient or health-related lexicons such as MedDRA6 , CLEF Consumer Health track [3], Reddit etc.']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].'],\n",
       "   ['The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 The 2020 CLEF eHealth Task 1 on IE, called CodiEsp supported by the Spanish National Plan for the Advancement of Language Technology (Plan TL), builds upon the five previous editions of the task in 2015-2019 [4,5,8,10,16] that have already addressed the analysis of biomedical text in English, French, Hungarian, Italian, and German.']],\n",
       "  [['2017;Suominen et al. 2018].'], ['2017;Suominen et al. 2018].']],\n",
       "  [['Three datasets have been released-namely, EMED 2017 Three datasets have been released-namely, EMED 2017 [20], EMED 2018 [21,34], and EMED 2019 [22].']],\n",
       "  [['Three datasets have been released namely EMED 2017 Three datasets have been released namely EMED 2017 [81], EMED 2018 [83,154] and EMED 2019 [85].']],\n",
       "  [['For example, for the 2018 Conference and Labs of the Evaluation Forum eHealth task 1 challenge [19], the objective of which was to extract ICD 10th Revision codes from the death certificates provided by the Centre for Epidemiology of Medical Causes of Death, Cossin et al [20] tested an approach based on ontologies, whereas Flicoteaux et al [21] proposed an approach using a probabilistic convolutional neural network (CNN), and Ive et al [22] resorted to the association of a recurrent neural network with a CNN.']]],\n",
       " [],\n",
       " [[['Medical tasks started in 2004 and have in some years been the majority of the tasks in ImageCLEF [14].']],\n",
       "  [['This is best exemplified by the test collection methodology employed by large-scale international efforts, such as TREC 10) , CLEF 11) , NTCIR 12) and in the multimedia field, efforts such as ImageCLEF 13) or MediaEval 14) .']],\n",
       "  [['For the computer assisted analysis, it was noted that no solution has sufficient prediction accuracy (10)(11)(12)(13)(14).']],\n",
       "  [[', 2018) and CLEF18 (Ionescu et al., 2018) medical VQA datasets, which are different from the existing VQA datasets.']],\n",
       "  [['Several fully automated solutions were presented as part of the ImageCLEF 2017 and 2018 evaluation challenge forums Several fully automated solutions were presented as part of the ImageCLEF 2017 and 2018 evaluation challenge forums [10].']],\n",
       "  [['The ImageCLEF 2018 Lifelog task [Ionescu et al., 2018] has two sub-tasks: lifelog moment retrieval (retrieving specific moments, e.']]],\n",
       " [[['The main aim is to enable research groups working on PIR to both experiment with and provide feedback on the proposed PIR evaluation methodology [13].']]],\n",
       " [[['org) [10][11][12], where training data is released.']],\n",
       "  [['In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included [77,78].'],\n",
       "   ['In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included In order to incorporate psycholinguistics into prediction models, the LIWC is almost always employed as a feature extractor, in most cases, all 104 constructs are included [77,78].'],\n",
       "   ['Notable features that are repeatedly included are reading scores [8,78], part of speech tagging [80], occasions of drug word usage [81] and sentiment or valence measures [78].'],\n",
       "   ['Notable features that are repeatedly included are reading scores [8,78], part of speech tagging [80], occasions of drug word usage [81] and sentiment or valence measures [78].'],\n",
       "   ['The majority of these approaches employ unsupervised learning [77,78].'],\n",
       "   ['As per the general trend in machine learning, the most recent publications often make use of deep learning architecture [76,78,80].']],\n",
       "  [[\"Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection [11,12] and eRisk 2018 early anorexia detection [12].\"],\n",
       "   [\"Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection Experiments were conducted on three of the CLEF's eRisk open tasks, namely eRisk 2017 and 2018 early depression detection [11,12] and eRisk 2018 early anorexia detection [12].\"],\n",
       "   ['As it is described in more detail in the overview of each task As it is described in more detail in the overview of each task [11,12] and the CLEF Working Notes, 8 a total of 180 models were submitted to these three eRisk tasks, ranging from simple to more advanced deep learning models.']],\n",
       "  [['In this respect, the Early Risk Prediction on the Internet (eRisk) [14,15], as well as the Computational Linguistics and Clinical Psychology (CLPsych) [9] workshops were the first to propose benchmarks to bring together many researchers to address the automatic detection of mental disorders in online social media.'],\n",
       "   ['Here, we study various collections released at different editions of the eRisk workshop Here, we study various collections released at different editions of the eRisk workshop [14,15].']],\n",
       "  [['eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection [4][5][6].'],\n",
       "   [\"A full description of this ranking-based evaluation approach can be found in the eRisk 2019's overview report [6].\"],\n",
       "   ['full description and analysis of the results can be found in the lab overviews full description and analysis of the results can be found in the lab overviews [4][5][6] and working note proceedings.']],\n",
       "  [['This model was chosen because it achieved comparable results to the best performing model at the recent eRisk shared task [17,22], and is based on an end-to-end architecture, which makes the reasoning behind its decision more easily explainable.'],\n",
       "   ['The dataset used is from the first sub-task of the eRisk 2019 shared task The dataset used is from the first sub-task of the eRisk 2019 shared task [17], whose focus is the early risk detection of anorexia.'],\n",
       "   ['[17], the dataset was collected following the extraction and annotation method, proposed by Coppersmith et al.'],\n",
       "   ['In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.']],\n",
       "  [['One of the few exceptions are the datasets developed under the CLEF eRisk challenge [18,[26][27][28][29].'],\n",
       "   ['The interactions between SM activity and psychological traces have been the focus of attention of popular evaluation campaigns, such as the CLPsych shared tasks [32,33], organised within the Computational Linguistics and Clinical Psychology Workshop, or the eRisk tasks [26][27][28], oriented to early detection of signs of psychological problems and organised within the Conference and Labs of the Evaluation Forum (CLEF).']],\n",
       "  [['The task and data used in this study are based on the CLEF Lab eRisk task 2 The task and data used in this study are based on the CLEF Lab eRisk task 2 [16].'],\n",
       "   ['More details about the tasks can be found in in [16].'],\n",
       "   ['As the problem is to detect as early as possible the sign of mental illnesses, a new measure named ERDE was defined in As the problem is to detect as early as possible the sign of mental illnesses, a new measure named ERDE was defined in [16].'],\n",
       "   ['There are also several evaluation frameworks related to social media analysis for mental illness detection such as eRisk [16] and CLPsych [17].'],\n",
       "   [\"Other participants' results are details in [16].\"]],\n",
       "  [['Automated methods have been designed to detect signs of AN, some of which address the development of early detection approaches Automated methods have been designed to detect signs of AN, some of which address the development of early detection approaches [6,7], as it has been proven that the signs and symptoms of mental disorders, including AN, can be traced using social media [6,[8][9][10][11][12][13].'],\n",
       "   ['They analyze the topics of interest using topic modeling techniques [11], and they also consider the frequency of the terms used through bag-of-words (BoW) models and n-grams [7].'],\n",
       "   ['In addition, sentiment analysis tools [15] and methods that use word embeddings, which are vector representations of terms, have been applied [7,16].'],\n",
       "   ['In addition, sentiment analysis tools [15] and methods that use word embeddings, which are vector representations of terms, have been applied [7,16].'],\n",
       "   ['; and (7) Are there significant differences between the focused and random control groups?']],\n",
       "  [[', 2019(Losada et al., , 2020)).']],\n",
       "  [['As a few examples of this type of research, we can mention dementia identification [28,29], depression detection [27,30,31], crisis counselling [32], suicide risks identification [31,33,34], mental illnesses classification [35,36], anxiety detection [37], personality traits identification [38,39], etc.']],\n",
       "  [['Accordingly, recent evaluation forums have focused on detecting people suffering from depression through the analysis of their social media posts [5][6][7].'],\n",
       "   ['Recently, due to the relevance of the problem, some evaluation forums such as eRisk 3 have motivated the development of computational approaches to face the early detection of social media users suffering from depression [5,6].'],\n",
       "   ['For evaluating the proposed approach, we used two benchmark datasets in English: a collection from Reddit users released in the context of the eRisk 2018 task For evaluating the proposed approach, we used two benchmark datasets in English: a collection from Reddit users released in the context of the eRisk 2018 task [5] (hereafter denoted as Reddit), and a collection of Twitter users described in [50] (hereafter denoted as Twitter).'],\n",
       "   ['The search was focused on: (i) the number of trees (10,20,30,40,50) and (ii) their depth (3,5,6,9,10).']],\n",
       "  [['Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 [23,40].']],\n",
       "  [[', , 2020Losada et al., , 2021)).'],\n",
       "   ['Here the model was built by training it using the given training corpus and a similar data released as part of the second shared task of eRisk 2021 Here the model was built by training it using the given training corpus and a similar data released as part of the second shared task of eRisk 2021 (Losada et al., 2021).'],\n",
       "   ['Here the Doc2Vec based model performs poorly as it was trained on the given training corpus and the corpus released as part of eRisk 2021 sharedtask for prediction of self-harm over social media Here the Doc2Vec based model performs poorly as it was trained on the given training corpus and the corpus released as part of eRisk 2021 sharedtask for prediction of self-harm over social media (Losada et al., 2021), which are reasonably small in size.']],\n",
       "  [['(2013) and Losada, Crestani, and Parapar (2018) focus on early detection of depression.']],\n",
       "  [[', 2020;Parapar et al., 2022).']],\n",
       "  [[', 2017(Losada et al., , 2018) ) in our experiments.'],\n",
       "   [', 2017)) and the test set (from the eRisk 2018 task(Losada et al., 2018)).']],\n",
       "  [[', 2017Losada et al., , 2018) ) where users have self-disclosed their mental health diagnosis.'],\n",
       "   [', 2017(Losada et al., , 2018) ) dataset.'],\n",
       "   [', 2017(Losada et al., , 2018) ) in our experiments.'],\n",
       "   [', 2017)) and the test set (from the eRisk 2018 task (Losada et al., 2018)).']],\n",
       "  [['e-Risk evaluation datasets: We utilize datasets from the e-Risk(Losada and Crestani, 2016) evaluation for anorexia e-Risk evaluation datasets: We utilize datasets from the e-Risk(Losada and Crestani, 2016) evaluation for anorexia (Losada et al., 2019), depression (Losada et al.']]],\n",
       " [[['Effective \"querying agents\" can then simulate users towards developing dynamic search systems [10].']],\n",
       "  [['During the whole search session, users provide one query to the search engine, and provide the feedback after each iteration [13].']],\n",
       "  [['Recently, the TREC Dynamic Domain Track (2015-2017) [45], TREC Tasks track (2015-2017) [22] and the CLEF Dynamic Search Lab (2017-2018) [21] have also brought significant benefits to the research progress in this area.']],\n",
       "  [['These efforts include the TREC Interactive Tracks from 1997 to 2002 [37,92,124], TREC Session Tracks from 2011 to 2014 [23][24][25][57][58][59], TREC Dynamic Domain Tracks from 2015 to 2017 [140][141][142], CLEF Dynamic Search Lab in 2018 [56] and the recent TREC Conversational Assistant (CAsT) Track [32] starting from 2019.'],\n",
       "   ['The CLEF Dynamic Search Lab in 2018 The CLEF Dynamic Search Lab in 2018 [56] is similar to and has further developed the TREC DD Tracks.'],\n",
       "   ['CLEF Dynamic Search Lab [56] evaluates reformulated queries indirectly by measuring how relevant those queries retrieve the documents.']]],\n",
       " [[['In the recent CLEF 2018 competition on check-worthiness detection [17], Zou et al.'],\n",
       "   ['Out of these 7 speeches, 4 are by Donald Trump and are made available by the CLEF 2018 lab on automatic identification and verification of political claims [17].'],\n",
       "   ['2016 presidential election (following the same setting as in the official CLEF 2018 competition on check-worthiness detection [17] but using even more data).']],\n",
       "  [['20194 is a continuation of the evaluation lab at CLEF-2018 [12].']],\n",
       "  [['The Fact-checking Lab at CLEF [3,9] looks at this problem by defining the task of ranking sentences according to their need to be fact-checked.']],\n",
       "  [['2 Whereas the 2019 edition [9,10] also focused on political debates, isolated claims were considered as well, in conjunction with a closed set of Web documents to retrieve evidence from.'],\n",
       "   ['The 2019 edition featured two tasks The 2019 edition featured two tasks [10]: Task 1 2019 .']],\n",
       "  [[\"As outlined in Atanasova et al. (2018), of a total of seven models compared, the most successful approaches used by the participants relied on recurrent and multi-layer neural networks, as well as combinations of distributional representations, matching claims' vocabulary against lexicons, and measures of syntactic dependency.\"]],\n",
       "  [['lab on identification and verification of claims (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [['Recent work has focused on the automatic classification of truthfulness or fact checking Recent work has focused on the automatic classification of truthfulness or fact checking [2,9,15,20,22,24].'],\n",
       "   ['CLEF developed a Fact-Checking Lab [9,22] to address the issue of ranking sentences according to some fact-checking property.']],\n",
       "  [[\"labs' shared tasks (Nakov et al., 2018;Elsayed et al.\"]],\n",
       "  [['2018;Nakov et al. 2018].']],\n",
       "  [['lab (Nakov et al., 2018;Elsayed et al.']],\n",
       "  [['The 2019 edition covered the various modules necessary to verify a claim: from check-worthiness, to ranking and classification of evidence in the form of Web pages, to actual fact-checking of claims against specific text snippets [24,25].']],\n",
       "  [['lab has focused on developing technology to assist the journalist fact-checker during the main steps of verification [7,8,18,19,[47][48][49]51,52].']],\n",
       "  [[', 2020;Atanasova et al., 2018;Barron-Cedeno et al.']]],\n",
       " [[[', 1999) and CLEF (Braschler, 2001).']],\n",
       "  [['We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign [3][4][5][6] for bilingual ad-hoc retrieval tracks 6 .']],\n",
       "  [['We create our training and evaluation data from the Cross-Language Evaluation Forum (CLEF) 2000-2008 campaign for bilingual ad-hoc retrieval tracks [3][4][5][6][27][28][29][30][31].']],\n",
       "  [['Many long-standing evaluation campaigns like TREC, NTCIR, CLEF, or FIRE [15,42,47,56] trace their roots back to the Cranfield paradigm [20], which relies on test collections that consist of (i) a document corpus, (ii) a set of information needs or topics, and (iii) relevance judgments for documents on the topics.']]],\n",
       " [[['More recently, the ARQMath labs at CLEF 2020 and 2021 have had a formula retrieval task [6,7].']]],\n",
       " [[['BioASQ [29] is similar to the SQuAQ dataset [4], where a span of text in the given context is used as the answer and thus no external knowledge source is needed.']],\n",
       "  [['In the corpus developed for the BioASQ MESINESP Task [4,10], 44% records (108,945 out of 250,539) had at least one disease-related MeSH descriptor, with disease-related MeSH descriptors being 58% for the clinical trials subset.']],\n",
       "  [[', 2020) and medical question answering (Demner-Fushman and Lin, 2006;Nentidis et al., 2021).']],\n",
       "  [[', 2021) and BIOASQ (Nentidis et al., 2021) -several major threads of research have emerged.']],\n",
       "  [[', 2015;Nentidis et al., 2021) comprises biomedical articles from PubMed,7 annotated with concepts from the Medical Subject Headings (MeSH) taxonomy.']],\n",
       "  [['2019(Nentidis et al. , 2020(Nentidis et al.'],\n",
       "   ['2019(Nentidis et al. , 2020(Nentidis et al.']],\n",
       "  [[', 2015;Nentidis et al., 2021).']],\n",
       "  [[', 2019), BioASQ (Nentidis et al., 2021), and QASPER (Dasigi et al.']]],\n",
       " [[[', 2020;Nakov et al., 2021) of claims in political debates and social media.']],\n",
       "  [['This has been most commonly used in shared tasks [92].']],\n",
       "  [[', 2020), and social media claims (Nakov et al., 2021).']],\n",
       "  [['Similarly, the 2021 edition focused on detecting check-worthy claims, previously fact-checked claims, and fake news [56,57].']],\n",
       "  [['To address the issue of imbalance classes, we included some examples from the Zenodo dataset, which is a COVID-19 false news dataset [41].']],\n",
       "  [['In addition, shared tasks has also been organized in the past years addressing factuality, fake news and harmful content (Nakov et al., 2021;Kiela et al.'],\n",
       "   ['This has been most commonly used in shared tasks (Nakov et al., 2021).']],\n",
       "  [[', 2021), human fact checkers use a wider range of modalities (Nakov et al., 2021).']],\n",
       "  [['lab has focused on developing technology to assist the journalist fact-checker during the main steps of verification [7,8,18,19,[47][48][49]51,52].']],\n",
       "  [[', 2020;Nakov et al., 2021;Shaar et al.'],\n",
       "   [', 2020;Nakov et al., 2021;Shaar et al.']],\n",
       "  [['[22] due to the continual, ongoing evolution of misinformation requiring the continual retraining of models.']]],\n",
       " [],\n",
       " [[[', the search for health information by people without special medical expertise (Suominen et al., 2021).'],\n",
       "   [', 2021;Suominen et al., 2021;Upadhyay et al.']],\n",
       "  [['uRPB wurde in [64] eingef√ºhrt, w√§hrend cRPB zum ersten Mal (basierend auf uRBP) in der 2020er Ausgabe des CLEF eHealth [2] auftaucht.']]],\n",
       " [[['In this respect, the Early Risk Prediction on the Internet (eRisk) [14,15], as well as the Computational Linguistics and Clinical Psychology (CLPsych) [9] workshops were the first to propose benchmarks to bring together many researchers to address the automatic detection of mental disorders in online social media.'],\n",
       "   ['Here, we study various collections released at different editions of the eRisk workshop Here, we study various collections released at different editions of the eRisk workshop [14,15].']],\n",
       "  [['eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection eRisk is a CLEF lab whose main goal is to explore issues of evaluation methodology, performance metrics and other challenges related to building testbeds for early risk detection [4][5][6].'],\n",
       "   [\"A full description of this ranking-based evaluation approach can be found in the eRisk 2019's overview report [6].\"],\n",
       "   ['full description and analysis of the results can be found in the lab overviews full description and analysis of the results can be found in the lab overviews [4][5][6] and working note proceedings.']],\n",
       "  [['This model was chosen because it achieved comparable results to the best performing model at the recent eRisk shared task [17,22], and is based on an end-to-end architecture, which makes the reasoning behind its decision more easily explainable.'],\n",
       "   ['The dataset used is from the first sub-task of the eRisk 2019 shared task The dataset used is from the first sub-task of the eRisk 2019 shared task [17], whose focus is the early risk detection of anorexia.'],\n",
       "   ['[17], the dataset was collected following the extraction and annotation method, proposed by Coppersmith et al.'],\n",
       "   ['In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.']],\n",
       "  [['One of the few exceptions are the datasets developed under the CLEF eRisk challenge [18,[26][27][28][29].'],\n",
       "   ['The interactions between SM activity and psychological traces have been the focus of attention of popular evaluation campaigns, such as the CLPsych shared tasks [32,33], organised within the Computational Linguistics and Clinical Psychology Workshop, or the eRisk tasks [26][27][28], oriented to early detection of signs of psychological problems and organised within the Conference and Labs of the Evaluation Forum (CLEF).']],\n",
       "  [['Presently eRisk encompasses the groups interested in this type of challenge, that of presenting computer models that can early identify users with symptoms of depression and anorexia [16].'],\n",
       "   ['The data collection provided by the eRisk 2019 challenge The data collection provided by the eRisk 2019 challenge [16] has served as a starting point for the scientific community interested in anorexia detection using NLP methods.']],\n",
       "  [[', 2019(Losada et al., , 2020)).']],\n",
       "  [['Note that we, following Losada et al. (2019), compute the speed of warnings only for grooming chats classified as such.'],\n",
       "   [', 2018;Losada et al., 2019), p is set such that the penalty is 0.']],\n",
       "  [['CLEF eRISK 2019 is about the severity of symptoms of depression, self-injury, and anorexia [28].']],\n",
       "  [['In this study, we use the datasets provided by eRisk 2019 and 2020 for the task \"Measuring the Severity of the Signs of Depression\" In this study, we use the datasets provided by eRisk 2019 and 2020 for the task \"Measuring the Severity of the Signs of Depression\" [5,6].']],\n",
       "  [['Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 Both LOSADA databases are databases that were available in the eRisk lab of the CLEF in the years of 2017 and 2018 [23,40].']],\n",
       "  [[', , 2020Losada et al., , 2021)).'],\n",
       "   ['Here the model was built by training it using the given training corpus and a similar data released as part of the second shared task of eRisk 2021 Here the model was built by training it using the given training corpus and a similar data released as part of the second shared task of eRisk 2021 (Losada et al., 2021).'],\n",
       "   ['Here the Doc2Vec based model performs poorly as it was trained on the given training corpus and the corpus released as part of eRisk 2021 sharedtask for prediction of self-harm over social media Here the Doc2Vec based model performs poorly as it was trained on the given training corpus and the corpus released as part of eRisk 2021 sharedtask for prediction of self-harm over social media (Losada et al., 2021), which are reasonably small in size.']],\n",
       "  [['For instance, many systems in an ERD competition, eRisk2019, spent several days for computation [Losada et al., 2019].'],\n",
       "   ['In practice, we also want to identify depression risk as early as possible, as is exemplified by the eRisk competitions In practice, we also want to identify depression risk as early as possible, as is exemplified by the eRisk competitions [Losada et al., 2019].']],\n",
       "  [['The posts were annotated in the context of other posts from timelines as carried out in the CLEF eRisk 2020 dataset (Losada and Crestani, 2016;Losada et al., 2020).']],\n",
       "  [[', 2019a;Losada et al., 2020).'],\n",
       "   [\"The posts from Reddit's mental health-related subreddits in a given time window (timeline) (Losada et al., 2020;Losada and Crestani, 2016;Zirikly et al.\"]],\n",
       "  [['Other initiatives have sought to predict from Twitter and Reddit posts whether people are depressed, suffering from anorexia, or likely to self-harm [46].'],\n",
       "   ['Other initiatives have sought to predict from Twitter and Reddit posts whether people are depressed, suffering from anorexia, or likely to self-harm [46].']],\n",
       "  [['A base de dados considerada neste trabalho √© a eRisk2021 A base de dados considerada neste trabalho √© a eRisk2021 [Parapar et al. 2021], que cont√©m publicac ¬∏√µes do Reddit anotadas com a classe depressiva ou n√£o (classificac ¬∏√£o bin√°ria).']],\n",
       "  [[', 2020;Parapar et al., 2022).']],\n",
       "  [['The depression severity identification task is becoming increasingly attractive due to the practical clinical implications, so some studies have emerged (Losada et al., 2019;Kayalvizhi et al.']],\n",
       "  [[\"2018] e eRisk [Losada and Crestani 2016], sendo este √∫ltimo tamb√©m a base de uma s√©rie de desafios computacionais (ou 'shared tasks') Early Risk Prediction on the Internet [Parapar et al. 2022].\"],\n",
       "   ['2019, Parapar et al. 2022] e outros, o objetivo √© distinguir indiv√≠duos depressivos da populac ¬∏√£o em geral, e n√£o distinguir indiv√≠duos depressivos de n√£o-depressivos.']],\n",
       "  [['This requires the development of agile and adaptive models capable of monitoring and responding to shifts in online mental health expressions in real-time [68], [107], [133].']],\n",
       "  [['e-Risk evaluation datasets: We utilize datasets from the e-Risk(Losada and Crestani, 2016) evaluation for anorexia e-Risk evaluation datasets: We utilize datasets from the e-Risk(Losada and Crestani, 2016) evaluation for anorexia (Losada et al., 2019), depression (Losada et al.']],\n",
       "  [['In Task 1 of the 2023 challenge [5], various teams employed vector representations of sentences and leveraged either semantic search with transformer-based models or cosine similarity to categorize documents according to the 21 symptoms of depression [6,7].']]],\n",
       " [[['Motivated by this, we extend our proposed model Motivated by this, we extend our proposed model [9] for the ImageCLEFmedical 2021 [10], [11] by adding an explainability module.'],\n",
       "   ['We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset [10], [11], which includes three sets: the training set composed of 2756 medical images; the validation set and the test set consisting of 500 and 444 radiology images, respectively.']],\n",
       "  [['To evaluate our proposed method, we employed the ImageCLEFmed 2021 dataset To evaluate our proposed method, we employed the ImageCLEFmed 2021 dataset [22,23], composed of three subsets for training, validation, and testing.']],\n",
       "  [['Studies have used MIMIC-CXR Studies have used MIMIC-CXR [18], Open-I [19], MS-COCO [20], and ImageCLEF [21] datasets, which are publicly available training datasets for various types of medical images, as well as chest X-ray images.']],\n",
       "  [['\" [Ion+21] or \"Is there gastric fullness?'],\n",
       "   ['The model is evaluated on VQA-Med 2019 [Aba+19], VQA-Med 2021 [Ion+21], VQA-RAD [Lau+18], and SLAKE [Liu+21a] datasets (see Table 3).'],\n",
       "   ['VQA-Med 2021 dataset contains 5, 500 radiology images obtained from MedPix, an open-access online medical image database, and 5, 500 QA pairs[Ion+21].']]],\n",
       " [[['Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].']],\n",
       "  [[', 2021;Joly et al., 2021;Reedha et al.']]],\n",
       " [],\n",
       " [],\n",
       " [[[\"The track's setup is based on the following pipeline: (1) select the information to be included in a simplified summary; (2) decide whether the selected information is sufficient and comprehensible or provide some background knowledge if not; (3) improve the readability of the text The track's setup is based on the following pipeline: (1) select the information to be included in a simplified summary; (2) decide whether the selected information is sufficient and comprehensible or provide some background knowledge if not; (3) improve the readability of the text [7].\"],\n",
       "   ['Our training data is a truly parallel corpus of directly simplified sentences (648 sentences for now) coming from scientific abstracts from the DBLP Citation Network Dataset for Computer Science and Google Scholar and PubMed articles on Health and Medicine [7,8,11,10].'],\n",
       "   ['We distinguish the following types of information distortion with corresponding severity level: Style (1); Insertion of unnecessary details with regard to a query (1); Redundancy (without lexical overlap) (2); Insertion of false or unsupported information (3); Omission of essential details with regard to a query (4); Overgeneralization (5); Oversimplification(5); Topic shift (5); Contra sense / contradiction (6); Ambiguity (6); Nonsense (7).']]],\n",
       " [[[', 2020b(Bondarenko et al., , 2021) ) featured a related track.']],\n",
       "  [[', 2019;Bondarenko et al., 2021).']],\n",
       "  [[', underlying reasons -sometimes called perspectives [18,21], premises [13,26] or frames [3,47]).']],\n",
       "  [[', 2017b;Bondarenko et al., 2021), argument analysis (Feng and Hirst, 2011;Janier et al.']],\n",
       "  [[', 2019;Bondarenko et al., 2021) or a suitable counter-argument given an input argument (Wachsmuth et al.']],\n",
       "  [[', 2018), during argument retrieval, (Bondarenko et al., 2021) or in the legal context for case law retrieval (Locke and Zuccon, 2018).'],\n",
       "   [', 2018), and Webis-Touch√© (Bondarenko et al., 2021).'],\n",
       "   ['Webis-Touch√© 2020 Webis-Touch√© 2020 (Bondarenko et al., 2021) is an argument retrieval dataset based on the args.']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']],\n",
       "  [['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.'],\n",
       "   ['me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.']]],\n",
       " [[['In the following we consider four public experimental collections, whose characteristics are reported in Table In the following we consider four public experimental collections, whose characteristics are reported in Table 3: (i) CLEF 2003, Multilingual-4, Ad-Hoc Track [1]; (ii) TREC 13, 2004, Robust Track [15]; (iii) CLEF 2009, bilingual X2EN, The European Library (TEL) Track [7]; and, (iv) TREC 21, 2012, Web Track [6].']],\n",
       "  [['We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign We performed experiments on the Cross-Language Evaluation Forum (CLEF) 2000-2003 campaign [3][4][5][6] for bilingual ad-hoc retrieval tracks 6 .']],\n",
       "  [['CLEF also hosted multiple cross-lingual ad-hoc retrieval tasks from 2000 to 2009 [3].'],\n",
       "   ['The CLEF initiative includes some non-English monolingual datasets, though these are primarily focused on European languages [3].']],\n",
       "  [['We create our training and evaluation data from the Cross-Language Evaluation Forum (CLEF) 2000-2008 campaign for bilingual ad-hoc retrieval tracks [3][4][5][6][27][28][29][30][31].']],\n",
       "  [['[17]  4 ; CLEF03 with German, French, Spanish, and English [5]; and Neu-CLIR 2022 [26] and 2023 [27].']]],\n",
       " [],\n",
       " [[['The work described in this paper provides a complementary capability to that provided by crosslingual question answering systems, as described, for example, in The work described in this paper provides a complementary capability to that provided by crosslingual question answering systems, as described, for example, in [4], [5] and [6].']],\n",
       "  [['2006;L√∂√∂f, Gollan, and Ney 2009), information retrieval (Nie 2010; Grefenstette 2012), question answering (Magnini et al. 2004;Neumann and Sacaleanu 2005), and plagiarism detection (Potthast et al.']],\n",
       "  [['More information about multi lingua system can be found in Magnini et al. (2003).'],\n",
       "   ['system developed by The Health on the Net Foundation supports English, French and Italian, while DIOGENE system supports only two languages: English and Italian, but in contrast to cross-lingual systems the question are asked in either Italian or English, retrieval of information is carried out in Italian or English, and the answer is given in the language of the query (Magnini et al., 2003).']],\n",
       "  [['Multilingual QA Much recent effort has been made to create non-English QA datasets to over- Our XOR-TYDI QA is also closely related to QA@CLEF 2003-2008 Multilingual QA Much recent effort has been made to create non-English QA datasets to over- Our XOR-TYDI QA is also closely related to QA@CLEF 2003-2008 (Magnini et al., 2003(Magnini et al.']]],\n",
       " [[[\"Enfin, les r√©sultats de 2005 soulignent le besoin d'offrir une interface donnant plus de contr√¥le √† l'individu pour la formulation et la reformulation des requ√™tes (Clough et al, 2005).\"]],\n",
       "  [['The test collection ImageCLEFMed 2005 contains Casimage (9000 images), MIR (2000 images), PEIR (33000 images), and PathoPic (9000 images) (Clough et al., 2005).']],\n",
       "  [[\"Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval (Clough, 2003), confirming Flank's conclusion that image CLIR can be made to work.\"],\n",
       "   ['As part of preparations for the formation of the imageCLEF collection As part of preparations for the formation of the imageCLEF collection (Clough and Sanderson, 2003), a preliminary evaluation of image CLIR was conducted on the St.']],\n",
       "  [['The IRMA 10000 databaseThe IRMA 10000 database1 was used in the automatic annotation task of the 2005 ImageCLEF evaluation [17].'],\n",
       "   ['Table 1 gives an overview of the best results obtained for the IRMA tasks from the ImageCLEF 2005 evaluation [17] along with the results we obtained using sparse patch histograms with and without position information.']],\n",
       "  [['The St Andrews collection has been used for the past three years at ImageCLEFThe St Andrews collection has been used for the past three years at ImageCLEF5 , the cross-language image retrieval task (Clough, M√ºller, Hersh, Deselaers, Lehmann, Grubinger, 2005;Clough, M√ºller, Sanderson, 2005;Clough and Sanderson, 2003).']],\n",
       "  [['Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.']],\n",
       "  [['ImageCLEF is another image benchmarking competition that is part of the Cross Language Evaluation Framework (CLEF) competition (Clough et al. 2004).']],\n",
       "  [['Another recent example is where users complement their traditional keyword query with additional information, such as example documents [24], tags [73], images [76,91], categories [338], or their search history [20].']],\n",
       "  [['Accordingly, this technique is less time-consuming compared to the technique that depends on texts for the purposes of indexing and retrieving [5].']],\n",
       "  [['The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].']],\n",
       "  [['The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.']],\n",
       "  [['Spreading from textual search evaluations [25], evaluation campaigns have been established for image retrieval [9], as well as for video content search [69].']]],\n",
       " [[['Research on SCR initially investigated IR for planned speech content such as news broadcasts and documentaries [1], [2].']],\n",
       "  [[', news [8], or TED talks [9,16]).']]],\n",
       " [],\n",
       " [[['A further related corpus was released as part of the ShARe/CLEF 2013 NER task [32].'],\n",
       "   ['The portability of the models trained on PhenoCHF was furthermore demonstrated through their application to the corpus released for the NER task of ShARe/CLEF 2013 The portability of the models trained on PhenoCHF was furthermore demonstrated through their application to the corpus released for the NER task of ShARe/CLEF 2013 [32], whose annotations partially overlap with those in PhenoCHF.']],\n",
       "  [['CLEF 2013 [40] started a task for acronym/abbreviation normalization, with a focus on mapping acronyms and abbreviations to concepts in the Unified Medical Language System (UMLS) [41].']],\n",
       "  [['Following recommendations such as those for the evaluation campaign CLEF eHealth [13], we considered that if an anatomical concept was part of a disease, the query should not be labelled as anatomical.']],\n",
       "  [['Specific mentions of diseases and signs or symptoms were similarly annotated under the ShARe scheme [16,17] and additionally linked to terms in the SNOMED Clinical Terms vocabulary [18].']],\n",
       "  [['Textual annotations used the ShARE/CLEF 2013 corpus [18] which we describe later.']],\n",
       "  [['It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation (Suominen et al., 2013;Pradhan et al.']],\n",
       "  [[\"In the framework of CLEF eHealth 2014 [12], one of the proposed shared tasks (task 3) addresses this challenge [8]: queries have been defined from real patient cases issued from the clinical documents provided by the CLEF eHealth 2014's task 2.\"]],\n",
       "  [['CLEF 2013 (Suominen et al., 2013) started a task for acronym/abbreviation normalization, using the UMLS 8 as target terminology.']],\n",
       "  [['Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words (Chapman et al., 2013).'],\n",
       "   ['For entity encoding, all participating systems were evaluated using accuracy, in \"strict\" and \"relaxed\" modes, as defined in (Suominen et al., 2013).']],\n",
       "  [['‚Ä¢ The annotated corpora from the ShARe/CLEF eHealth Evaluation Lab ‚Ä¢ The annotated corpora from the ShARe/CLEF eHealth Evaluation Lab (Suominen et al., 2013).']],\n",
       "  [['The more recent 2013 CLEF-eHEALTH challenge (Suominen et al., 2013) corpus consists of EHRs annotated with named entities referring to disorders and acronyms/abbreviations, mapped to UMLS concept identifiers.']],\n",
       "  [['2014;Kelly et al. 2014).'],\n",
       "   ['As evaluation set we use the training and test collections from CLEF eHealth task 3a As evaluation set we use the training and test collections from CLEF eHealth task 3a (Kelly et al. 2014): the CLEF document collection and five train ?'],\n",
       "   ['As evaluation set we use the training and test collections from CLEF eHealth task 3a As evaluation set we use the training and test collections from CLEF eHealth task 3a (Kelly et al. 2014): the CLEF document collection and five train ?']],\n",
       "  [[', i2b2 [5,7,43,44] and the ShARe/CLEF eHealth Evaluation Labs [45][46][47].'],\n",
       "   [', i2b2 [5,7,43,44] and the ShARe/CLEF eHealth Evaluation Labs [45][46][47].'],\n",
       "   ['However, the increasing emergence of annotated corpora that include gold-standard links between entity mentions and concepts in terminological resources [33,34,45] has stimulated a large amount of research into the development of dedicated normalisation methods for both biomedical scientific text and narrative clinical text.']],\n",
       "  [['Moreover, many biomedical shared tasks are devoted to disorder recognition, including i2b2 2010 15 , ShARe/ CLEF eHealth task 2013 16 , SemEval 2014 task 7 17 , and SemEval 2015 task 14 18 .']],\n",
       "  [['In the past several years, lots of machine learning-based clinical entity recognition systems have been proposed, may due to some publicly available corpora provided by organizers of some shared tasks, such as the Center for Informatics for Integrating Biology & the Beside (i2b2) 2009 [8], 2010 [9][10][11][12][13], 2012 [14][15][16][17][18] and 2014 track1 [19][20][21][22][23] datasets, ShARe/CLEF eHealth Evaluation Lab (SHEL) 2013 dataset [24], and SemEval (Semantic Evaluation) 2014 task 7 [25], 2015 task 6 [26] 2015 task 14 [27], and 2016 task 12 [28] datasets.']],\n",
       "  [['These results are consistent with previous investigations [7] on i2b2/VA 2010 and ShARe/CLEF 2013 [33] datasets.']],\n",
       "  [['Using the ShARe corpus, participants in the 2014 ShARe/CLEF Task 2 were tasked with normalizing semantic modifiers related to disease mentions in clinical texts [35].']],\n",
       "  [[', 2011) and ShARe/CLEF 2013 (Suominen et al., 2013).'],\n",
       "   [', 2011), the TREC Medical Records Track collection (Voorhees & Tong, 2011), and the ShARe/CLEF 2013 train set (Suominen et al., 2013) (for further technical details see (De Vine et al.']],\n",
       "  [['Communitywide shared task competitions have relied on such corpora to advance the start-of-the-art in biomedical NLP [19,[32][33][34][35].']],\n",
       "  [['2010), to help patients better understand their diseases and treatments (Suominen et al. 2013), etc.'],\n",
       "   ['2009, Suominen et al. 2013).'],\n",
       "   ['Specifically, we used the 299 clinical records annotated with disorder mentions normalised to concept unique identifiers (CUIs) in SNOMED-CT, released in the context of the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al. 2013), and the 1,500 PubMed abstracts annotated with disease mentions normalised to MeSH IDs, created for the BioCreative V Chemical Disease Relation (CDR) Task (Li et al.']],\n",
       "  [['The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure The topic of patient-friendly multilingual communication formed the focus of CLEF eHealth from 2012 to 2017 and generated a total scholarly influence of 962,559 citations (and scholarly impact of 1299 citations) for the 184 CLEF eHealth papers and reached 741 authors from 33 countries across the world (Multimedia Appendix 3, Figure 2) [17][18][19][20][21][22].']],\n",
       "  [[', [44,45,54,55]).'],\n",
       "   [', [54,55,110,111]).'],\n",
       "   [', [54,110]), thus helping to demonstrate the utility of the methods.'],\n",
       "   [', the ShaRE/CLEF corpus [54] (see  14) in which disorders are normalised to UMLS concept unique identifiers (CUIs) associated with SNOMED-CT concepts, and the Biocreative V CDR corpus [55] (see Table 15), in which chemicals are normalised to MESH IDs.']],\n",
       "  [['Many researchers made important contributions to dataset construction including the GENIA corpus [36], the NCBI disease corpus [37], and the ShARe/CLEF eHealth evaluation [38].']],\n",
       "  [['In the past several years, as a number of manually annotated corpora have been publically available for clinical entity recognition in challenges such as the Center for Informatics for Integrating Biology & the Beside (i2b2) In the past several years, as a number of manually annotated corpora have been publically available for clinical entity recognition in challenges such as the Center for Informatics for Integrating Biology & the Beside (i2b2) [4,[9][10][11], ShARe/CLEF eHealth Evaluation Lab (SHEL) [12,13], SemEval (Semantic Evaluation) [14][15][16][17], etc.']],\n",
       "  [['To promote the development of machine learning-based system, many publicly available corpora have been developed by organizers of some clinical NLP challenges such as the Informatics for Integrating Biology and the Bedside (i2b2) 2009 [8], 2010 [9][10][11][12][13], 2012 [14][15][16][17][18], 2014 [19][20][21][22][23], ShARe/CLEF eHealth Evaluation Lab 2013 dataset [24], and Semantic Evaluation 2014 task 7 [25], 2015 task 6 [26], 2015 task 14 [27], and 2016 task 12 [28] datasets.']],\n",
       "  [['Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].'],\n",
       "   ['In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].']],\n",
       "  [['We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30].']],\n",
       "  [['12,13 Besides, a series of challenges have been organized and played as significant roles to push forward state of the art, including ShARe/CLEF eHealth 2013 Task 1, 14 SemEval-2014 Task 7, 15 and SemEval-2015 Task 14.'],\n",
       "   ['For the ML ranking system design, we adapted the idea from previous works For the ML ranking system design, we adapted the idea from previous works [12][13][14][15][16][17][18][19][20][21][22] that concept normalization benefits from either task-specific similarity scoring functions or task-specific text representations.']],\n",
       "  [['30 The clinical NLP community has organized a series of open challenges with a focus on clinical concept extraction, including Informatics for Integrating Biology & the Bedside (i2b2), 23,31,32 National NLP Clinical Challenges (n2c2), 25 SemEval, [33][34][35] and ShARe/CLEF 36,37 in the past decade.']],\n",
       "  [[\"The clinical NLP community has organized a series of shared tasks for retrieving various patients' information from clinical narratives including diseases or disorders [15][16][17], adverse drug events [18,19], and medical temporal relations [20].\"]],\n",
       "  [['Most of these corpora come in the form of shared tasks such as i2b2 [14], ShARe/CLEF [15,16], and SemEval [17].']],\n",
       "  [['In the clinical domain, the ShARe/CLEF 2013 eHealth Eval-51 uation Lab and the i2b2/VA challenge methodologies have been applied in shared tasks 52 [11][12][13].']],\n",
       "  [[', 2015, Suominen et al., 2013, Bethard et al.']],\n",
       "  [['For those interested in this task, a good place to start are the disorder normalization systems built for the SHARe/CLEF eHealth 2013 Evaluation Lab, a community NLP challenge focusing on clinical named entity recognition and concept normalization (65,42).'],\n",
       "   ['A related issue is that clinical information extraction models are generally trained using the same few annotated datasets (54,109,110,65,111), which limits the kinds of annotations they can produce.']],\n",
       "  [['In addition to these, ShARe/CLEFE clinical dataset used by BLUE benchmark which uses the train, dev and test split released by (Suominen et al., 2013) is used for NER task.']],\n",
       "  [['Research on concept normalization has grown thanks to shared tasks such as disorder normalization in the 2013 ShARe/CLEF Research on concept normalization has grown thanks to shared tasks such as disorder normalization in the 2013 ShARe/CLEF (Suominen et al., 2013), chemical and disease normalization in BioCreative V Chemical Disease Relation (CDR) Task (Wei et al.'],\n",
       "   [', 2019) and ShARe/CLEF (Suominen et al., 2013)  We take 40 clinical notes from the released data as training, consisting of 5,334 mentions, and the standard evaluation data with 6,925 mentions as our test set.']],\n",
       "  [['‚Ä¢ Informatics for Integrating Biology & the Bedside (i2b2) ‚Ä¢ Informatics for Integrating Biology & the Bedside (i2b2) [11] ‚Ä¢ Computational Medicine Center (CMC) corpus [12] ‚Ä¢ ShARe/CLEF eHealth [13] ‚Ä¢ Multiparameter Intelligent Monitoring in Intensive Care (MIMIC) [14] ‚Ä¢ BioScope Corpus [15].']],\n",
       "  [['State-of-the-art models are increasingly successful in high-resource languages such as English or Spanish, where labeled datasets include ShARe/CLEF eHealth 2013 Task 1 (Suominen et al., 2013), SemEval-2014Task 7 (Prad-han et al.'],\n",
       "   ['Following previous works on entity linking (Suominen et al., 2013;Pradhan et al.']],\n",
       "  [['The representative clinical text datasets are MIMIC-III [25] and NLP community challenges, such as n2c2 NLP Research Data Sets [26], ShARe/CLEF eHealth [27], and CEGS N-GRID [19].']],\n",
       "  [['ShARe13ShARe132  (Suominen et al., 2013) is a discontinuous NER dataset with a corpus of clinical notes and contains annotations of disorder mentions.']],\n",
       "  [[', 2003;Petersen and Ostendorf, 2007;De Belder and Moens, 2010;Suominen et al., 2013).']],\n",
       "  [[', 2012;Mohan and Li, 2019), and electronic health records (EHRs) (Suominen et al., 2013).']]],\n",
       " [[['[35] and [36].'],\n",
       "   ['Therefore, a future step in the evaluation of our search approach would be to benchmark our methods against these existing IR techniques specifically developed for the prior art search, for example, using the CLEF-IP datasets [35,36].']],\n",
       "  [['2012) and 2013 (Piroi et al. 2013) in a more challenging setting.'],\n",
       "   ['2012(Piroi et al. , 2013)), and chemical structure recognition (Piroi et al.']],\n",
       "  [['Dataset We use the CLEF-IP2013 Passage Retrieval task Dataset We use the CLEF-IP2013 Passage Retrieval task [26].']],\n",
       "  [[', by the CLEF-IP labs [25,26,27,28,29].']]],\n",
       " [[['Unreleased sequences from ViDRILO have been successfully used in the RobotVision at Image-CLEF competition [13] in 2013 [3] and 2014 [2].']],\n",
       "  [['ImageCLEFImageCLEF10 is the image retrieval track of the Cross Language Evaluation Forum (CLEF) 11  [6].']],\n",
       "  [['In this context, using multimedia identification and collaborative data management tools is considered as one of the most promising solution to help bridging the taxonomic gap In this context, using multimedia identification and collaborative data management tools is considered as one of the most promising solution to help bridging the taxonomic gap [18,5,4,17,25,19].']],\n",
       "  [['ImageCLEFImageCLEF1  [6] is a benchmark on cross-language image annotation and retrieval in various domains.'],\n",
       "   ['3196 [6] and this was better than the purely textual runs.']],\n",
       "  [[\"12   To construct different databases we used the standard MPEG-7 [28], ImageCLEF-2013 [11] and Kimia (Brown University's) [47] datasets.\"],\n",
       "   [\"12   To construct different databases we used the standard MPEG-7 [28], ImageCLEF-2013 [11] and Kimia (Brown University's) [47] datasets.\"]],\n",
       "  [['Initially set up for cross-lingual purposes, the ImageCLEF series have factually become monolingual [20].']],\n",
       "  [['First, owing to the high modality diversity, a single algorithm cannot be capable to differentiate from medical archives 10 .']]],\n",
       " [[['Automatic text generation, particularly multi-document extractive summarization, systems also face SO problem [3,4].']]],\n",
       " [[['Writing style, rather than topic information, is the primary factor in text forensics tasks [11].'],\n",
       "   ['PAN has spurred widespread interest in this task among the research community, obtaining rather high participation figures in verification tasks from 2013 to 2015 [11,26,34].'],\n",
       "   ['The first edition was organized with the aim of investigating age and gender identification in a social media realistic scenario [11].']],\n",
       "  [['Computational authorship analysis of the homeric poems of anonymous documents Computational authorship analysis of the homeric poems of anonymous documents (Chaski, 2005) -and the process of detecting phishing e-mail (Gollub et al., 2013).']],\n",
       "  [[', [10,38,40,90,95]).']]],\n",
       " [[['Machine reading MCQs (qa4mre in Table Machine reading MCQs (qa4mre in Table 1) are provided by the evaluation campaign QA4MRE 2012 (Pe√±as et al., 2013) for the main task.']],\n",
       "  [['Our data consist of the question answering test sets at QA4MRE 2013 Our data consist of the question answering test sets at QA4MRE 2013 (Pe√±as et al., 2013).']],\n",
       "  [[', 2021;Pe√±as et al., 2013) and derive an AD-specific QA dataset from them.'],\n",
       "   [', 2021) consisting of diverse biomedical and clinical questions from various sources, 4) QA4MRE (Pe√±as et al., 2013) containing a subset of questions for AD derived from PubMed and Medline.']],\n",
       "  [['The reading comprehension datasets include RACE (27) and QA4MRE (45).']]],\n",
       " [[[', taking several days to train over 3000 sentences, and difficult to adapt to other KBs, let alone retrieving multiple KBs within one query (some questions in the QALD task are answered via querying over both DBpedia and Yago (Cimiano et al. 2013)).']],\n",
       "  [['For English, we developed M-ATOLL using both training and test data of the ontology lexicalization task of the QALD-3 challenge For English, we developed M-ATOLL using both training and test data of the ontology lexicalization task of the QALD-3 challenge [5] as development set, i.']],\n",
       "  [['For this reason, we created a set of natural language questions for the specific commercial domain of the phone industry, following the guidelines described by the QALD organizers for the creation of their question sets [1].']],\n",
       "  [[', counting), comparatives, or superlatives, even though those features are relatively frequent [14].'],\n",
       "   ['Section 7 evaluates the NL coverage, the naturalness, and the performance of SQUALL on questions from the QALD challenge (Query Answering over Linked Data) [14].'],\n",
       "   ['Interestingly, the participants of the QALD-3 challenge [14] cover a wide range of the formality continuum, even if most of them (4/6) fall in the \"spontaneous natural language\" category.']],\n",
       "  [['In question answering systems such as QALD [1,2,3], answer to the queries is found from DBpedia which is updated on a regular basis.']],\n",
       "  [['Existing systems in the first stage translate a natural language question N into SPARQL queries [11,18,32], which are evaluated in the second stage.']],\n",
       "  [['The main objective of question answering over linked data [17,26] is to facilitate, in part, multilingual access to the information originally produced in different culture and language.']],\n",
       "  [['Finally, the most complex query Show me all songs from Bruce Springsteen released between 1980 and 1990 contains a date range constraint and was found too hard to answer by all systems evaluated in the QALD evaluation [5].']],\n",
       "  [['The Web site also provides a number of examples, in particular the 200 questions from the QALD-3/DBpedia 16 chal-48 CONTENTS lenge [Cimiano et al., 2013], reformulated in Squall.'],\n",
       "   [', 2013], which was the second task of the QALD 2013 challenge [Cimiano et al., 2013].'],\n",
       "   [\"[Kaljurand and [Kaljurand and Kuhn, 2013]   37 auto-completion, 15, 41, 50 Business Intelligence (BI), 22,34,50 Case-Based Reasoning (CBR), 54 completeness, 11,29,39,45 concept lattice,17,31,38,53 conceptual navigation,27,48 controlled natural language,14,41,46,50 cube,22,34,35,54,58 DBpedia,10,40,47 disjunction,30,32,38,45 dominant decomposition,17 dynamic taxonomy,31 exploratory search,48 expressivity,10,12,15,19,23,29,38,40,47,48 extension,11,18,27,28,54,58 faceted search, 20, 31, 38 federated search, 56 file system, 16,32 focus,38,40,48 Formal Concept Analysis (FCA),17,27,38,46, 53 formal language, 13 functional dependency, 37 Geographical Information System (GIS), 33, 51 graph, 15 graphical editor, 41 guidance, 12, 14, 18, 23, 29, 43 hierarchy, 16, 22, 37 hypertext, 15 index, 28, 31 Inductive Database (IDB), 58 Inductive Logic Programming (ILP), 53 information access, 9 intension,11,18,27,28,58 interactive view,19,29,41,48 knowledge base,27 lexicon,43,47,53 Linked Open Data (LOD),10,16,44 logic,30,44,   10,13,27,29,37,41,46,48,58 Query-based Faceted Search (QFS), 38,42 RDF,16,39,46,55 readability,12,15,18,23,29,40,47, 49 Relational Concept Analysis (RCA), 38 reliability, 12 safeness, 11,12,15,19,24,29,39,44,45 scalability,12,19,24,29,40,50,55 Semantic Web,16,21,41 software Abilis,33,36,50,51 Camelis,32 Geolis,33 LisFS,32 PEW,44 Portalis,33 Sewelis,40,42,44,50 Sparklis,40 squall2sparql,47 SPARQL,13,21,23,37,40,41,47,50,51 SPARQL endpoint,40,55 spatial geometry,33 specificity,12,15,18,24,43 SQL,13,37,41 Squall,46 update language,41,46 usability,10,12,18,23,29,43,48 visualization,23,37,51 Several researchers have recognized the name problem in information systems (Gifford, Jouvelot, Sheldon, and O'Toole, 1991;Gopal and Manber, 1999).\"],\n",
       "   [', counting), comparatives, or superlatives, even though those features are relatively frequent [14].'],\n",
       "   ['Section 7 evaluates the NL coverage, the naturalness, and the performance of SQUALL on questions from the QALD challenge (Query Answering over Linked Data) [14].'],\n",
       "   ['Interestingly, the participants of the QALD-3 challenge [14] cover a wide range of the formality continuum, even if most of them (4/6) fall in the \"spontaneous natural language\" category.']],\n",
       "  [['For QALD-  shows the results achieved in QALD-3 for DBpedia test set [105].']],\n",
       "  [['Solving the QALD tasks requires mapping natural language questions in multiple languages into a corresponding SPARQL query (Cimiano et al., 2013).']],\n",
       "  [['For example, the series of Question Answering over Linked Data (QALD) (Cimiano et al., 2013).']]],\n",
       " [[['The proposed system was validated in the context of two international shared task evaluations: RepLab [30,31] and SemEval: Twitter Sentiment Analysis [23,25], and ranked among the top places in both, attesting the adequacy of the approach.']],\n",
       "  [['3 These tasks were organized as a CLEF evaluation task [8,9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['This has recently given birth to \"online reputation management\" within the marketing domain where automated and semi-automated methods facilitate monitoring reputation of entities instead of relying completely on the manual reputation management by an expert (or a group of experts) as was traditionally done [8].'],\n",
       "   ['RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.'],\n",
       "   ['We use the dataset provided by CLEF 2013 RepLab filtering task We use the dataset provided by CLEF 2013 RepLab filtering task [8] 13 which basically comprises a collection of tweets.'],\n",
       "   ['8 For books.']],\n",
       "  [[', 2013), CLEF 2013 RepLab 2013 (Amig√≥ et al., 2013), and TASS 2013(Villena-Rom√°n and Garc√≠a-Morera, 2013) have recently targeted tweets or cell phone messages as analysis text.']],\n",
       "  [['Indeed the 2014 tweet contextualization topics were a selection of 240 tweets from RepLab 2013 [3], it was thus necessary to use prior Wikipedia dumps.']],\n",
       "  [[\"We use the context of RepLab We use the context of RepLab [2,3] tasks to evaluate our proposal that is to say: to propose an overview of 61 entity's (drawn in 4 domains: Automotive, Banking, Music and University) e-Reputation regarding experts taxonomies using provided set and pertaining of Micro-Blogs concerning each entity.\"]],\n",
       "  [['The focus of RepLab-2013 -An evaluation campaign for Online Reputation Management Systems (Amig√≥ et al, 2013) shared task was on target level SA.'],\n",
       "   ['-RepLab: For target level SA, we used a Twitter database created for the RepLab-2013 shared task -RepLab: For target level SA, we used a Twitter database created for the RepLab-2013 shared task (Amig√≥ et al, 2013).']],\n",
       "  [['In their study on a sample of 2000 tweets, Pear Analytics In their study on a sample of 2000 tweets, Pear Analytics [1] classified 40% as containing \"pointless babble\", with another 37.']],\n",
       "  [['From our point of view, this separation is not correct: as other previous works have noted [15,16], in certain application domains facts may also have polar orientations, since they may have negative/positive implications for users (patients, in our case).'],\n",
       "   ['Moreover, agreement achieved is similar to those reported in other 3-classes text classification problems [15,60].'],\n",
       "   ['Similar results were found in other domain-specific sentiment analysis tasks, such as reputation polarity classification, where classes are also very unbalanced [15].']],\n",
       "  [['Online reputation management: Reputation management in social media Online reputation management: Reputation management in social media [76] has been proposed in RepLab competitive evaluation campaign for Online Reputation Management Systems [12,11].']],\n",
       "  [[', 2012(Amig√≥ et al., , 2013(Amig√≥ et al.']],\n",
       "  [['sentiment analysis or reputation management [2].']],\n",
       "  [['Sometimes accuracy is used, like in NTCIR-7 [22], but this is a metric for nominal scales; in some other cases, like in RepLab 2013 [3], Reliability and Sensitivity [4] are used, which consider only ranking information and no category membership; even metrics for interval scales, like Mean Average Error (MAE), have been used [16].']],\n",
       "  [['The focus of RepLab-2013 -An evaluation campaign for Online Reputation Management Systems The focus of RepLab-2013 -An evaluation campaign for Online Reputation Management Systems (Amig√≥ et al., 2013) shared task was on target-level sentiment analysis, more specifically on entitylevel.'],\n",
       "   ['RepLab A Twitter database was created for the RepLab-2013 shared task RepLab A Twitter database was created for the RepLab-2013 shared task (Amig√≥ et al., 2013) for entity reputation monitoring.'],\n",
       "   ['1 we detail our surfaceformbased submission (Hangya and Farkas, 2013a) to the RepLab-2013 -An evaluation campaign for Online Reputation Management Systems (Amig√≥ et al., 2013) shared task, which outperformed all participants.'],\n",
       "   ['The goal of the RepLab-2013 -An evaluation campaign for Online Reputation Management Systems challenge The goal of the RepLab-2013 -An evaluation campaign for Online Reputation Management Systems challenge (Amig√≥ et al., 2013) was to monitor the reputation of several entities, like companies, organizations, celebrities, etc.'],\n",
       "   [', 2012) and (Amig√≥ et al., 2013).'],\n",
       "   ['Many shared tasks were proposed to boost research for the second subtask, which also motivated our work Many shared tasks were proposed to boost research for the second subtask, which also motivated our work (Amig√≥ et al., 2013;Pontiki et al.'],\n",
       "   ['(ii) A BACKGROUND corpus of 296K English and 150K Spanish (non-annotated) tweets released with the test data of the RepLab task (ii) A BACKGROUND corpus of 296K English and 150K Spanish (non-annotated) tweets released with the test data of the RepLab task (Amig√≥ et al., 2013).'],\n",
       "   ['Training Data for Sentiment Classifiers For sentiment classification, we use data from the RepLab 2013 shared task Training Data for Sentiment Classifiers For sentiment classification, we use data from the RepLab 2013 shared task (Amig√≥ et al., 2013), which was already introduced and used in Chapter 3.'],\n",
       "   ['Comparing our best system which used all training data to the official results (Amig√≥ et al., 2013), we would rank 2 nd even though our system is not fine-tuned for the RepLab dataset.']],\n",
       "  [['‚Ä¢ RepLab polarity dataset ‚Ä¢ RepLab polarity dataset [1]: A dataset of 84,745 tweets mentioning companies, annotated for polarity as positive, negative or neutral.']],\n",
       "  [[\"The 'alert detection' task of RepLab 2013 The 'alert detection' task of RepLab 2013 [4] was to predict the priority of an entity-related topic, from among the three classes -alert, mildly relevant and unimportant, in decreasing order of priority.\"],\n",
       "   ['Apart from brand consistency, we consider the aspects of negative polarity and centrality explored in the \"Reputation Alert Detection\" task of RepLab 2013 [4].']],\n",
       "  [['Twitter datasets were annotated for the task of reputation monitoring Twitter datasets were annotated for the task of reputation monitoring [1,9].']],\n",
       "  [['Sometimes accuracy is used, like in NTCIR-7 [22], but this is a metric for nominal scales; in some other cases, like in RepLab 2013 [3], Reliability and Sensitivity [4] are used, which consider only ranking information and no category membership; even metrics for interval scales, like Mean Average Error (MAE), have been used [16].']],\n",
       "  [['It is important to distinguish between sentiments and reputation polarity derived from text, as pointed out in [1] and [20] because a negative sentiment text sample might actually prove to be a positive reputation text sample for a target entity and vice versa.'],\n",
       "   ['RepLab is a well-known project regarding ORM competition, and identifying the reputation polarity of tweets has been one of the major tasks of RepLab [1].'],\n",
       "   ['The RepLab 2013 dataset The RepLab 2013 dataset [1] contains tweets regarding 61 entities from four domains that are automotive, banking, universities, and celebrities.'],\n",
       "   ['The traditional approaches to this task include using sentiment lexicons and linguistic features [1].'],\n",
       "   ['All the approches listed in this table use the RepLab 2013 dataset comprised of all 61 entities for their evaluation [1,2,4,5,6,7,8,9,10,20] under the same training and testing sets.']],\n",
       "  [['Although significant advances have been made in RepLab 1 [1,2].'],\n",
       "   ['Most of the contributions on reputation monitoring to extract sets of tweets requiring a particular attention from a reputation manager have been proposed in the last editions of RepLab [1,2].'],\n",
       "   [\"We perform a supervised classification over Replab'2013-14 dataset We perform a supervised classification over Replab'2013-14 dataset [1,2].\"]]],\n",
       " [[['The biomedical text mining community regularly verifies the progress of the field through competitive evaluations, such as BioCreative The biomedical text mining community regularly verifies the progress of the field through competitive evaluations, such as BioCreative [8][9][10], BioNLP [11,12], i2b2 [13], CALBC [14], CLEF-ER [15], DDI [16], BioASQ [17], etc.']],\n",
       "  [['Another important initiative was the CLEF-ER challenge that took place in 2013 as part of the Mantra project aimed at providing multilingual documents and terminologies for the biomedical domain Another important initiative was the CLEF-ER challenge that took place in 2013 as part of the Mantra project aimed at providing multilingual documents and terminologies for the biomedical domain [69].']]]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc123cf5-98ee-4daa-8be8-6c50cf26908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    flat_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten(item))\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b928de60-4dbe-4516-9804-93751c59a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = flatten(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3daa3ac-a1b6-4196-af09-cf60dca1c9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One of the proposed tasks asked to the participants on this data was to automatically pre-populate handover forms with relevant text-snippets (slot filling) [16].',\n",
       " 'The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers [51].',\n",
       " 'The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers The CLEF eHealth 2016 Task 1 required the participants to implement systems that are able to identify relevant text snippets from free-text nursing handovers [51].',\n",
       " 'Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 [51].',\n",
       " 'Further development of search technologies for consumer health search considers self-diagnosis information needs and needs related to treatment and management of health conditions [28].',\n",
       " 'Evaluation campaigns and resources in this domain are presented, including TREC Medical Records Track [43,45,46], TREC Clinical Decision Support Track [36][37][38], CLEF eHealth (consumer health search [13,14,35,55] and as of 2017 search systems for the compilation of systematic reviews), i2b2 Shared Task Challenges¬≥, ALTA Shared Task ¬≥https://www.',\n",
       " 'Further development of search technologies for consumer health search considers self-diagnosis information needs and needs related to treatment and management of health conditions (Zuccon et al. 2016).',\n",
       " 'To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth [18].',\n",
       " ', [3,27].',\n",
       " 'These queries are part of the Conference and Labs of the Evaluation Forum (CLEF) 2016 electronic health (eHealth) collection [14], which is extensively used in this paper.',\n",
       " 'The CLEF 2016 collection contains 300 queries and 3298 relevant documents that also have been assessed with respect to understandability [14].',\n",
       " 'We used the thresholds U=2 for CLEF 2015 and U=40 for CLEF 2016, based on the distribution of understandability assessments and the semantic of understandability labels [44,14].',\n",
       " 'Relevance assessments on the CLEF 2015 and 2016 collections are incomplete Relevance assessments on the CLEF 2015 and 2016 collections are incomplete [44,14], that is, not all top ranked Web pages retrieved by the investigated methods have an explicit relevance assessment.',\n",
       " 'To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth To investigate the influence choices in KB retrieval have on query expansion for the CHS task, we empirically evaluated methods using the CLEF 2016 eHealth (Zuccon et al. 2016).',\n",
       " 'In 2013In , 2014In , 2015In , 2016In , 2017In , 2018, and 2019 as many as 170, 220, 100, 116, 67, 70, and 67 teams have registered their expression of interest in the CLEF eHealth tasks, respectively, and the number of teams proceeding to the task submission stage has been 53, 24, 20, 20, 32, 28, and 9, respectively [4,5,[8][9][10]16,17].',\n",
       " 'In the 2017 CLEF eHealth CHS task, similarly to 2016, we used the ClueWeb 12 B13In the 2017 CLEF eHealth CHS task, similarly to 2016, we used the ClueWeb 12 B134 document collection [12,18].',\n",
       " 'As stated in the overview of the corresponding CLEF eHealth 2016 task While some of the text lines were short and contained a term that could be directly linked to a single ICD10 code, other lines could be run-on [10].',\n",
       " 'As summed up in As summed up in [10], the systems that took part in the rst edition of this shared task opted for dierent methods, namely: dictionary-based pattern matching, machine learning (including topic modeling) and information retrieval methods.',\n",
       " '(2007) o er to work on radiology reports while N√©v√©ol et al. (2016) release a large dataset for ICD-10 coding of death certi cates.',\n",
       " '(2007) proposent de travailler sur des rapports de radiologie tandis que N√©v√©ol et al. (2016) ont distribu√© un ensemble de certi cats de d√©c√®s annot√©s avec des codes ICD-10.',\n",
       " 'Recently [3,4,5], the C√©piDC task consisted in extracting ICD-10 codes from death reports in several languages (French in 2016, French and English in 2017, French, Hungarian and Italian in 2018).',\n",
       " 'Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].',\n",
       " 'Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data Probably more than 50% of the figures in the biomedical literature in PubMed Central (PMC) 3 are compound figures (figures consisting of several subfigures) based on estimations of analysing a subset of the data [11].',\n",
       " 'Figure 2 shows that hierarchy of images classes that was used [10,11] to classify all subfigures into types.',\n",
       " 'These figures were distributed for the Im-ageCLEFmed 2016 multi-label and subfigure classification tasks7  [11] together with the figure captions.',\n",
       " 'More information can be found in the working notes of CLEF 2016 [11].',\n",
       " 'We briefly summarize the metrics here; please see [10], [31] for full details.',\n",
       " 'The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].',\n",
       " 'The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].',\n",
       " 'Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].',\n",
       " '7% in ImageCLEF2016 [6].',\n",
       " 'The best results are obtained by a combination of cross-media predictions [5,6].',\n",
       " 'Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance Our group of DUTIR (Information Retrieval Laboratory of Dalian University of Technology) took part in the subtask of compound figure detection in ImageCLEF2016 and achieved good performance [6].',\n",
       " '7% in ImageCLEF2016 [6].',\n",
       " 'For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset For our experiments, we utilize the ImageCLEF 2015 and ImageCLEF2016 Compound Figure Detection dataset [5,6] using a subset of PubMed Central.',\n",
       " 'In ImageCLEF 2016 In ImageCLEF 2016 [6], they expand their training set to 20,997 figures and reduce the size of the test set to 3456.',\n",
       " 'Take ImageCLEF medical [5,33,34] as an example; it provides thousands of labeled medical images for modality classification, which is a much smaller amount than the ImageNet dataset [20], which contains 1.',\n",
       " 'The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].',\n",
       " 'The subtask of subfigure classification was first introduced in ImageCLEF2015 [33] and continued in ImageCLEF2016 [34], but was similar to the modality classification subtask organized in ImageCLEF2013 [5].',\n",
       " 'For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.',\n",
       " 'In ImageCLEF2016 [34], they expand the training set to 6776 figures and the test set to 4166 figures.',\n",
       " 'For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].',\n",
       " 'For this subtask, visual and textual methods are possible; however, visual features play a major role when making predictions based on cross-media [5,33,34].',\n",
       " 'We obtained good performance We obtained good performance [36] using CNN-6 in the Compound Figure Detection Task [33,34].',\n",
       " 'Identifying image types has been done many times [6,7] but so far it has not allowed to leverage machine learning on a very large scale to our knowledge.',\n",
       " 'A similar problem was later reintroduced as the ImageCLEF subfigure classification sub-task in 2015 and 2016 which focused on modality classification of individual subfigures of compound figures, in effect removing the \"COMP\" or compound figure modality [14,15].',\n",
       " 'In addition, the feature extraction process is computationally expensive and demands expertise in developing algorithms, requiring extensive labeling and accounting for the limited visibility and variability in morphology and position of the region of interest (ROI) for modality detection [8].',\n",
       " 'National Library of Medicine (NLM), the ImageCLEF2013 modality classification challenge [8], and the World Wide Web.',\n",
       " 'The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.',\n",
       " 'The evaluation required to have a minimum overlap for the subfigure division between the ground truth and the data supplied by the groups in their runs (Garc√≠a Seco de Herrera et al, 2013).',\n",
       " 'Text has been used in most retrieval applications [10] but has also obtained very good results in modality classification [17,2], as it is complementary to visual information.',\n",
       " 'They also implemented training on artificially-constructed datasets and reported superior performances on ImageCLEF data sets [11].',\n",
       " 'For the imageCLEF2016 dataset [11], we trained 50 epochs using a smaller batch size of 8.',\n",
       " '[27] using the ImageCLEF2016 dataset [11].',\n",
       " 'They also implemented training on artificially constructed datasets and reported superior performances on ImageCLEF dataset (Garc√≠a Seco de Herrera et al., 2016).',\n",
       " 'For the imageCLEF2016 dataset (Garc√≠a Seco de Herrera et al., 2016), we trained 50 epochs using a smaller batch size of 8.',\n",
       " '(2020) using the ImageCLEF2016 dataset (Garc√≠a Seco de Herrera et al., 2016).',\n",
       " 'On the information retrieval and keyword spotting front, there are a plethora of works dealing with handwritten document indexing and retrieval On the information retrieval and keyword spotting front, there are a plethora of works dealing with handwritten document indexing and retrieval [26,22,8,2,57].',\n",
       " 'One relevant example is the ImageCLEF 2016 Handwritten Scanned Document Retrieval challenge [57], aimed at developing retrieval systems for handwritten documents.',\n",
       " 'We use the ImageCLEF 2016 Bentham Handwritten Retrieval dataset [57], which has images from the Bentham Transcriptorium project [10].',\n",
       " 'In the field of information retrieval and keyword spotting, there have been numerous efforts on handwritten document indexing and retrieval [15]  [16].',\n",
       " 'In the field of information retrieval and keyword spotting, there have been numerous efforts on handwritten document indexing and retrieval [15]  [16].',\n",
       " 'Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.',\n",
       " 'First, we briefly explain the ImageCLEF collection [22] for the scalable image annotation task.',\n",
       " \"Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge Looking at the impressive results achieved by CNN's in the 2015 and 2016 edition of the international Plant-CLEF challenge [31,33] on species identification, there is no doubt that they are able to capture discriminant visual patterns of the plants in a much more effective way than previously engineered visual features.\",\n",
       " 'To this end, we fine-tune our network on leaf training images from the 2016 PlantCLEF challenge [18] which is a large but unrelated plant dataset with images taken in a different setting than either of our testing datasets.',\n",
       " 'Some famous networks such as AlexNet [20], GoogLeNet [25], VGG [24] have also been applied for plant identification, especially in PlantClef competition from 2014 to 2017 and have obtained higher results compared to traditional methods based on hand-designed features [6], [14], [17], [26], [27].',\n",
       " 'Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10].',\n",
       " 'GoogLeNet, VGGNet, CaffeNet, AlexNet, ResNet, Inception v4 and Inception-ResNet are used by most teams in the PlantCLEF 2016/2017 competition [9,10], including the winning team.',\n",
       " 'Concerning plant identification from image of single organ, we apply CNN as it has been proved to be effective in previous studies [9].',\n",
       " '[25].',\n",
       " 'Some participants of PlantCLEF2016 competition [12] tried using geolocation information.',\n",
       " 'To the best of our knowledge, there are only two fine-grained datasets that have been used in geolocation related research in this field [4,12].',\n",
       " 'The dataset for one of the Image-CLEF/LifeCLEF competitions [2], PlantCLEF2016 [12], contains partial geolocation information (less than half of the data) and is restricted to only plants from France.',\n",
       " '(1) plants (1) plants [10], (2) car brands and models [11], and (3) insects (the Formicidae ants genera) [5].',\n",
       " 'The PlantCLEF 2016 dataset The PlantCLEF 2016 dataset [10] consists of observations of plant specimen and provides annotations in terms of organ, species, genus, and family.',\n",
       " 'To obtain an accurate flower identity descriptor ùêπ ùëì ùëñùëë , we adopt the EfficientNet [57] and pretrain it on LifeCLEF2021 Plant Identification [5,[23][24][25].',\n",
       " 'A captura em ambiente natural, tamb√©m √© realizada em alguns trabalhos, como em [Go√´au et al. 2016, Sun et al.',\n",
       " 'For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.',\n",
       " 'In [60], the technical program of LIFEClef 2016 describes the future of birdsong recognition centered in DNN, as, for instance, the one described in [61].',\n",
       " 'The datasets of BAD task includes Warblr [3], Chernoby1 Exculusion Zone (CEZ) [3], Freefield [16], HJA [12], Bird-CLEF [17].',\n",
       " 'Statistical methods with great potential for automated identification are continuously appearing in the scientific literature Statistical methods with great potential for automated identification are continuously appearing in the scientific literature [35], and as discussed above, PROTAX-Sound provides a statistically rigorous method to combine the strengths of the different techniques.',\n",
       " ', [23], [13], [24], [25].',\n",
       " 'A number of studies also apply deep learning technologies in bird voice classifica-tion [42,43,44], however, they only use conventional deep learning approaches such as CNN and do not make any novel improvement.',\n",
       " 'This category involves several species, from the smallest such as insects or birds emitting trains of voiced pulses [2,3] to the largest mammals.',\n",
       " 'Deep neural networks (DNNs) have consistently outperformed traditional methods for bird detection since 2016 Deep neural networks (DNNs) have consistently outperformed traditional methods for bird detection since 2016 [4,5].',\n",
       " 'Publications related to these competitions have shown successful approaches for that task, allowing progress in the study of biodiversity (Go√´au et al., 2016), particularly since convolutional neural networks started to be used as models trained from images representing the temporal evolution of acoustic features (Piczak, 2016;Sprengel et al.',\n",
       " '2016;Stamatatos et al. 2016).',\n",
       " 'The most effective methods can mislead the identification systems in almost half of the cases [199].',\n",
       " ', author obfuscation [3,4,12,15]) used primarily term frequencies [5,9] and features from stylometry [8,9,18].',\n",
       " '[29] presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 [19,37,38].',\n",
       " '[29] presented the first large-scale evaluation of three AO approaches that aim to attack 44 AV methods, which were submitted to the PAN-AV competitions during 2013-2015 [19,37,38].',\n",
       " '[Potthast et al. 2016] propuseram um experimento para comparar a performance de diversos ofuscadores de texto.',\n",
       " 'According to According to [7], author obfuscation performance is measured based on three parameters, namely safety (the ability to disguise the original author from a given text), soundness (the level of similarity between the text modified from the obfuscation process with the original text), and sensibleness (the quality of grammar and legibility of the resulting text).',\n",
       " 'Al√©m deste trabalho, [Potthast et al. 2016] avaliaram a efici√™ncia de tr√™s m√©todos de ofuscac ¬∏√£o [Keswani et al.',\n",
       " 'Evaluating content preservation in text is important even if we value safety Evaluating content preservation in text is important even if we value safety (Potthast et al., 2016).',\n",
       " 'Indeed, it has been shown that many AId systems can be easily deceived in adversarial contexts [11,44].',\n",
       " 'In the task of author profiling, such characteristics can vary along dimensions of age, gender, and personality type, though we will focus on age, which has conventionally been the most difficult [15].',\n",
       " 'Age was determined either by a provided birth date in the profile, or via an estimation with the degree starting date in the education section [15].',\n",
       " ', gender) [18]; on the other hand, other works aim at preventing disclosure by masking the data that may disclose that authorship [19,20].',\n",
       " 'Numerous works on the topic have been published based on the results of the shared Author Profiling Tasks at digital text forensics events by PAN initiative [2,5,7,[27][28][29][30].',\n",
       " 'One way to predict age and other demographic information uses differences in linguistics to infer age groupings One way to predict age and other demographic information uses differences in linguistics to infer age groupings [4][5][6][7][8][9].',\n",
       " '\\uf0b7 In 2016 (Rangel et al., 2016b), the focus was on the cross-genre evaluation, that is, training in one genre (Twitter) and evaluating in another one (blogs, social media and reviews).',\n",
       " ', [41] for the latest edition), or the affective text [49], sentiment analysis [35], and other tasks in SemEval.',\n",
       " 'In 2016 PAN competition [13] the goal was to test the robustness of methods from the cross-genre perspective and SVMs were the dominant paradigm.',\n",
       " \"For example, in author profiling, the task is to recognize author's characteristics, such as age or gender [10], based on a collection of author's text samples, where the effect of data size is known to be an important factor influencing classification performance [11].\",\n",
       " 'Our main dataset, PAN16 TWIT (Rangel et al., 2016), is split into train and development, following Elazar and Goldberg (2018).',\n",
       " '[38], we plan further studies in this field.',\n",
       " 'Age classes included a gap in between: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27), 30s (33-48).',\n",
       " 'The best results were obtained on blogs for English with an accuracy above 75% for gender and below 60% for age identification [25].',\n",
       " 'Possible differences between female and male writing style have been widely investigated in the literature also with respect to the interference of textual genre (Rangel et al., 2016) Comparing the performance of the classification models, it turned out that lexical information is the most predictive one 5 .',\n",
       " 'The use of supervised learning algorithms for AP is shown in The use of supervised learning algorithms for AP is shown in [15].',\n",
       " 'For training and evaluating our AP approach we used the corpus of PAN2017 competition For training and evaluating our AP approach we used the corpus of PAN2017 competition [15], which was compiled from Twitter in Spanish.',\n",
       " 'However, in the context of the 2016 PAN evaluation campaign, a cross-genre setting was introduced for gender prediction on English, Spanish, and Dutch, and best scores were recorded at an average accuracy of less than 60% However, in the context of the 2016 PAN evaluation campaign, a cross-genre setting was introduced for gender prediction on English, Spanish, and Dutch, and best scores were recorded at an average accuracy of less than 60% (Rangel et al., 2016).',\n",
       " 'Unlike Facebook Unlike Facebook [10] and Twitter [11][12][13][14], there is only a handful of studies dissecting age demographics across cQA services [2,15].',\n",
       " 'html (accessed on 1 February 2021)) in Twitter, blogs and social media [11][12][13][14].',\n",
       " '97%) in this competition using stylistic and second-order features into SVM [20].',\n",
       " 'Twitter dataset (Rangel et al., 2016).',\n",
       " 'Elazar and Goldberg use the Twitter dataset Rangel et al. (2016), set the sensitive attribute to be age, and try to produce representations that would perform well on the main task of \"conversation detection\" (mention detection) on Tweets.',\n",
       " ', 2014;Rangel et al., 2016;Verhoeven et al.',\n",
       " 'Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .',\n",
       " ', 119 , while three 34,49,66 used data sets that were created for the PAN-CLEF author profiling shared tasks [103][104][105] .',\n",
       " 'A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .',\n",
       " 'We therefore opt for a subset of the PAN16 dataset (Pardo et al., 2016).',\n",
       " ', 2018a), and mention prediction with two attributes: gender and age of authors (Rangel et al., 2016) totaling 12 experiment setups.',\n",
       " 'The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF The background for this workshop is derived from the Interactive Track (2014-2016) of the Social Book Search Lab at CLEF [8], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.',\n",
       " 'This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( [5,6]) and is closely related to the Interactive Track of the CLEF Social Book Search Lab of 2015 and 2016 [7,8].',\n",
       " 'The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab [4,5,6], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.',\n",
       " 'They cover a period ranging from May 2015 to November 2016 and are composed by 134 different languages [5,4].',\n",
       " 'This resulted in an impressive amount of data and a pilot task, which are described in detail in (Ermakova et al. 2016).',\n",
       " '‚Ä¢ The Wikipedia collections WIKI10 and WIKI11 were used in the Wikipedia image retrieval task of ImageCLEF 2010 and 2011 [Popescu et al. 2010].',\n",
       " 'We start with a summary of multimedia databases because they represent a mature domain with more than 25 years of experience in close range digital photography; we have to mention image data collections such as ImageCLEF Wikipedia We start with a summary of multimedia databases because they represent a mature domain with more than 25 years of experience in close range digital photography; we have to mention image data collections such as ImageCLEF Wikipedia [5], PASCAL [6], ImageNet [7], and LabelMe [8].',\n",
       " 'The ImageCLEF Wikipedia The ImageCLEF Wikipedia [5] collection consists of 237 434 images for which 137 users provided annotations.',\n",
       " 'The Image-CLEF 2011 Wikipedia collection uses the ImageCLEF 2010 Wikipedia Collection [12], which is based on the September 2009 Wikipedia dumps.',\n",
       " ', TerraSAR-X) the number of categories that can be identified in each image is very limited [1] and these categories are not semantically annotated contrary with multimedia where this stage is very well established and the identified categories are already annotated [2] √∑ [4].',\n",
       " 'The ImageCLEF 2010 Database The image sequences used for the Robot Vision Task at the ImageCLEF 2010 challenge evaluation were taken from the previously unreleased COLD-Stockholm database The ImageCLEF 2010 Database The image sequences used for the Robot Vision Task at the ImageCLEF 2010 challenge evaluation were taken from the previously unreleased COLD-Stockholm database [34].',\n",
       " 'The number of participant groups increased to 8 for the 2010@ICPR edition [17], and the number of submitted runs also rose(29).',\n",
       " 'The third edition of the Robot Vision challenge The third edition of the Robot Vision challenge [32] was attended by seven groups, with three of them participating to both tasks: mandatory and optional.',\n",
       " \", 2010), together with the relevant class name and class description from a set of 1,000 different classes, and (bottom-row) ImageClef VCDT'10 data set (Nowak and Huiskes, 2010) together with some of the relevant concepts from a set of 93 labels.\",\n",
       " 'In this section we compare SVM and TagProp on the data set of the ImageClef 2010 Visual Concept Detection and Annotation Task In this section we compare SVM and TagProp on the data set of the ImageClef 2010 Visual Concept Detection and Annotation Task (Nowak and Huiskes, 2010).',\n",
       " \"The SVM and TagProp methods using the combination of Text, SIFT and LCS features, are the two best performing submissions to the ImageClef'10 challenge (Nowak and Huiskes, 2010), for more details see (Mensink et al.\",\n",
       " \"ImageCLEF'10 data set We use ImageClef'10 to refer to the subset of the MIR-Flickr data set ImageCLEF'10 data set We use ImageClef'10 to refer to the subset of the MIR-Flickr data set (Huiskes and Lew, 2008) that was used as training set in the ImageClef'10 Photo Annotation Challenge2  (Nowak and Huiskes, 2010).\",\n",
       " 'Our baseline method is the winning system of the challenge (Nowak and Huiskes, 2010;Mensink et al.',\n",
       " 'For an overview of the challenge, including the participants, different methods and results, see (Nowak and Huiskes, 2010).',\n",
       " ', diversifying visual contents, we tested a broad category of visual descriptors which are known to perform well in image retrieval tasks [14]: global color naming histogram (CN, 11 values) -maps colors to 11 universal color names: \"black\", \"blue\", \"brown\", \"grey\", \"green\", \"orange\", \"pink\", \"purple\", \"red\", \"white\", and \"yellow\" [15]; global Histogram of Oriented Gradients (HoG, 81 values) -represents the HoG feature computed on 3 by 3 image regions [16]; global color moments computed on the HSV Color Space (CM, 9 values) -represent the first three central moments of an image color distribution: mean, standard deviation and skewness [17]; global Locally Binary Patterns computed on gray scale representation of the image (LBP, 16 values) [18]; global Color Structure Descriptor (CSD, 64 values) -represents the MPEG-7 Color Structure Descriptor computed on the HMMD color space [19]; global statistics on gray level Run Length Matrix (GLRLM, 44 dimensions) -represents 11 statistics computed on gray level run-length matrices for 4 directions: Short Run Emphasis, Long Run Emphasis, Gray-Level Non-uniformity, Run Length Non-uniformity, Run Percentage, Low Gray-Level Run Emphasis, High Gray-Level Run Emphasis, Short Run Low Gray-Level Emphasis, Short Run High Gray-Level Emphasis, Long Run Low Gray-Level Emphasis, Long Run High Gray-Level Emphasis [20]; global descriptor which is obtained by the concatenation of all values.',\n",
       " 'The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task [17].',\n",
       " 'The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task The iCLEF10 data set was used in the ImageCLEF 2010 Photo Annotation task [17].',\n",
       " '2011;Nowak and Huiskes 2010;Stanek et al.',\n",
       " 'Previous work Previous work [16] have shown that annotation quality significantly affects the performance; 2) the MR datasets are picked up from photos uploaded by thousands of individuals, and represent the image retrieval area much more effectively; 3) there are several action concepts in the concept set of TV, however, in this work we only use static features which cannot capture the space-time aspects that are more effective to detect the action; 4) another important reason is that concepts for TV are much more infrequent than those of MR; 5) the most important but not the last reason is the two evaluation modes are different.',\n",
       " \"We conduct experiments using three public benchmark data sets: the Scene Understanding data set We conduct experiments using three public benchmark data sets: the Scene Understanding data set [4] (SUN'09), the data set of the ImageCLEF'10 Photo Annotation Task [5] (ImageCLEF), and the Animals with Attributes data set [6] (AwA).\",\n",
       " \"ImageCLEF'10 data set: We use ImageCLEF'10 to refer to the subset of the MIR-Flickr data set ImageCLEF'10 data set: We use ImageCLEF'10 to refer to the subset of the MIR-Flickr data set [26] that was used as training set in the ImageCLEF 2010 Photo Annotation Challenge [5].\",\n",
       " 'The same features have been used in our system that won the challenge [5]; for more details see [28].',\n",
       " 'However, to compare our methods to the ImageCLEF 2010 challenge results, we   4 shows both top performing results of the participants in the ImageCLEF 2010 challenge (see [5] for an overview of the participants, different methods and results) and the performances of our methods.',\n",
       " \"We conduct experiments using three public benchmark data sets: the Scene Understanding dataset We conduct experiments using three public benchmark data sets: the Scene Understanding dataset [4] (SUN'09), the dataset of the ImageCLEF'10 Photo Annotation Task [12] (ImageCLEF), and the Animals with Attributes dataset [11] (AwA).\",\n",
       " \"The ImageCLEF'10 data set is a subset of the MIR-Flickr data set The ImageCLEF'10 data set is a subset of the MIR-Flickr data set [10] used in the ImageCLEF Photo Annotation task in 2010 [12] as training set.\",\n",
       " 'For this we use the training set of the data set used in the ImageCLEF Photo Annotation task in 2010 [13].',\n",
       " \"We report on empirical results on image data sets from the PASCAL visual object classes (VOC) 2009 [27] and ImageCLEF2010 PhotoAnnotation [28] challenges, showing that non-sparse MKL significantly outperforms the uniform mixture and ' 1 -norm MKL.\",\n",
       " 'Both, the BoW-S and HoG kernels (Kernels 24-25,31-32) use gradients and therefore are moderately correlated; the same holds for the BoW-C and HoC kernel groups (Kernels [26][27][28][29][30].',\n",
       " 'ImageCLEF 2010 PhotoAnnotation: The Image-CLEF2010 PhotoAnnotation data set[28] consists of 8000 labeled training images taken from flickr and a test set with recently disclosed labels.',\n",
       " 'Many of these datasets are built from online photo sharing communities such as Flickr [1,2,3,4] and even collections built from image search engines [5] consist largely of Flickr images.',\n",
       " \"-The ImageCLEF Annotation Task ('CLEF') uses a subset of 18,000 images from the MIR dataset, though the correspondence is provided only for 8,000 training images [3].\",\n",
       " 'Details about these datasets can be found in [1,2,3,4].',\n",
       " 'Comparing our method to the best text-only method reported in the ImageCLEF 2011 competition [3], we observe a 7% improvement in MAP.',\n",
       " \"In the ImageCLEF'10 dataset (Nowak and Huiskes, 2010) the images are labeled with 93 diverse concepts, see Figure 4.\",\n",
       " ', 2004, Nowak, 2010].',\n",
       " '-CLEF -CLEF [53] is a subset of MIR dataset with newly added labels, which contains 18,000 images.',\n",
       " ', 2004;Carneiro and Vasconcelos, 2005;Nowak and Huiskes, 2010;Semenovich and Sowmya, 2010;Moser and Serpico, 2013].',\n",
       " ', 2010;Nowak and Huiskes, 2010;Nowak et al.',\n",
       " ', 2010;Nowak and Huiskes, 2010;Nowak et al.',\n",
       " 'Experimental results on two challenging datasets [Nowak and Huiskes, 2010;Nowak et al.',\n",
       " ', 2004;Carneiro and Vasconcelos, 2005;Nowak and Huiskes, 2010;Semenovich and Sowmya, 2010;Zhang et al.',\n",
       " \"‚Ä¢ ImageClef '10‚Ä¢ ImageClef '105 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2010 photo annotation challenge [Nowak and Huiskes, 2010].\",\n",
       " 'In particular, we used: an extended version of the c@1 introduced in 2009 in the ResPubliQA exercise of the CLEF 4 initiative [19], the standard accuracy measure, and the mean reciprocal rank (MRR) [25].',\n",
       " 'La principal contribuci√≥n es la adaptaci√≥n autom√°tica de sistemas de BR existentes (espec√≠ficamente de sus patrones de pregunta-respuesta y las taxonom√≠as de TRE) a diferentes dominios restringidos, sin requerir ning√∫n esfuerzo manual como el resto de propuestas previas existentes (Sekine;Sudo;Nobata, 2002;Hovy;Hermjakob;Ravichandran, 2002;Metzler;Croft, 2005;Li;Roth, 2006;Ferr√©s;Rodr√≠guez, 2006;Terol;Mart√≠nez-Barco;Palomar, 2006;Kosseim;Yousefi, 2008;Pe√±as et al., 2009).',\n",
       " 'For illustration purposes we present the three best performing systems that participated in the TREC QA 2007 [28], QA@CLEF 2009 (monolingual English) [108] and NTCIR QA Track 2008 (English-Simplified Chinese and English-Japanese) [81], see Table 1.',\n",
       " 'The idea was to adapt the system introduced in [62] and used as part of the participation in the Paragraph Selection (PS) Task and Answer Selection (AS) Task of QA@CLEF 2010 -ResPubliQA [64].',\n",
       " 'fbk4faq -In fbk4faq -In (Fonseca et al., 2016), the authors proposed a system based on vector representations for each query, question and answer.',\n",
       " 'A deep analysis of results is reported in A deep analysis of results is reported in (Fonseca et al., 2016), where the authors have built a custom development set by paraphrasing original questions or generating a new question (based on original FAQ answer), without considering the original FAQ question.',\n",
       " ', 2008;Pe√±as et al., 2009;P√©rez et al.',\n",
       " 'The evaluation has been performed on the Re-sPubliQA 2010 Dataset adopted in the 2010 CLEF QA Competition The evaluation has been performed on the Re-sPubliQA 2010 Dataset adopted in the 2010 CLEF QA Competition (Penas et al. (2010)).',\n",
       " 'Advancements in this domain also include emanating evaluation forums producing large-scale research methodologies like the Text REtrieval Conference (TREC) (Voorhees 2004) and the Cross-Lingual Evaluation Forum (CLEF) (Pe√±as et al. 2010).',\n",
       " 'This evaluation task was proposed at the Cross-Language Evaluation Forum (CLEF) (Pe√±as et al., 2009(Pe√±as et al.',\n",
       " 'Most of the approaches to this task use standard information retrieval (IR) processes [7], [8].',\n",
       " '26 [7].',\n",
       " 'The first project is the annual PAN International Competition on Plagiarism Detection (PAN-PC), initiated in 2009 (Potthast, Stein, Eiselt, Barr√≥n Cede√±o, & Rosso, 2009).',\n",
       " '24 in 2009 (Potthast et al., 2009).',\n",
       " 'Since 2010, academic competitions have been held to enhance and compare automatic vandalism detection techniques in Wikipedia, using an annotated corpus of changes as the ground truth (Potthast and Holfeld, 2011).',\n",
       " 'Due to its use in the 1st International Competition on Wikipedia Vandalism DetectionDue to its use in the 1st International Competition on Wikipedia Vandalism Detection17  (Potthast, Stein, and Holfeld 2010) it is one of the most widely used corpus in the scientific literature.',\n",
       " 'A more exhaustive compilation can be found in (Potthast, Stein, and Holfeld 2010).',\n",
       " 'The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in (Mola-Velasco 2010;Potthast, Stein, and Holfeld 2010).',\n",
       " 'Detailed analyses of both tasks have been published as CLEF Notebook Papers [3,6], which can be downloaded at www.',\n",
       " 'By performing 1 At PAN 2010 and 2011, a task on Wikipedia vandalism detection was organised (Potthast and Holfeld, 2011;Potthast et al., 2010c).',\n",
       " 'This software has been successfully used in the PAN International Competition on Wikipedia Vandalism Detection (Adler, de Alfaro, Mola-Velasco, Rosso, and West, 2010;Potthast et al., 2010c).',\n",
       " '(Potthast and Holfeld, 2011;Potthast, Stein, and Holfeld, 2010c) and authorship identification in 2011(Argamon and Juola, 2011) have been organised in PAN as well.',\n",
       " 'Potthast and Holfeld (2011);Potthast et al. (2010c) for an overview on automatic detection of this kind of \"contribution\".',\n",
       " '[24] as part of the vandalism detection competition associated with PAN 2010.',\n",
       " 'See [24] for additional details.',\n",
       " 'We chose to use the features of We chose to use the features of [6] as being representative of a solution focused on Language (L) features due to its top-place performance in the PAN 2010 competition [24].',\n",
       " 'Note that performance numbers reported forNote that performance numbers reported for[6] and[7] differ from those reported in[24] due to our use of 10-fold cross validation over the entire PAN2010 corpus and differences in ML models (e.',\n",
       " 'On the PAN-WVC-10 corpus, which has become fairly popular since, more recent work [20], [19] reported an accuracy of 96% (thus comparable to ours).',\n",
       " 'The interest in this aspect has been reflected in the holding of two International Competitions on Wikipedia Vandalism Detection in 2010 and 2011, in the context of the Cross Language Evaluation Forum Conferences, CLEFs [5,6].',\n",
       " 'In fact, a shared task organized by Potthast, Stein, and Holfeld (2010) resulted in plenty of approaches.',\n",
       " 'Otherwise, most papers on Wikipedia vandalism propose automatic vandalism detection tools: Potthast, Stein, and Gerling (2008) first developed machine learning technology for this purpose, and many of the approaches in existence today have been developed or derived from the results of two shared tasks at PAN 2010and PAN 2011(Potthast, Stein, and Holfeld 2010;Potthast and Holfeld 2011).',\n",
       " 'ROC-AUC has been a standard machine learning metric [16].',\n",
       " 'ROC-AUC has been a standard machine learning metric [16].',\n",
       " 'The interest in this aspect has been reflected in the holding of two International Competitions on Wikipedia Vandalism Detection in 2010 and 2011, in the context of the Cross Language Evaluation Forum Conferences, CLEFs [5,6].',\n",
       " 'Thus, here we are using data provided by Wikipedia, gathered using Mechanical Turk [13].',\n",
       " 'To help address this issue, Wikipedia has compiled a set of rules, the Wikipedia norms [13], to maintain and organize its content.',\n",
       " 'This use case is relevant because Wikipedia is an open and collaborative community with norms to maintain and organize its content [76], including the requirement to use proper writing style, refrain from removing content, avoid editing wars, and not engage in hate speech.',\n",
       " 'For system evaluation in English, our system (RGAI) participated in the third Web People Search Task challenge [1].',\n",
       " 'The third WePS shared task The third WePS shared task [1] introduced a novel subtask which sought to mine attributes for persons, i.',\n",
       " 'The previous value is maximal (1) when the number of shared categories is lower than or equal to the number of shared clusters, and it is minimal (0) when the two items do not share any category.',\n",
       " 'Web People Search (WePS) systems Web People Search (WePS) systems [3] are concerned with clustering the results of ambiguous name queries in order to distinguish between people with the same names.',\n",
       " 'Evaluation campaigns of the Web People Search task Evaluation campaigns of the Web People Search task [4,5,6,3] generally focus on ways of clustering documents into sets which characterize different persons that share the same name.',\n",
       " 'Our approach to processing Web data sets has been very much influenced by work done in the Web People Search (WePS) task Our approach to processing Web data sets has been very much influenced by work done in the Web People Search (WePS) task [4].',\n",
       " 'The second [9] and third [4] editions of the WePS campaign included an Attribute Extraction task [10], where personal information like date of birth, birthplace, occupation, and nationality had to be extracted from Web search results.',\n",
       " \"The clustering measures used in the first WePS campaign don't apply in MTT, but the more traditional recall and precision measures used in the later WePS-3 Attribute Extraction Task [4] are useful for measuring the performance of the nominal filter.\",\n",
       " 'The WePS (Web People Search) Evaluation cam- paign evaluated systems that extract personal attributes, including dates of birth, birth places, affiliations, occupations, and awards [1].',\n",
       " 'Our approach is based on the following processes: (1) creating curriculum vitae using related work Our approach is based on the following processes: (1) creating curriculum vitae using related work [1], (2) extracting the names of places where the person studied and worked from the vitae, (3) getting location information from the place names using Google Maps API, and (4) displaying a vitae that includes location information along with a map using Google Maps JavaScript API.',\n",
       " 'Recall = correct extracted place names correct place names to be extracted Recall = correct extracted place names correct place names to be extracted (2) Note that correct place names are duplicated.',\n",
       " 'WePS-3 conducted a competitive evaluation on person attribute extraction on web pages WePS-3 conducted a competitive evaluation on person attribute extraction on web pages [2].',\n",
       " \"There is also a workshop on web people searching, called WePS (standing for Web People Search) [1][2][3], that is concerned with organizing web results for a person's given name.\",\n",
       " 'Most similarity measurements (Artiles et al., 2010;Miller, 1995) could be used in the proposed method.',\n",
       " 'The WePS campaign (Artiles et al., 2010) introduced a task which sought to mine attributes for persons, i.',\n",
       " 'The third Web People Search (WePS3) The third Web People Search (WePS3) (Artiles et al., 2010) datasets were used for testing our English name disambiguation approach described in Section 6.',\n",
       " 'The third WePS shared task (Artiles et al., 2010) introduced a novel subtask which sought to mine attributes for persons, i.',\n",
       " 'Our system participated in the third WePS challenge Our system participated in the third WePS challenge (Artiles et al., 2010) and achieved top results on the person attribute extraction subtask.',\n",
       " 'When we investigated the manually annotated attributes in the training and test sets of the second WePS Campaign attribute extraction task (Artiles et al., 2010), we also found that the majority of attributes consisted of multiword named entities.',\n",
       " 'The author participated in the third WePS challenge (Artiles et al., 2010) and achieved top results on the person attribute extraction subtask.',\n",
       " 'There is a sub-task of WePS (Web People Search Workshop) (Artiles et al. 2010;Artiles et al.',\n",
       " 'The WePS-3 workshop was an evaluation Task under the scope of TebleCLEF (Artiles et al., 2010).',\n",
       " '5 Document annotation matrix from the TREC KBA track.',\n",
       " 'The slot filling task has been addressed at the Web People Search (WePS) evaluation campaign, targeting only persons The slot filling task has been addressed at the Web People Search (WePS) evaluation campaign, targeting only persons [5], and at the Knowledge Base Population track of the Text Analysis Conference (TAC KBP), targeting persons, organizations, and geo-political entities [37,45,74].',\n",
       " 'For details of the test collections, evaluation methodology, and evaluation metrics, we refer to the overview papers of the respective campaigns [5,74].',\n",
       " 'WePS-2/3 [11] conducted competitive evaluation on person attribute extraction on web pages.',\n",
       " 'WePS-2/3 conducted competitive evaluation on person attribute extraction on web pages [8].',\n",
       " 'For the microblog category, the collection was obtained from the trial and training data sets of the Task 2 of the well recognised international competition known as WePS-3 evaluation campaign For the microblog category, the collection was obtained from the trial and training data sets of the Task 2 of the well recognised international competition known as WePS-3 evaluation campaign [1].',\n",
       " 'However, research works dealing with the problem of word ambiguity (in this case, to determine whether a word refers to a company or not) have only been studied in literature recently (Amigo, Artiles, Gonzalo, Spina & Liu, 2010).',\n",
       " 'The works which have attempted a solution on the tweet categorization task as part of Task 2 of the WePS-3 evaluation campaign 4 for the online reputation management are summarised in The works which have attempted a solution on the tweet categorization task as part of Task 2 of the WePS-3 evaluation campaign 4 for the online reputation management are summarised in (Amigo et al., 2010).',\n",
       " 'According to [46], the classifier described in [45] outperformed all other competing classifiers in the WePS-3 evaluation campaign.',\n",
       " \", understanding whether the mention of an entity like 'apple' refers to the fruit or to the company) [3] by exploiting the content generated by users on other social networks.\",\n",
       " 'As these company profiles were developed in the context of the WePS3 task [3], we assume that their accuracies are bounded by the values in the first row of Table III.',\n",
       " 'Several works Several works [3], [28], [29] have addressed the problem of tweet classification in various contexts.',\n",
       " 'The results of all the participant systems can be seen in (Amig√≥ et al. 2010) .',\n",
       " 'In that table we can also see other metrics like precision, recall, and F-measure for related tweets (+) and unrelated tweets(-), all of them, defined in (Amig√≥ et al. 2010).',\n",
       " 'We evaluated our methods on two different datasets: the \"WWW\" dataset We evaluated our methods on two different datasets: the \"WWW\" dataset [9], that was used as a benchmark in a number of entity resolution methods and the WePS dataset [3].',\n",
       " 'We evaluated our methods on two different datasets: the \"WWW\" dataset We evaluated our methods on two different datasets: the \"WWW\" dataset [9], that was used as a benchmark in a number of entity resolution methods and the WePS dataset [3].',\n",
       " 'Most CLIR research has been focused on the cross-lingual matching function although the recent CLEF conferences have initiated some work on the second function: (partial) translation for document selection (Oard & Gonzalo, 2002).',\n",
       " 'The interactive CLEF experimental framework The interactive CLEF experimental framework (Gonzalo & Oard, 2002) was followed, but additional measurements (both objective and subjective) were recorded.',\n",
       " \"El objetivo del iCLEF'2002 El objetivo del iCLEF'2002 (Gonzalo and Oard, 2002) fue proporcionar un marco de referencia com√∫n para realizar experimentos comparando dos sistemas de recuperaci√≥n de informaci√≥n transling√ºe que permitan a un usuario que desconoce el idioma de los documentos realizar una expansi√≥n interactiva de la consulta, una selecci√≥n interactiva de documentos (al igual que el a√±o anterior), o ambas opciones a la vez.\",\n",
       " '(Oard & Gonzalo, 2002)).',\n",
       " 'A special task was designed this year for interactive user experiments A special task was designed this year for interactive user experiments (Oard and Gonzalo 2001).',\n",
       " 'For the 67  Training Protocol We train a single encoder-only transformer model with a separate classification head for each of the 107 tasks.',\n",
       " 'The use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing The use of auxiliary classifiers at every node of the decision tree is not feasible when the hierarchical tree is huge, such as the large hierarchical terminologies for medical literature indexing (Gasco et al., 2021).',\n",
       " 'For the Spanish paper, we use paper abstracts open sourced by the Mesinesp (Gasco et al., 2021).',\n",
       " ', CheckThat 2021 Cross-Language Evaluation Forum (CLEF) [41,42].',\n",
       " ', 2018;Shaar et al., 2021) largely ignore relevant attributes of the claim (e.',\n",
       " 'e, tweets containing verifiable claims that we think will be of general interest Shaar et al. (2021), and consider them as authority supporting tweets.',\n",
       " 'To further verify the effectiveness of our method, we conduct a comparison on the document-level CE dataset (CLEF-2021, subtask 1B To further verify the effectiveness of our method, we conduct a comparison on the document-level CE dataset (CLEF-2021, subtask 1B (Shaar et al., 2021)) using our method and Claimbuster.',\n",
       " 'More sophisticated representation techniques, such as LIWC [10] and ELMo [11], have also been investigated.',\n",
       " 'initiative [5][6][7], that has advanced research in this field by producing benchmarks and baselines and organizing shared tasks.',\n",
       " 'Lab [6,7,25].',\n",
       " 'Lab [6,7,25].',\n",
       " 'Lab Task 2 (verified claim retrieval ) challenges of the CLEF2020 [25], CLEF2021 [6] and CLEF2022 [7] initiatives in English language.',\n",
       " ', CheckThat 2021 Cross-Language Evaluation Forum (CLEF) [41,42].',\n",
       " 'We examine three datasets: the widely-used LIAR (Wang, 2017), CT-FAN-22 We examine three datasets: the widely-used LIAR (Wang, 2017), CT-FAN-22 (K√∂hler et al., 2022) that contains both English and German corpora, and a new dataset LIAR-New.',\n",
       " 'Second, we use the CT-FAN-22 dataset (K√∂hler et al., 2022) for additional tests, including transfer and multilingual settings.',\n",
       " 'The state-of-the-art approaches are reported by The state-of-the-art approaches are reported by K√∂hler et al. (2022) from a competition run on this dataset.',\n",
       " 'Recent works Recent works [58,70,86,38,22] have prominently recognized evidence as a premier element in fake news detection apart from patterns.',\n",
       " 'Finally, different challenges have been organized lately for named entity recognition and relation extraction in Spanish biomedical texts Finally, different challenges have been organized lately for named entity recognition and relation extraction in Spanish biomedical texts [19,20].',\n",
       " 'However, only recently studies have been carried out in the health domain, also due to the growing impact of Consumer Health Search (CHS) [24,25].',\n",
       " 'VQA-MED-2018 [9], VQA-RAD [14], VQAMED-2019 [4], RadVisDial [13], PathVQA [11], VQA-MED-2020 [3], SLAKE [18], and VQA-MED-2021 [5].',\n",
       " 'The VQA-MED-2020 dataset represented the third iteration of this influential initiative, presented as part of ImageCLEF 2020 to advance the answer to medical visual questions [278].',\n",
       " ', 2020), VQA-Med-2021(Ben Abacha et al., 2021), and MIMIC-Diff-VQA (Hu et al.',\n",
       " 'In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.',\n",
       " 'Motivated by this, we extend our proposed model Motivated by this, we extend our proposed model [9] for the ImageCLEFmedical 2021 [10], [11] by adding an explainability module.',\n",
       " 'We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset We used for our medical image captioning model evaluation, the ImageCLEFmed 2021 dataset [10], [11], which includes three sets: the training set composed of 2756 medical images; the validation set and the test set consisting of 500 and 444 radiology images, respectively.',\n",
       " ', 2022;Pelka et al., 2021;Hsu et al.',\n",
       " \"To the best of our knowledge, no existing figure-caption datasets explicitly contain the figures' accompanying documents (Pelka et al., 2021;Hsu et al.\",\n",
       " 'The ROCO dataset has been used in the medical caption tasks The ROCO dataset has been used in the medical caption tasks [3][4][5][6] at the Image Retrieval and Classification Lab of the Conference and Labs of the Evaluation Forum (ImageCLEF) 7 .',\n",
       " 'For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.',\n",
       " '[20] Natural Images 50,000 10,000 Tiny ImageNet [21] Natural Images (ImageNet subset) 100,000 10,000 Stanford dogs [22] Natural Images (Dog breeds) 12,000 8,580 Flowers-102 [23] Natural Images (Flower species) 2,040 6,149 CUB-200-2011 [24] Natural Images (Bird species) 5,994 5,794 Stanford Cars [25] Natural Images (Car models) 8,144 8,041 Food-101 [26] Natural Images (Food categories) 75,750 25,250 DTD [27] Texture Images 1,880 1,880 47 NEU Surface Defects [28] Surface Defect Images 1,440 360 6 UC Merced Land Use [29] Remote Sensing Images 1,680 420 21 EuroSAT [30] Remote Sensing Images 18,900 8,100 10 PlantVillage [31] Plant Images 44,343 11,105 39 PlantCLEF [32] Plant Images 10,455 1135 20 Galaxy10 DECals [33] Astronomy Images (Galaxy Morphology) Stanford Dogs [22]: Stanford Dogs dataset is a comprehensive dataset for fine-grained image classification, containing 20,580 images of 120 different dog breeds.',\n",
       " 'PlantCLEF PlantCLEF [32]: The PlantCLEF dataset is a large-scale dataset for plant identification, comprising millions of images covering thousands of plant species, including trees, flowers, fruits, and leaves.',\n",
       " 'More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning More recent BirdCLEF efforts to identify species from bird vocalizations have utilized deep learning [13,17].',\n",
       " 'Participants in recent iterations of BirdCLEF have relied heavily on DCNNs, making use of transfer learning and data augmentation [17].',\n",
       " 'Participants in recent iterations of BirdCLEF have relied heavily on DCNNs, making use of transfer learning and data augmentation [17].',\n",
       " 'Common to pipelines evaluated in BirdCLEF competition are specific post-processing techniques such as filtering by time and location of recording, boosting, and ensembling [17].',\n",
       " \"Dataset yang digunakan pada penelitian ini berasal dari dataset ESC-50 Dataset yang digunakan pada penelitian ini berasal dari dataset ESC-50 [8] untuk suara gergaji mesin, UrbanSound8K [9] untuk suara tembakan senjata, dan BirdClef-2021 [10] untuk 8 jenis suara burung (Wild Turkey, Black-bellied Plover, American Coot, Great Crested Flycatcher, Townsend's Solitaire, Ruddy Turnstone, Common Chlorospingus, dan Black-andwhite Warbler).\",\n",
       " 'The SSL pretraining used the BirdCLEF2021 dataset from Kaggle challenge The SSL pretraining used the BirdCLEF2021 dataset from Kaggle challenge [15], containing ‚àº 63 k recordings of various lengths from the Xeno-Canto public repository of bird sound recordings [16].',\n",
       " 'However, the precondition for employing deep learning is the availability of adequate datasets 4 .',\n",
       " 'Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification.',\n",
       " 'Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].',\n",
       " ', 2018), and snakes (Picek et al., 2021).',\n",
       " 'Therefore, we will try to handle all these drawbacks in our future works 27,28 .',\n",
       " 'Other paper used CNN and InceptionV3 to achieve an accuracy of 90% for 2 snake classes [13].',\n",
       " ', 2018(Kestemont et al., , 2019(Kestemont et al.',\n",
       " ', 2018(Kestemont et al., , 2019(Kestemont et al.',\n",
       " '–û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ [7].',\n",
       " '–ú–Ω–æ–≥–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç, —Å—Ç–∞–≤—à–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: —á–∞—Å—Ç–æ—Ç—ã —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å–ª–æ–≤, n-–≥—Ä–∞–º–º—ã —Å–ª–æ–≤, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ [7].',\n",
       " 'These include work on controlled corpora of blog, emails and interviews (Barlas and Stamatatos 2020), and fanfiction (Kestemont et al. 2020).',\n",
       " 'We consider such test (Kipnis, 2020) in the context of the authorship verification challenge of (Kestemont et al., 2020).',\n",
       " '2020;2021) for AV that was shown to perform well at previous PAN authorship verification shared tasks (Kestemont et al. 2020).',\n",
       " '‚Ä¢ In the 2020 PAN authorship verification task [15], methods employing neural networks made an appearance, particularly in the form of Siamese neural networks.',\n",
       " 'At the 2021 Conference and Labs of the Evaluation Forum (CLEF), for instance, 66 academic teams participated in the task of directly detecting whether a Twitter user is likely to spread hate or not [51].',\n",
       " 'At the 2021 Conference and Labs of the Evaluation Forum (CLEF), for instance, 66 academic teams participated in the task of directly detecting whether a Twitter user is likely to spread hate or not [51].',\n",
       " 'Several works have been used in this category, such as the author profiling of the Hate Speech Spreader Detection shared task organized by PAN 2021 [21], char/word-IDF [22,23], and Vader/RoBERTa word embeddings [24], trigram features and POS tags [25][26][27], n-grams [28][29][30][31][32], and emotions [26,33].',\n",
       " 'Shallow lexical features [25], dictionaries [146], sentiment analysis [147], linguistic characteristics [148], knowledge-based features [149], and meta-information [21] were described in the literature as features connected to tweets.',\n",
       " '0% for tweets in Spanish (Rangel et al., 2021).',\n",
       " 'The text simplification task (Ermakova et al. 2021) is a promising research direction for providing simplified explanations.',\n",
       " ', 2020b(Bondarenko et al., , 2021) ) featured a related track.',\n",
       " ', 2019;Bondarenko et al., 2021).',\n",
       " ', underlying reasons -sometimes called perspectives [18,21], premises [13,26] or frames [3,47]).',\n",
       " ', 2017b;Bondarenko et al., 2021), argument analysis (Feng and Hirst, 2011;Janier et al.',\n",
       " ', 2019;Bondarenko et al., 2021) or a suitable counter-argument given an input argument (Wachsmuth et al.',\n",
       " ', 2018), during argument retrieval, (Bondarenko et al., 2021) or in the legal context for case law retrieval (Locke and Zuccon, 2018).',\n",
       " ', 2018), and Webis-Touch√© (Bondarenko et al., 2021).',\n",
       " 'Webis-Touch√© 2020 Webis-Touch√© 2020 (Bondarenko et al., 2021) is an argument retrieval dataset based on the args.',\n",
       " 'me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.',\n",
       " 'me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.',\n",
       " 'me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.',\n",
       " 'me -Touch√© (Bondarenko et al., 2021(Bondarenko et al.',\n",
       " \"Enfin, les usagers √©taient dispos√©s √† soumettre de nouveau leur requ√™te ou √† la reformuler et √† examiner un bon nombre d'images, afin de trouver ce qu'ils cherchaient (Clough, Sanderson et M√ªller, 2004).\",\n",
       " 'As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2004 track As part of the Cross Language Evaluation Forum (CLEF), the ImageCLEF 2004 track [17] that promotes cross language image retrieval has initiated a new medical retrieval task in 2004.',\n",
       " '3 √ó 1 and 1 √ó 3), we set the similarity to zero if the difference in a grid dimension between two image indexes   3 compares the MAPs of the automatic VisMed run with those of the top 5 automatic runs as reported in ImageCLEF 2004 [17] where the percentages of improvement are shown in brackets, \"RF\" stands for the use of pseudo relevance feedback and \"Text\" means the case notes were also utilized.',\n",
       " 'Using the ImageCLEF 2004 CasImage medical database and retrieval task, we have demonstrated the effectiveness of our framework that is very promising when compared to the current automatic runs [17].',\n",
       " 'Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.',\n",
       " 'ImageCLEF 6 [5,6] has started within CLEF 7 (Cross Language Evaluation Forum [38]) in 2003 with the goal to benchmark image retrieval in multilingual document collections.',\n",
       " 'It is worth noting that image classification is a well studied area with several standard measures developed for the evaluation of automated classification It is worth noting that image classification is a well studied area with several standard measures developed for the evaluation of automated classification [26][27][28].',\n",
       " 'It was created for evaluation on the image track of the Cross Language Evaluation Forum [5].',\n",
       " 'As a benchmark project, ImageCLEF is more and more wellknown with its open data platform As a benchmark project, ImageCLEF is more and more wellknown with its open data platform [12].',\n",
       " 'One of the first medical multimedia retrieval challenges was ImageCLEF that started with a medical task in 2004 [60].',\n",
       " '[9] for instance already shared a number of issues with KVQAE, such as multimodal information fusion.',\n",
       " 'Research on SCR initially investigated IR for planned speech content such as news broadcasts and documentaries [1], [2].',\n",
       " 'Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].',\n",
       " \"Pour le moment, les syst√®mes de QR bilingues sont beaucoup moins performants que les syst√®mes de QR monolingues, mais les chercheurs se disent encourag√©s par les r√©sultats obtenus jusqu'√† maintenant (Vallin et al, 2005).\",\n",
       " 'Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 Multilingual tasks such as IR and Question Answering (QA) have been recognized as an important issue in the on-line information access, as it was revealed in the the Cross-Language Evaluation Forum (CLEF) 2006 [6].',\n",
       " 'Besides, section 5 presents and discusses the results obtained using all official English questions of QA CLEF 2004 [8] and 2006 [6].',\n",
       " 'This fact has been confirmed in the last edition of CLEF 2006 [6].',\n",
       " 'Nowadays, at CLEF 2006 [6], three different approaches are used by CL-QA systems in order to solve the bilingual task.',\n",
       " 'Furthermore, this affirmation is corroborated checking the official results on the last edition of CLEF 2006 [6] where our method [3] has being ranked first at the bilingual English-Spanish QA task.',\n",
       " '07% at CLEF 2006) and than other current bilingual QA systems [6].',\n",
       " 'BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006).',\n",
       " 'For this purpose we have used the CLEF 2006 set of questions, the EFE corpora, the evaluation measures34 proposed by the CLEF organization (Magnini et al, 2006) and our official results in this competition.',\n",
       " 'The Cross-Language Evaluation Forum or CLEFThe Cross-Language Evaluation Forum or CLEF3 , established in 2000, promotes multilingual question answering, where the question is posed in a different language than the language of the documents in the repository [73].',\n",
       " 'We compared the results of the best systems of both EQueR and CLEF QA task in 2004 (Valin et al., 2004) and found them to be consistent.',\n",
       " 'We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.',\n",
       " ', , 2004;;Vallin et al., 2005;Magnini et al.',\n",
       " \"Enfin, les r√©sultats de 2005 soulignent le besoin d'offrir une interface donnant plus de contr√¥le √† l'individu pour la formulation et la reformulation des requ√™tes (Clough et al, 2005).\",\n",
       " 'The test collection ImageCLEFMed 2005 contains Casimage (9000 images), MIR (2000 images), PEIR (33000 images), and PathoPic (9000 images) (Clough et al., 2005).',\n",
       " \"Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval Working across a number of European languages -and in the case of one research group, Chinese -it was shown that cross language retrieval was operating at somewhere between 50% and 78% of monolingual retrieval (Clough, 2003), confirming Flank's conclusion that image CLIR can be made to work.\",\n",
       " 'As part of preparations for the formation of the imageCLEF collection As part of preparations for the formation of the imageCLEF collection (Clough and Sanderson, 2003), a preliminary evaluation of image CLIR was conducted on the St.',\n",
       " 'The IRMA 10000 databaseThe IRMA 10000 database1 was used in the automatic annotation task of the 2005 ImageCLEF evaluation [17].',\n",
       " 'Table 1 gives an overview of the best results obtained for the IRMA tasks from the ImageCLEF 2005 evaluation [17] along with the results we obtained using sparse patch histograms with and without position information.',\n",
       " 'The St Andrews collection has been used for the past three years at ImageCLEFThe St Andrews collection has been used for the past three years at ImageCLEF5 , the cross-language image retrieval task (Clough, M√ºller, Hersh, Deselaers, Lehmann, Grubinger, 2005;Clough, M√ºller, Sanderson, 2005;Clough and Sanderson, 2003).',\n",
       " 'Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.',\n",
       " 'ImageCLEF is another image benchmarking competition that is part of the Cross Language Evaluation Framework (CLEF) competition (Clough et al. 2004).',\n",
       " 'Another recent example is where users complement their traditional keyword query with additional information, such as example documents [24], tags [73], images [76,91], categories [338], or their search history [20].',\n",
       " 'Accordingly, this technique is less time-consuming compared to the technique that depends on texts for the purposes of indexing and retrieving [5].',\n",
       " 'The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].',\n",
       " 'The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.',\n",
       " 'Spreading from textual search evaluations [25], evaluation campaigns have been established for image retrieval [9], as well as for video content search [69].',\n",
       " 'At other times, corpora is automatically obtained (for instance, English and Czech interview recordings of Survivors of the Shoah Visual History Foundation using in CL-SDR 2006 [11] were transcribed using a ASR system with the consequent increase of transcription errors).',\n",
       " '2004), oft-recorded in noisy environments, mean WER for the 2006 ASR transcripts is reported as 25% (Pecina et al. 2008).',\n",
       " 'These results are confirmed for a very different retrieval task of unstructured oral testimonies in the speech retrieval task introduced at CLEF 2005 [6].',\n",
       " 'We are currently exploring the use of our document correction method for the CLEF speech retrieval task based on oral testimonies [6].',\n",
       " 'While best retrieval accuracy was achieved using a monolingual evaluation task where the queries were English, our results for the cross language task were the best among those making formal submissions to the CLEF 2007 CL-SR task, showing the lowest decrease relative to monolingual performance when queries were translated from their source language to English [12].',\n",
       " 'The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) The Spoken Document Retrieval track in CLEF evaluation campaign uses a corpus of spontaneous speech for cross-lingual speech retrieval (CL-SR) [11,13].',\n",
       " 'For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) For the search sub-task, three metrics were used to evaluate the submissions of the participants: mean reciprocal rank (MRR), mean generalized average precision (mGAP) [22] and mean average segment precision (MASP) [10].',\n",
       " 'The data is drawn from the Cross-Language Speech Retrieval Track of the Crosslanguage Evaluation Forum (CLEF CL-SR) (Pecina et al., 2007) collection.',\n",
       " 'The transcripts produced with Automatic Speech Recognition (ASR) systems tend to contain many recognition errors, leading to low Information Retrieval (IR) performance [1] unlike the retrieval from broadcast speech, where the lower word error rate did not harm the retrieval [2].',\n",
       " 'Since the RSR 2011 task was a known-item search, one useful evaluation metric is the Mean Reciprocal Rank (MRR); additionally we apply a metric that evaluates the ranking and takes account of the distance between the predicted and actual jump-in point (mean Generalized Average Precision (mGAP)) [15].',\n",
       " 'mGAP [24] awards runs that not only find the relevant items earlier in the ranked output list, but also are closer to the jump-in point of the relevant content.',\n",
       " 'More recently in 2005 the Cross-Language Evaluation Forum (CLEF) has started a speech retrieval track on spoken interviews [33].',\n",
       " 'Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.',\n",
       " ', 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007;Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b).',\n",
       " 'Moreover, user studies have repeatedly revealed that in interactive applications real users would often willingly trade some potential of retrieval effectiveness if doing so would lead to more understandable and predictable system behavior [Zhang et al. 2007].',\n",
       " 'Evaluating search systems using predefined passages is at best an imperfect model of the real task faced by a speech retrieval system, but work on alternative evaluation designs has started only recently [Pecina et al. 2007].',\n",
       " 'In this paper we report on work carried out for the Cross-Language Evaluation Forum (CLEF) 2005 Cross-Language Speech Retrieval (CL-SR) track (White et al, 2005).',\n",
       " 'See (Oard et al, 2004) and (White et al, 2005) for details.',\n",
       " 'In all experiments, we used the training dataset from the CLEF CL-SR 2007 task [18].',\n",
       " 'The focus then shifted towards spoken content that is produced spontaneously such as interviews, lectures and TV shows [3].',\n",
       " 'These videos were provided by the CLEF 2 speech retrieval tracks [3].',\n",
       " 'These videos were provided by the CLEF 2 speech retrieval tracks [3].',\n",
       " 'We used the retrieval model PL2 and QE model BO1 described in the previous section (see Equations 1,3) to calculate the results shown in Table III, which shows performance in terms of Mean Average Precision (MAP), Recall and Precision for top 10 documents (P@10).',\n",
       " 'Performance quickly improved with the yearly evaluations produced by the Cross-Language Evaluation Forum (CLEF) [9,26].',\n",
       " '(2) spoken document retrieval: podcasts can be represented by their transcripts of their spoken content -in this way podcast search is related to spoken document retrieval [28], [3], [62].',\n",
       " 'Retrieval from an archive of oral history has also been well-studied [15], but lacks the multispeaker, multi-genre aspect of podcasts.',\n",
       " 'To deal with these issues, a lot of research has been done in recent years, and the output has been presented in different styles, including research papers [7], books [8,9], doctoral dissertations [10,11], test collections [12], retrieval evaluation events [13], etc.',\n",
       " 'Procedure-1 computes the BM25 score of all the papers against the initial search query, which is combined with the citation analysis score to compute the final base weight of the candidate papers and ranks the initial search results in steps [6][7][8][9][10][11][12][13][14].',\n",
       " 'For example, the overviews of the last CLEF workshops report figures where the MAP of a bilingual IRS is around 80% of the MAP of a monolingual IRS for the main European languages [1,2,3].',\n",
       " 'As pointed out by As pointed out by [4], a statistical methodology for judging whether measured differences between retrieval methods can be considered statistically significant is needed and, in line with this, CLEF usually performs statistical tests on the collected experiments [1,2] to assess their performances.',\n",
       " 'The experimental collections and experiments used are fully described in The experimental collections and experiments used are fully described in [1,2].',\n",
       " 'It was then used for producing reports and overview graphs about the submitted experiments [1,3].',\n",
       " 'To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are To address this issue, evaluation forums have traditionally carried out statistical analyses, which provide participants with an overview analysis of the submitted experiments, as in the case of the overview papers of the different tracks at TREC and CLEF; some recent examples of this kind of papers are [12,13].',\n",
       " 'This is the case, for example, of the CLEF 2005 multilingual merging track [12], which provided participants with some of the CLEF 2003 multilingual experiments as list of results to be used as input to their merging algorithms.',\n",
       " ', 2005;Nunzio et al., 2005).',\n",
       " 'Several QA reports [6,14] indicate that the translation errors cause an important drop in accuracy for cross-language tasks with respect to the monolingual exercises.',\n",
       " 'Recently there has been intensive research in this area, fostered by evaluation-based conferences such as the Text REtrieval Conference (TREC) (Voorhees, 2001b), the Cross-Lingual Evaluation Forum (CLEF) (Vallin et al., 2005), and the NII-NACSIS Test Collection for Information Retrieval Systems workshops (NTCIR) (Kando, 2005).',\n",
       " 'QA can be open-domain QA can be open-domain [7,8] or closed-domain [9].',\n",
       " 'Voorhees, 2005;Vallin, 2005).',\n",
       " 'We kept the datasets of CLEF 2004 and 2005 (cf [14] and [19]) that concern newspapers from 1994 and 1995.',\n",
       " ', , 2004;;Vallin et al., 2005;Magnini et al.',\n",
       " 'A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in [1].',\n",
       " '(GBIF) Train and test occurrences datasets from the previous year edition [5] were merged to feed the current challenge.',\n",
       " 'Many recent works, in particular those involved in recent bird classification Many recent works, in particular those involved in recent bird classification [39] and bird audio detection evaluations [40], have focused on the use of CNNs.',\n",
       " 'It was shown in previous editions of BirdCLEF [35,36] that systems for identifying birds from mono-directional recordings are now performing very well and several mobile applications implementing this are emerging today.',\n",
       " 'Convolutional Neural Networks performed well in other fine-grained species identification tasks, including plant species classification [11,12], dog classification [19], bird classification [35,36], or classification of species in general [33].',\n",
       " 'For the FGVCx Fungi Classification challenge, we trained an ensemble of Inception-v4 and Inception-ResNet-v2 networks [28], inspired by the winning submission in the ExpertLifeCLEF plant identification challenge 2018 [11].',\n",
       " 'Other recent research by [24,25] focused on investigating how automated methods for identifying species from imagery data compare to manual annotations performed by citizen scientists.',\n",
       " 'Further, research on performing large-scale experiments on automatic approaches for identifying species from imagery data collected from citizen science portals revealed that automated species classification performs similarly and sometimes even better than manual annotations performed by citizen scientists [24,25,29].',\n",
       " 'In particular the authors of [24] presented a study encompassing 10,000 plant species while the authors of [25] performed a study for more than 5,000 categories of plants, animals, and fungi.',\n",
       " 'Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].',\n",
       " 'For the FGVCx Fungi Classification challenge, we trained an ensemble of six models (listed in Table 2) based on Inception-v4 and Inception-ResNet-v2 architectures [65], and inspired by our winning submission in the ExpertLifeCLEF plant identification challenge 2018 [41].',\n",
       " 'To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data To evaluate the methods on practical tasks with prior shift, we experiment with fine-grained plant classification on the PlantCLEF data [7,8] and with learning to classify ImageNet [19] classes from a long-tailed noisy training dataset downloaded from the web, Webvision 1.',\n",
       " 'These advances allowed large-scale experiments and even the construction of models that surpassed human performance in some plant recognition tasks [Go√´au et al. 2018].',\n",
       " 'For plant recognition, some datasets are available including PlantCLEL [10][11][12][13][14][15], Pl@ntNet [16], iNaturalist [17], etc.',\n",
       " 'In the arms race between authorship attribution and authorship obfuscation, it is important that both attribution and obfuscation consider the adversarial threat model In the arms race between authorship attribution and authorship obfuscation, it is important that both attribution and obfuscation consider the adversarial threat model (Potthast et al., 2018).',\n",
       " 'The research of user profiling identification and detection for Arabic language is even more scarce [9], [10].',\n",
       " 'To address this situation, one potential benchmark setting is to require participating teams to submit or upload the used executable processing pipelines that generate automatic results [6].',\n",
       " 'Twitter verisi √ºzerinden cinsiyet tahminlemesi konusunda CLEF 2018 kapsamƒ±nda PAN 2018 c ¬∏alƒ±s ¬∏tayƒ±nda ortak bir c ¬∏alƒ±s ¬∏ma Twitter verisi √ºzerinden cinsiyet tahminlemesi konusunda CLEF 2018 kapsamƒ±nda PAN 2018 c ¬∏alƒ±s ¬∏tayƒ±nda ortak bir c ¬∏alƒ±s ¬∏ma [3] d√ºzenlenmis ¬∏tir.',\n",
       " ', 2017;RANGEL et al., 2018).',\n",
       " 'Only for Arabic it was possible to improve accuracy (albeit less than 2%) [21].',\n",
       " \"For instance, more recently, during PAN 2018 [4], multiple techniques were explored to predict user's gender on twitter posts including images, the best result was reported by Takahashi et al.\",\n",
       " \"For instance, more recently, during PAN 2018 [4], multiple techniques were explored to predict user's gender on twitter posts including images, the best result was reported by Takahashi et al.\",\n",
       " 'This traditional techniques try to analyze the semantics and relationship between words [4,7] using techniques like word2vec, bag of words or TF-IDF [8].',\n",
       " 'in [4], the CNN with 16 layers, VGG, performs well for age or gender prediction on social network images.',\n",
       " 'The use of the weights of a pre-trained network to initialize a new CNN is a technique that has reported good results for age-gender prediction The use of the weights of a pre-trained network to initialize a new CNN is a technique that has reported good results for age-gender prediction [4,5].',\n",
       " 'The latter can be confirmed by reviewing the results from the PAN1 competitions (Rangel et al., 2018), where the best-performing systems employed content-based features for representing documents regardless of their genre.',\n",
       " 'However, few approaches and little labeled data exists for this task for languages other than English, with some exceptions [16,32,62].',\n",
       " 'As part of the PAN at CLEF initiative [1] there have been multiple author profiling challenges in the past years, which generated a substantial body of research in this field, summarized in [18], [20] and [19].',\n",
       " 'Similar to our research, some studies have developed classifiers to identify the gender or age of users in text data [55] and in social media such as Twitter [56,57], Sina Weibo [58], Facebook [59], and Netlog [60].',\n",
       " \"Rangel and colleagues Rangel and colleagues [11] point out that due to the huge amount of information available on social networking platforms, it is possible to obtain information about different attributes such as gender, age, personality, native language, or political orientation from the analysis of an author's profile.\",\n",
       " 'Theoretical and empirical studies have demonstrated a strong relationship between social factors and linguistic attitudes, since language is perceived as a social activity that reflects and influences social reality Theoretical and empirical studies have demonstrated a strong relationship between social factors and linguistic attitudes, since language is perceived as a social activity that reflects and influences social reality [11,27].',\n",
       " 'However, at present time, researchers mainly focus on digital social networks, where language is more spontaneous and less formal [11].',\n",
       " 'Consequently, social activities represent a great challenge for the selection and identification of the user profile, which is caused mainly by the diversity of texts and complex social structures Consequently, social activities represent a great challenge for the selection and identification of the user profile, which is caused mainly by the diversity of texts and complex social structures [11,29,30].',\n",
       " 'PAN2013 [2,3] ten itibaren bu g√∂revler i√ßerisine giren cinsiyet tespiti bug√ºn bile √∂nemini korumaktadƒ±r.',\n",
       " 'Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.',\n",
       " 'Yazar profili olu≈üturma g√∂revleri ya≈ü ve cinsiyet belirleme [3,8], yazarƒ±n bot olup olmadƒ±ƒüƒ±nƒ±n tespiti [9], bir kullanƒ±cƒ±nƒ±n sahte haber yaymaya istekli olup olmadƒ±ƒüƒ± [1], metinin yanƒ±nda g√∂r√ºnt√º yardƒ±mƒ±yla cinsiyet tespiti [3] gibi alt g√∂revlerden olu≈üur.',\n",
       " 'This research line is very well summarized by Ikae and Savoy in [13] and has as corner stone the author profiling task at PAM-CLEF competitions [14,15,16,17,18,19,20].',\n",
       " 'Among the 19 studies that used previously annotated data sets, nine 34,36,37,47,49,65,66,75,100 used corpora from the PAN-CLEF author profiling tasks [102][103][104][105][106][107][108] , while ten studies 51,54,62,64,72,83,84,94,96,101 relied on data sets from other studies 92,[109][110][111][112][113][114][115] .',\n",
       " 'A longstanding shared task focused on author profiling has been hosted at the PAN workshop at the Conference and Labs of the Evaluation Forum (CLEF) [102][103][104][105][106][107][108] .',\n",
       " 'The PAN 2015, 2016, 2017 and 2018 datasets The PAN 2015, 2016, 2017 and 2018 datasets [19,20] consisting of tweets in different languages.',\n",
       " 'Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions Existing Datasets To our knowledge, the only (partially) available real-world datasets are published by the recurring PAN competitions [11,18,19], require manual vetting, and often need to be reconstructed from pointers to proprietary APIs (e.',\n",
       " 'In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection In this section, we report previous works that have been done for the intrinsic plagiarism detection task, and other related tasks, such as Style Breach detection [9], and Style Change Detection [7].',\n",
       " 'Concerning the style change detection sub-task, we chose as a baseline model, the method of Zlatkova Concerning the style change detection sub-task, we chose as a baseline model, the method of Zlatkova [2] that is state-of-theart on PAN 2018 Competition [7].',\n",
       " \"For the next edition, we shall continue working with 'fanfiction' For the next edition, we shall continue working with 'fanfiction' [10,11].\",\n",
       " 'However, this task was deemed as highly complex and hence, was relaxed in 2018, asking participants to predict whether a given document is single-or multi-authored [11].',\n",
       " '2, n-grams models are still the most widely used state-of-art methods used in authorship attribution and provide overall good and stable results, for example, in the 2018 PAN-challenge [47].',\n",
       " 'In a 2018 authorship attribution competition [12], \"n-grams were the most popular type of features to represent texts in\" one of the primary tasks in the competition.',\n",
       " 'Here, we describe our approach, which powered the winning system Here, we describe our approach, which powered the winning system [20] for the PAN@CLEF 2018 task on Style Change Detection [7].',\n",
       " 'We used data provided by the organizers of the CLEF-2018 PAN task on Style Change DetectionWe used data provided by the organizers of the CLEF-2018 PAN task on Style Change Detection5  [7], which was based on user posts from StackExchange covering different topics with 300-1,000 tokens per document.',\n",
       " 'For NLP researchers, fanfiction provides a large source of literary text with metadata, and has already been used in applications such as authorship attribution (Kestemont et al., 2018) and character relationship classification (Kim and Klinger, 2019).',\n",
       " 'Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and G√≥mez-Rodr√≠guez, 2019), finegrained entity typing (Chu et al.',\n",
       " 'At the other extreme, the high performance of applying machine learning on authorship verification and attribution (Kestemont et al., 2018(Kestemont et al.',\n",
       " 'Despite the overwhelming success of deep learning, traditional features are still effective in authorship analysis (Kestemont et al., 2018(Kestemont et al.',\n",
       " ', 2015;Kestemont et al., 2018).',\n",
       " 'The PAN18-FF dataset The PAN18-FF dataset (Kestemont et al., 2018) consists of fan fiction prose texts written by admirers of authors, novels, TV shows, movies, etc.',\n",
       " '75 -the second-best score reported for this part of the challenge; see (Kestemont et al., 2018).',\n",
       " 'Up until the very recent PAN 2018 and PAN 2020 Authorship event [3,4], the most popular and effective approaches still largely relies on n-gram features and traditional machine learning classifiers, such as support vector machines (SVM) [5] and trees [6].',\n",
       " 'As described by Kestemont et al. (2018), the shared task of cross-fandom AA presents a collection of fan fiction writings from different domains and aims to detect authors for unseen documents.',\n",
       " '‚Ä¢ In the 2018 PAN authorship attribution task [14], features continued to focus on characters and word n-grams, with various weighting and normalization methods.',\n",
       " '2020;Kestemont et al. 2018).',\n",
       " 'Another is to build statistical models that can predict and compensate for the issues arising from the mismatches (Daum√© 2009;Daum√© and Marcu 2006;Kestemont et al. 2018).',\n",
       " 'Various methods have been applied to the task, ranging from SVM based approaches, such as (Kestemont et al., 2018), to transformer based models, like (Bauersfeld et al.',\n",
       " 'The third and final dataset, The third and final dataset, PAN-2018(Kestemont et al., 2018), contains medium-length texts of around 800 words each, centered on fan fiction.',\n",
       " ', 2022) and (Kestemont et al., 2018).',\n",
       " ', 2022) and (Kestemont et al., 2018).',\n",
       " ', 2022) and (Kestemont et al., 2018).',\n",
       " 'It is shown that this automatically identifying the intended sense of ambiguous words improves the performance of clinical and biomedical applications such as medical coding and indexing, 12,13 detection of adverse drug event, 14 automatic medical reporting, 15,16 and other secondary uses of data such as information retrieval and extraction, 17 and question-answering systems.',\n",
       " 'The program performances were evaluated on a shared task for disease detection using death certificates [17], and the program was described in-detail at this occasion [18].',\n",
       " ', 2017(N√©v√©ol et al., , 2018)).',\n",
       " 'Recently [3,4,5], the C√©piDC task consisted in extracting ICD-10 codes from death reports in several languages (French in 2016, French and English in 2017, French, Hungarian and Italian in 2018).',\n",
       " 'Best CLEF eHealth 2018 results on the raw and aligned French dataset [5] the coding decisions.',\n",
       " ', 2017(N√©v√©ol et al., , 2018) ) are good candidates for such analysis.',\n",
       " 'Several existing studies have shown that BM25 also provides solid baseline performance in screening prioritization [3,104] Seed-driven Document Ranking (SDR) is a ranking model specialized for screening prioritization.',\n",
       " 'For details of the creation of CLEF18 dataset, we refer to[3,104].',\n",
       " 'Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant Technology-assisted reviewing seeks to foreshorten this process by only screening the subset of the collection most likely to be relevant [10][11][12][13].',\n",
       " \"Existing work on relevance assessment is mostly related to TREC and CLEF evaluation challenges through the means of constructing test collections and reporting annotators' experience [19][20][21].\",\n",
       " 'Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].',\n",
       " 'Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making Evidence-Based Medicine (EBM) plays a significant role in health care and policy-making [7][8][9]12].',\n",
       " '-We validate the effectiveness of the proposed framework and provide a detailed analysis of its components on various datasets including the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the Text Retrieval Conference (TREC) Total Recall datasets [16], and the TREC Legal datasets [13].',\n",
       " 'Three datasets have been released-namely, EMED 2017 Three datasets have been released-namely, EMED 2017 [20], EMED 2018 [21,34], and EMED 2019 [22].',\n",
       " '[9] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [20,21,22].',\n",
       " 'To examine the effectiveness of the proposed framework, we compared it against the Knee, Target, SCAL, SD-training, and SD-sampling methods and provided detailed analysis on various datasets including the CLEF Technology-Assisted Reviews in Empirical Medicine datasets [20][21][22], the TREC Total Recall datasets [16], and the TREC Legal datasets [13].',\n",
       " '[1,2,6].',\n",
       " 'This approach was demonstrated to be robust by evaluating it using rankings of varying effectiveness produced by participants from the CLEF eHealth task ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].',\n",
       " 'The approach was demonstrated to be robust on a set of rankings of varying effectiveness produced by participants in a shared task [6].',\n",
       " 'Following Following [13], we utilise the publicly available submissions to the CLEF 2017 e-Health Lab Task 2 ≈ÇTechnology Assisted Reviews in Empirical Medicine\" [6].',\n",
       " 'HRR tasks include electronic discovery in the law (eDiscovery) [3], systematic review in medicine [22][23][24]47], document sensitivity review [34], online content moderation [55], and corpus annotation to support research and development [60].',\n",
       " 'loss er is introduced for the first time in [37] and latter used as a metric for the total recall task in the CLEF technology assisted reviews in empirical medicine overview tracks [82,84,86].',\n",
       " 'Other applications include open government document requests Other applications include open government document requests [3] and systematic review in medicine [14][15][16]32].',\n",
       " 'Unfortunately, it is not clear these measures were actually used: neither track overview discusses results on them [24,25].',\n",
       " 'A similar asymmetry can occur in systematic review in medicine [24,25].',\n",
       " 'TAR evaluation conferences TAR evaluation conferences [17,[23][24][25]38] have emphasized interventional stopping rules, i.',\n",
       " 'We use topics from the CLEF TAR task from 2017, 2018, and 2019 We use topics from the CLEF TAR task from 2017, 2018, and 2019 [11][12][13].',\n",
       " 'In addition, several recent challenges, such as TREC [17,24] and CLEF eHealth task 2 [25][26][27], further promote the development of automatic document screening.',\n",
       " 'It has been shown that this active learning solution outperforms its counterparts in many real-world cases [17,[24][25][26][27].',\n",
       " 'We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].',\n",
       " 'We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task We use three CLEF Technological Assisted Review (TAR) datasets to evaluate the effectiveness of the screening prioritisation task [18][19][20].',\n",
       " 'Methods to automate such review processes were evaluated in the scope of the Conference and Labs of Evaluation Forum (CLEF) with the so-called eHealth challenges regarding Technology Assisted Reviews (TAR) for systematic reviews (SR) in Empirical Medicine Methods to automate such review processes were evaluated in the scope of the Conference and Labs of Evaluation Forum (CLEF) with the so-called eHealth challenges regarding Technology Assisted Reviews (TAR) for systematic reviews (SR) in Empirical Medicine [10], in which relevant documents must be automatically retrieved for a given topic.',\n",
       " 'For the external validation of the two implemented search methods to retrieve publications relevant to a given medical topic, the established CLEF 2018 eHealth TAR dataset for the \"Subtask 1: No Boolean Search\" was used [10].',\n",
       " 'We explore initial experiments on the CLEF TAR 2019 dataset [16].',\n",
       " 'The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings The effectiveness of automatic approaches for search strategy creation and systematic review screening has been traditionally evaluated using binary relevance ratings [16,27,34], often sourced at the title and abstract screening level, rather than at the full-text level.',\n",
       " 'Cost-based and economic-based metrics are also used, especially in the context of the query formulation task in the CLEF TAR shared task [14][15][16], e.',\n",
       " 'Traditionally, retrieval was conducted at the level of publications [14][15][16].',\n",
       " 'We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets We used a collection of 38 systematic reviews of interventions from the CLEF TAR 2019 training and test datasets [16].',\n",
       " 'However, there are several other types of systematic reviews, such as diagnostic test accuracy reviews, prognostic reviews, and qualitative research reviews, each of which presents unique challenges for automation and evaluation [16].',\n",
       " 'The TREC Legal [1,6,18,20], TREC Total Recall [10], and the CLEF eHealth Technology-Assisted Review Tasks [11,12] have provided researchers with access to datasets and standardised evaluation methods.',\n",
       " 'Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets [28][29][30] and systematic review collection with seed studies (Seed Collection) [79].',\n",
       " 'These methods are evaluated and compared against alternative approaches on a range of datasets used to evaluate TAR approaches: the CLEF Technology-assisted Review in Empirical Medicine [30][31][32], the TREC Total Recall tasks [24], and the TREC Legal Tasks [17].',\n",
       " 'Its development was informed by experience from TREC Total Recall tracks [24,49] and also adopted by the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32].',\n",
       " 'We follow the CLEF Technology-assisted Reviews in Empirical Medicine Tracks [30][31][32] and Li and Kanoulas [39] in choosing a = 1 and b = 100.',\n",
       " 'We also include in our experiments three corpora from the Conference and Labs of the Evaluation Forum (CLEF) Technology-Assisted Reviews in Empirical Medicine datasets from the years 2017, 2018, and 2019 [24][25][26].',\n",
       " 'Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews Identifying all, or a significant proportion of, the relevant documents in a collection has applications in multiple areas including identification of scientific studies for inclusion in systematic reviews [13,[16][17][18], satisfying legal disclosure requirements [2,12,23], social media content moderation [38] and test collection development [22].',\n",
       " 'CLEF 2017/2018/2019 [16][17][18]: Collections of systematic reviews produced for the Conference and Labs of the Evaluation Forum (CLEF) 2017, 2018, and 2019 e-Health lab Task 2: Technology-Assisted Reviews in Empirical Medicine.',\n",
       " 'We rely on the CLEF-TAR 2017, 2018 and 2019 Subtask 2 datasets [22][23][24] (abbreviated as .',\n",
       " 'Search scenarios were selected from the CLEF 2018 collection Search scenarios were selected from the CLEF 2018 collection [12], a collection used for evaluating search engines tailored to consumer health search.',\n",
       " 'This collection consists of over 5 million medical webpages from selected domains acquired from the CommonCrawl [7].',\n",
       " 'These spoken queries are generated by 6 individuals using the information needs derived for the 2018 challenge [7].',\n",
       " 'Finally, we also report on a generalisation effort made to evaluate our predictive technology over new datasets [2,3].',\n",
       " 'A final section is also provided in which our experiments are extended and applied to two new datasets [2,3] for the sake of achieving generalisation.',\n",
       " 'As can be seen, there are several concepts intimately related such as reliabilty [1], trustworthiness [2], credibility [3], or veracity [22].',\n",
       " '[3] and the CLEF eHealth consumer health search task 2018 [2] were used to further evaluate this classification tech-  nology.',\n",
       " 'eu, accessed on 15 February 2021) has promoted the eHealth track since 2013 and, the CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of building search engines that are robust to query variations to support information needs of health consumers.',\n",
       " 'The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition.',\n",
       " 'The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition.',\n",
       " ', 2015;Jimmy et al., 2018;Cross et al.',\n",
       " 'This despite its heavy focus on gathering, organizing, and delivering a relevant social data related to events  Cross language/argumentative mining Cossu et al. (2018) generating a large number of micro-blog posts and web documents (such as, and in particular, cultural festivals).',\n",
       " 'The cross language task The cross language task (Cossu et al. 2018) was specific to movies.',\n",
       " 'The argumentation mining task The argumentation mining task (Cossu et al. 2018) aimed to automatically identify reason-conclusion structures from text, which can model the position, stance or attitude (as expressed via Twitter microblogs) of a social web user about a cultural event.',\n",
       " 'While we note significant efforts being made through various vehicles, such as NTCIR [10] and ImageCLEF [4], to support off-line adhoc search tasks, by the release of a first generation of lifelog test collection, until now, there was no dedicated benchmarking effort for interactive lifelog search, nor is there a test collection designed to support such benchmarking.',\n",
       " 'Based on the collections from NTCIR-12 and NTCIR-13, rigorous comparative benchmarking initiatives have been organised: the NTCIR 12 -Lifelog [9], and ImageCLEFlifelog2017 [3] exploited the NTCIR-12 collection and NTCIR-13 Lifelog 2 [10], ImageCLEFlifelog2018 [4] were proposed based on the NTCIR-13 collection.',\n",
       " 'This sub-task follows the success of the LMRT sub-task in ImageCLEFlifelog 2018 [7] with some minor adjustments.',\n",
       " 'People usually want to keep footage of the events that happen around them for many purposes such as reminiscence People usually want to keep footage of the events that happen around them for many purposes such as reminiscence [1], retrieval [2] or verification [3].',\n",
       " 'The increasing uptake of lifelogging as a personal and practitioner technology have also lead to related activities in ImageCLEF [2], the Lifelog Search Challenge [11] and a related task at MediaEval 2019.',\n",
       " 'While there have been some initial efforts, such as the lifelogging challenge at NT-CIR [16], or the lifelogging tasks at ImageCLEF [7] and MediaEval, even these can be considered to be pseudo-personal data challenges, since the test collections involved (lifelogs) are typically small in nature (from a few individuals).',\n",
       " '7 https://every-sense.',\n",
       " '2017aNguyen et al. , 2018) ) which focused on a series of image-retrieval and summarisation focused benchmarking initiatives since 2017, and the Lifelog Search Challenge (LSC) (Gurrin et al.',\n",
       " 'In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].',\n",
       " 'Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges [82][83][84]; Lifelog Search Challenge [85][86][87], which aims to encourage the development of efficient interactive lifelog retrieval systems; and NTCIR Lifelog Tasks [77].',\n",
       " 'Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].',\n",
       " 'The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [23] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [24][25][26]55] and the ImageCLEF Lifelog tasks [12][13][14]46].',\n",
       " 'To benchmark for different search engines in indexing and retrieving multimodal lifelog data, there have been a number of interactive lifelog retrieval challenges such as NTCIR Lifelog [7,8], ImageCLEF Lifelog [4,5,20], and Lifelog Search Challenge (LSC) [9,10].',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'In case the user opens the moment-detailed box (3) for further browsing, the user can use the temporal browsing panel (5) to view the previous/after moments of the selected one by horizontal scrolling the panel as well as adjusting the time delta to view the temporal nearby or further apart moments.',\n",
       " 'The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].',\n",
       " 'The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].',\n",
       " ', 2017) of IMAGE-CLEF (de Herrera et al., 2018).',\n",
       " '8M images based on their type and to extract the non-compound clinical ones, leading to 232,305 images along with their respective captions (de Herrera et al., 2018).',\n",
       " ', 2017;de Herrera et al., 2018) and stopped in 2019.',\n",
       " 'Out of the four publicly available ones, PEIR Gross1 and ImageCLEF [20] suffer from severe shortcomings [55], which are also discussed briefly below.',\n",
       " 'In previous work [55], we reported that three publicly available datasets can be used for DC research, namely PEIR Gross, ICLEFcaption [20], and IU X-Ray [21].',\n",
       " 'The average of the four variants was used as the official measure in ICLEFcaption [20,24].',\n",
       " 'Retrieval-based systems were also the top performing submissions of the ImageCLEF Caption Prediction subtask, a task that ran for two consecutive years [20,24].',\n",
       " 'It is promising in various applications such as human-computer interaction [8], [9], information retrieval [10], and medical image understanding [11].',\n",
       " 'Our DCR method reweights the RL reward for each ground truth captions as (10), and also directly uses CIDErBtw as a part of the final reward as (11).',\n",
       " 'In the 6th edition [12,13,[22][23][24] of the task, there will be two subtasks: concept detection and caption prediction.',\n",
       " 'For the computer assisted analysis, it was noted that no solution has sufficient prediction accuracy (10)(11)(12)(13)(14).',\n",
       " 'ImageCLEFtuberculosis task (Cid et al., 2018) is one such example which was published to build models for detection, classification, and severity measurement of TB from the provided chest-CT.',\n",
       " 'The usual classification of TB is infiltrative, focal, tuberculoma, miliary, and fibrocavitory [3].',\n",
       " 'Traditional computer-aided diagnosis technology is usually aimed at one disease; for example, for judging the probability of lung cancer based on the presence of pulmonary nodules in Computed Tomography(CT) images of the chest [10], for detecting tuberculosis and classifying its severity [11], or for detecting breast cancer based on chest radiographs [12].',\n",
       " 'The three tasks will be: figure caption analysis [8,13], tuberculosis analysis [8,13], and visual question answering [12].',\n",
       " 'The first attempt for building the dataset for medical VQA is by ImageCLEF-Med [6].',\n",
       " 'The first attempt for building the dataset for medical VQA is by ImageCLEF-Med [6].',\n",
       " 'The 2018 ImageCLEF-Med challenge [6] provides a good overview about the approaches and their results.',\n",
       " '‚Ä¢ Word-based Semantic Similarity (WBBS) ‚Ä¢ Word-based Semantic Similarity (WBBS) (Hasan et al., 2018) We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al.',\n",
       " '‚Ä¢ Word-based Semantic Similarity (WBBS) ‚Ä¢ Word-based Semantic Similarity (WBBS) (Hasan et al., 2018) We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al.',\n",
       " 'We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper We follow the evaluation setup discussed in ImageCLEF2018 VQA-Med 2018 challenge overview paper (Hasan et al., 2018) to evaluate the performance of the system.',\n",
       " 'When the ImageCLEF2018 competition [7] proposed a VQA-Med task in 2018, VQA was applied to the medical field for the first time.',\n",
       " 'between CBSS and WBSS is that CBSS uses Meta-Map between CBSS and WBSS is that CBSS uses Meta-Map [48] to extract biomedical concepts from answers through the pymetamap wrapper [7], the dictionaries of the concepts and predicted answers are established, and the semantic similarity between them is calculated by cosine similarity, which is similar to the principle of similarity calculation mentioned in Eq.',\n",
       " 'Due to these advantages, VQA models for medical applications have also been proposed [6,8,12,13,24,28], whereby allowing clinicians to probe the model with subtle differentiating questions and contributing to build trust in predictions.',\n",
       " ', 2015;Hasan et al., 2018;Lau et al.',\n",
       " 'The mainstream VQA datasets in the medical domain include VQA-MED-2018 [40], VQA-MED-2019 [6], and VQA-MED-2020 [7], which were proposed by the challenge tasks.',\n",
       " 'VQA-MED-2018 [9], VQA-RAD [14], VQAMED-2019 [4], RadVisDial [13], PathVQA [11], VQA-MED-2020 [3], SLAKE [18], and VQA-MED-2021 [5].',\n",
       " 'The VQA-MED-2018 dataset was the first of its kind created specifically for visual QA (VQA) using medical images The VQA-MED-2018 dataset was the first of its kind created specifically for visual QA (VQA) using medical images [276].',\n",
       " 'Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models Introduced at CLEF 2017, the Personalised Information Retrieval (PIR-CLEF) task sought to develop a framework for the repeatable evaluation of algorithms for personalized search, and for the evaluation of user models [7][8] [9].',\n",
       " 'In a task involving the detection of a mental health problem, such as anorexia, the number relevant and informative posts is quite rare [14][15][16][17], while even in a similar task, there may be several ways of inferring information from a particular document.',\n",
       " 'For social media datasets, we use two public datasets, the eRisk-2018 For social media datasets, we use two public datasets, the eRisk-2018 (20) and Clpsych-2015 datasets (21).',\n",
       " 'We divide data into a, b, c, and d four age groups, indicating early adolescence (10)(11)(12)(13)(14)(15), late adolescence (16)(17)(18)(19)(20)(21)(22), adolescence (22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35), early middle-aged (22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35), and middleaged (35-).',\n",
       " ', check-worthiness estimation has been severely understudied as a problem [1,5,7].',\n",
       " 'Task 1: Check-Worthiness [ Barr√≥n-Cede√±o et al., 2018] asks to predict if a given claim from a political debate should be prioritized for fact checking.',\n",
       " ', 2018;Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al.',\n",
       " 'lab featured a shared task on fact-checking a claim in the context of a political debate (Barr√≥n-Cede√±o et al., 2018;Nakov et al.',\n",
       " 'Recently, a great number of the studies proposing approaches on fake news detection are based on deep learning methods (Barr on-Cedeno et al., 2018;Popat et al.',\n",
       " ', , 2019) ) and verification (Barr√≥n-Cede√±o et al., 2018;Hasanain et al.',\n",
       " 'Till now, most researchers have tried to handle this issue by checking if a post is worth-checking [6] to reduce the number of claims.',\n",
       " '[6] has hosted since 2018 an open detection task of checkworthy claims.',\n",
       " 'Earlier works in identifying check-worthy claims were at the granularity of entire sentences Earlier works in identifying check-worthy claims were at the granularity of entire sentences [5,26,54,81].',\n",
       " '[5].',\n",
       " '[5]), sentences in medical newswire are long and complex, often positioning the primary claim(s) within a larger context of other information [96], we obtain 6,000 news articles from the \"Health\" category of Google News during April 2018 and augment this with the top 25 RSS feeds in the \"Health and Healthy Living\" category5 from November 2018 through April 2019 to get over 34,000 news articles.',\n",
       " 'Predicting the score of a claim-sentence pair is formulated as regression learning with target set Predicting the score of a claim-sentence pair is formulated as regression learning with target set [1,5] aligned with the Likert-type scale.',\n",
       " ', 2017;Atanasova et al., 2018) are equipped.',\n",
       " 'Recently, Several works are carried out to detection of offensive language (Kalaivani and Thenmozhi, 2020a), hate speech (Kalaivani and Thenmozhi, 2020b), fake news detection, trustworthiness (Atanasova et al., 2018) and fact-checking (Elsayed et al.',\n",
       " ', 2015), and trust-worthiness prediction (Barr√≥n-Cede√±o et al., 2018).',\n",
       " ', 2018;Atanasova et al., 2019;Barr√≥n-Cedeno et al.',\n",
       " ', 2018;Barr√≥n-Cede√±o et al., 2018;Atanasova et al.',\n",
       " '2019 were obtained using Long Short-Term Memory (LSTM) networks [7].',\n",
       " 'Feature representation methods, including word embeddings, Bag of Words, Named Entity Recognition, and Part of Speech tagging [6,7,9] have been widely used to enhance model understanding of the task.',\n",
       " 'In the recent CLEF 2018 competition on check-worthiness detection [17], Zou et al.',\n",
       " 'Out of these 7 speeches, 4 are by Donald Trump and are made available by the CLEF 2018 lab on automatic identification and verification of political claims [17].',\n",
       " '2016 presidential election (following the same setting as in the official CLEF 2018 competition on check-worthiness detection [17] but using even more data).',\n",
       " '20194 is a continuation of the evaluation lab at CLEF-2018 [12].',\n",
       " '(Barr√≥n-Cede√±o et al., 2018) FactCheck.',\n",
       " 'Lab Task 2 [2,32] contains 94 claims from three debates as a training set, and 192 claims from seven debates and speeches as a test set.',\n",
       " \"As outlined in Atanasova et al. (2018), of a total of seven models compared, the most successful approaches used by the participants relied on recurrent and multi-layer neural networks, as well as combinations of distributional representations, matching claims' vocabulary against lexicons, and measures of syntactic dependency.\",\n",
       " 'lab on identification and verification of claims (Nakov et al., 2018;Elsayed et al.',\n",
       " \"labs' shared tasks (Nakov et al., 2018;Elsayed et al.\",\n",
       " ', 2015); trust-worthiness prediction and fact-checking (Atanasova et al., 2018;Atanasova et al.',\n",
       " 'These datasets are similar to ours in order of magnitude, however, use a different definition of claim, as is the case with others tackling the determination of check-worthiness of claims [4,17,31,47].',\n",
       " 'lab featured a shared task on fact-checking a claim in the context of a political debate (Barr√≥n-Cede√±o et al., 2018;Nakov et al.',\n",
       " 'Recently, a great number of the studies proposing approaches on fake news detection are based on deep learning methods (Barr on-Cedeno et al., 2018;Popat et al.',\n",
       " 'lab (Nakov et al., 2018;Elsayed et al.',\n",
       " ', 2015), and trust-worthiness prediction (Barr√≥n-Cede√±o et al., 2018).',\n",
       " ', 2020;Atanasova et al., 2018;Barron-Cedeno et al.',\n",
       " ', 2018;Barr√≥n-Cede√±o et al., 2018;Atanasova et al.',\n",
       " 'In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations [48].',\n",
       " '868 [36].',\n",
       " 'Since understanding temporality regarding clinical events conveyed in the narrative text is a crucial prerequisite for the utilization of the narratives, automatic means to identify temporal information from clinical narratives have gained much attention from the community [1][2][3][4][5][6][7][8][9][10].',\n",
       " ', the 2012 Informatics for Integrating Biology and the Bedside (i2b2) challenge [14], the 2013/ 2014 CLEF/ShARe challenges [4], and the 2015/2016/ 2017 Clinical TempEval challenges [5][6][7]).',\n",
       " 'The 2015 Analysis of Clinical Text (ACT) shared task The 2015 Analysis of Clinical Text (ACT) shared task [39] utilized the ShaRe dataset [40] consisting of 531 manually annotated discharge summaries, electrocardiograms, echo, and radiology reports from MIMIC-II.',\n",
       " 'The * SEM 2012 Shared Task (Morante and Blanco 2012) was devoted to processing the scope and focus of negation, and in the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al. 2014) participants had to detect whether a disorder was negated.',\n",
       " 'In this University of Pennsylvania Institute Review Board (IRB)-approved pilot study, we leveraged the pre-annotated 2014 ShARe/CLEF eHealth Challenge corpus In this University of Pennsylvania Institute Review Board (IRB)-approved pilot study, we leveraged the pre-annotated 2014 ShARe/CLEF eHealth Challenge corpus [21], a subset of the Medical Information Mart for Intensive Care (MIMIC)-II database [22] collected from the intensive care units of Beth Israel Deaconess Medical Center.',\n",
       " 'To align our annotated classes with current and well-adopted annotation efforts in the NLP community, we added new and expanded existing annotation classes to the ShARe [21], TimeML [14], and CALEX [17] schemas.',\n",
       " 'ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 [43] focused on facilitating understanding of information in clinical reports by extracting several attributes such as negation, uncertainty, subjects, severity, etc.',\n",
       " 'Finally, we used an open-source data set for part of our validation from the ShARe/CLEF 2014 challenge [22,23] We used two datasets for validation.',\n",
       " 'To evaluate the performance of our tool, we have used the ShAReCLEF dataset from task 2 of the 2014 challenge To evaluate the performance of our tool, we have used the ShAReCLEF dataset from task 2 of the 2014 challenge [22,23].',\n",
       " 'Content Generation Content Generation [124] generating new medical descriptions or knowledge based on a given input  [127] 2012 ‚àº4K English RE Link ShARe13 [128] 2013 ‚àº29K English NER Link GENIA13 [129] 2013 ‚àº5K English EE Link NCBI [21] 2014 ‚àº7K English NER Link ShARe14 [130] 2014 ‚àº35K English NER Link CADEC [20] 2015 ‚àº7.',\n",
       " ', 2020) and CLEF 2014 (Mowery et al., 2014) datasets in this paper.',\n",
       " 'The ShARe/CLEF eHealth 2014 shared task [8] focused on facilitating understanding of information in narrative clinical reports, such as discharge summaries, by visualizing and interactively searching previous eHealth data (Task 1) [9], identifying and normalizing disorder attributes (Task 2), and retrieving documents from the health and medicine websites for addressing questions monoand multi-lingual patients may have about the disease/disorders in the clinical notes (Task 3) [10].',\n",
       " 'boosting easy to read documents for laypeople [49,40,11], or search aids, such as query suggestions to match the searcher expertise.',\n",
       " \"In the framework of CLEF eHealth 2014 [12], one of the proposed shared tasks (task 3) addresses this challenge [8]: queries have been defined from real patient cases issued from the clinical documents provided by the CLEF eHealth 2014's task 2.\",\n",
       " ', 2013 ;Goeuriot et al., 2014), ce qui garantit que la recherche est faite dans un espace de fiabilit√© et fournit des informations de confiance.',\n",
       " ', 2013 ;Goeuriot et al., 2014) semblent √™tre assez uniques pour ce domaine de recherche.',\n",
       " \"Our experimental framework is the CLEF eHealth 2014's task 2 [10], for which queries are defined from real patient cases issued from clinical documents within the KRESMOI project [11].\",\n",
       " '0560 [10].',\n",
       " 'This fact is also acknowledged for the average results of all the participants [10].',\n",
       " \"The patient uses 'layman' query terms, while the discharge summary contains an expert description of the diagnosis (Goeuriot et al. 2014;Kelly et al.\",\n",
       " 'The use of the Web as source of health-related information is a wide-spread practice among health consumers The use of the Web as source of health-related information is a wide-spread practice among health consumers [19] and search engines are commonly used as a means to access health information available online [7].',\n",
       " 'Evaluation campaigns and resources in this domain are presented, including TREC Medical Records Track [43,45,46], TREC Clinical Decision Support Track [36][37][38], CLEF eHealth (consumer health search [13,14,35,55] and as of 2017 search systems for the compilation of systematic reviews), i2b2 Shared Task Challenges¬≥, ALTA Shared Task ¬≥https://www.',\n",
       " 'While organizers published overview papers in 2013 [14] and 2014 [18], no deeper analysis of these results has so far been reported.',\n",
       " 'More detailed descriptions are available in the 2013 and 2014 task overview papers in the CLEF proceedings [14,18].',\n",
       " '11 Further details on baselines used are provided in the Task overview papers [14,18].',\n",
       " 'The CLEF 2015 collection contains 50 queries and 1437 documents that have been assessed as relevant by clinical experts and have an assessment for understandability The CLEF 2015 collection contains 50 queries and 1437 documents that have been assessed as relevant by clinical experts and have an assessment for understandability [44].',\n",
       " 'We used the thresholds U=2 for CLEF 2015 and U=40 for CLEF 2016, based on the distribution of understandability assessments and the semantic of understandability labels [44,14].',\n",
       " 'Relevance assessments on the CLEF 2015 and 2016 collections are incomplete Relevance assessments on the CLEF 2015 and 2016 collections are incomplete [44,14], that is, not all top ranked Web pages retrieved by the investigated methods have an explicit relevance assessment.',\n",
       " ', 2013;Goeuriot et al., 2014) but may also be provided from works of researchers, such as POS-tag (Tsuruoka et al.',\n",
       " 'With the spreading awareness on the exploration of information extracted from medical discharge documents and clinical reports by laymen patients, searching online health related web-forums and other sources for health advice has become a common habit [17,18,19].',\n",
       " 'In addition, the present information retrieval systems pays more attention on a specific group of people with expert health knowledge [18,20,22].',\n",
       " 'However, while addressing users diverse information needs, such as searching information on a specific disease, preceding researches targeted only a specific group of users with expert health knowledge [17,18,20].',\n",
       " 'Previous researches on medical information retrieval also disclosed how desperate patients are in apprehending the content of their medical discharge reports and clinical reports [14,17,18].',\n",
       " 'In the medical domain, querying of the Internet for useful information has become increasingly important, owing to the huge amount of information available [17,18].',\n",
       " 'In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 In the clinical domain, the related techniques develop rapidly due to several shared tasks, such as the NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) in 2009 [6], 2010 [7], 2012 [8] and 2014 [9], the NLP challenges organized by SemEval in 2014 [10], 2015 [11] and 2016 [12], and the NLP challenges organized by ShARe/CLEF in 2013 [13] and 2014 [14].',\n",
       " 'These are often built for the purposes of shared tasks [26,27].',\n",
       " 'We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30].',\n",
       " 'The first set is a set of 250K images [13], collected by querying web image search engines.',\n",
       " 'In spite of sustained efforts and important progress over the last decade In spite of sustained efforts and important progress over the last decade [341,342], a number of important challenges still have to be addressed before including automatic annotation in Web retrieval pipelines.',\n",
       " 'Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.',\n",
       " '[28] used Case Retrieval in Radiology (CaReRa) project, which includes Clinical Experience Sharing (CES) concepts, to treat liver cancer.',\n",
       " ', Office-10+Caltech-10 [14], Office31 [31], ImageCLEF [5] and Digit Recognition.',\n",
       " 'ImageCLEF ImageCLEF [5] is developed for the ImageCLEF domain adaptation task 1 .',\n",
       " 'We study this in the context of the INEX Social Book Search Track2  [12,14,15].',\n",
       " 'The INEX Social Book Search Track The INEX Social Book Search Track [12,14,15] investigates book search in collections with both professional metadata and social media content.',\n",
       " 'For the retrieval system, we use the Amazon/LibraryThing collection [2] that is also used in the INEX Social Book Search Track [15].',\n",
       " 'INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time [43].',\n",
       " ') [14].',\n",
       " \"[3] experiment with traditional query expansion techniques, exploiting Wikipedia articles as rich sources of information that can augment the user's query.\",\n",
       " 'A concrete example is the user-selectable interface panels as evaluated in G√§de et al. (2016), which include a Browse view, a Search view and a Book-bag view, aiming to support pre-focus, focus formulation and post-focus stages.',\n",
       " 'Efforts have been undertaken to transfer this paradigm to less systems-focused research, including the TREC Interactive and Session tracks [12], the INEX Interactive track [14,16], the Interactive Social Book Search track [6,7,11], and the RePAST archive [5].',\n",
       " 'The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab The background for this workshop is derived from the Interactive Track of the CLEF/INEX Social Book Search Lab [4,5,6], which investigates scenarios with complex book search tasks and develops systems and interfaces that support the user through the different stages of their search process.',\n",
       " 'Details of the participants and the methods used in the runs are synthesised in the overview working note of the task [19].',\n",
       " 'A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task [19] and further developed in the individual working notes of the participants who submitted one (BME TMIT [22], FINKI [28], I3S [31], IBM AU [26], IV-Processing [30], MIRACL [23], PlantNet [24], QUT [25], Sabanki-Okan [27], SZTE [29]).',\n",
       " 'A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task A total of 10 participating groups submitted 27 runs that are summarised in the overview working note of the task [19] and further developed in the individual working notes of the participants who submitted one (BME TMIT [22], FINKI [28], I3S [31], IBM AU [26], IV-Processing [30], MIRACL [23], PlantNet [24], QUT [25], Sabanki-Okan [27], SZTE [29]).',\n",
       " 'The three datasets are the Caltech-UCSD-2011 (CUB200-2011) [22], Birdsnap [3], and PlantCLEF 2015 [14].',\n",
       " 'In recent years there has been an increasing interest in the problem of plant species classification in images In recent years there has been an increasing interest in the problem of plant species classification in images [3,7,12,18].',\n",
       " 'This paper presents the participation of Inria ZENITH team to the 2015-edition of this challenge [9,19].',\n",
       " 'Recently, structural features such as LBP variants have been used [9,10], [15,16].',\n",
       " 'We used the dataset released for the plant task of the LifeCLEF 2015 challenge [2].',\n",
       " 'Allowing the mass of citizens to produce accurate plant observations requires to equip them with much more accurate identification tools [50].',\n",
       " 'Theoretically, it is possible to combine multiple images from the same observation in the recognition process, which could further improve the accuracy, as shown for a different dataset by [33].',\n",
       " 'Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014.',\n",
       " 'Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10].',\n",
       " 'They use Transfer learning to fine-tune the pre-trained models, using LifeCLEF plant dataset (Go√´au et al., 2014) and applied classic data augmentation techniques based on image transforms such as rotation, translation, reflection, and scaling to decrease the chance of overfitting.',\n",
       " 'The meta-data [7], [8]consists of ObservationID: the plant observation ID from which several pictures, associated with the organ, File Name, MediaID, ClassID, Species, Family Name etc.',\n",
       " 'Recently, deep learning techniques have dominated the PlantCLEF challenge (Go√´au et al., 2014).',\n",
       " 'In this work, 80% of the images were randomly selected as training data, whereas the rest (20%) were used for validation, as in previous studies of deep learning-based computer vision applications [27,28].',\n",
       " 'In this work, 80% of the image randomly selected as training data, whereas the rest (20%) were used for validatio previous studies of deep learning-based computer vision applications [27,28].',\n",
       " \"An overview of the approaches of last year's participants of NEWSREEL 2014 is provided in [10].\",\n",
       " 'This non-parametric test is common in computational authorship studies, such as the PAN competition (Stamatatos et al., 2014), to compare the output of two authorship attribution systems, where we cannot make assumptions about the (potentially highly complex) underlying distributions.',\n",
       " 'in [11], which also incorporates data provided by the PAN evaluation lab [28,29,30].',\n",
       " 'The accessable dataset (see The accessable dataset (see [11,28,29,30]) contains about 9300 instances of the form Dknown, dunknown, l , where Dknown is a set of documents from a known author, dunknown defines a document in question and l ‚àà {0, 1} denotes the class label.',\n",
       " 'Wright, Chin 13 trained support vector machine to classify the Five Factor personality.',\n",
       " 'We use the following datasets for these experiments: PAN14 (Rangel et al., 2014) 4 -we include the English data from PAN14 for the following domains: BLOGS, hotel REVIEWS, and Social Media (SOME).',\n",
       " ', , 2014;;Rangel et al., 2015).',\n",
       " 'is currently receiving a growing interest in the Computational Linguistics community as it is also testified by the first shared task organized in 2013 on Author Profiling at PAN 2013 (Rangel et al., 2013).',\n",
       " 'These expressions and 200 training examples from QALD-4 [11], used as input to the ZC05 algorithm, can be found at http://pub.',\n",
       " \"Today's artificially intelligent agents are good at answering factual questions about our world Today's artificially intelligent agents are good at answering factual questions about our world [9,15,41].\",\n",
       " 'There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].',\n",
       " 'Then, we apply our proposed system to Task 2 (Biomedical question answering over interlinked data) of QALD-4 [19] to compare its precision and recall to state-of-the-art question answering systems in the biomedical domain.',\n",
       " 'Biomedical question answering over interlinked data) of QALD-4 [19], using the FOL to SPARQL translation.',\n",
       " 'Datasets: We manually annotate superlative expressions from QALD-4 evaluation dataset (Unger et al., 2014) and TREC QA (2002QA ( , 2003) ) datasets (NIST, 2003), and guarantee that all the labeled superlative instances can be grounded to gradable Freebase predicates.',\n",
       " 'We evaluate HAWK against the QALD We evaluate HAWK against the QALD [21] benchmark.',\n",
       " 'The open challenge on QA over Linked Data [16] has many participants.',\n",
       " 'The QALD 2 proceedings are included in ILD 2012, QALD 3 [25] and QALD 4 [137]  SQA Surveys For each participant, problems and their solution strategies are given: Athenikos and Han [9] give an overview of domain specific QA systems for biomedicine.',\n",
       " 'In question answering systems such as QALD [1,2,3], answer to the queries is found from DBpedia which is updated on a regular basis.',\n",
       " '85 RO FII (mentioned in [34]) semi-manual 0.',\n",
       " 'The QALD benchmarking initiative [12], now in its fifth year, puts emphasis on linked open data in RDF format.',\n",
       " 'One of the means to improve the access to the Linked Data Web lies in the development of natural-language interfaces that can transform human languages or even controlled languages into a representation suitable for querying large knowledge bases [10].',\n",
       " 'One of the means to improve the access to the Linked Data Web lies in the development of natural-language interfaces that can transform human languages or even controlled languages into a representation suitable for querying large knowledge bases [10].',\n",
       " 'DBpedia has been succesfully used as the primary knowledge source for Question answering Systems, especially within the Question Answering over Linked Data workshop [20].',\n",
       " 'The QALD-6 training set 25 , which is the recent successor of QALD-5 [19], contains 350 questions, including the questions from previous QALD challenges.',\n",
       " 'QALD is a series of international competitions on mapping natural language questions to knowledge base queries QALD is a series of international competitions on mapping natural language questions to knowledge base queries [9].',\n",
       " 'Multiple successful question answering systems were presented in the previous QALD competitions (see for example the overview in Multiple successful question answering systems were presented in the previous QALD competitions (see for example the overview in [9]).',\n",
       " 'In Table In Table 2, we report preliminary evaluation results on the training data set for Task 4 of the QALD-7 Shared Task using the metrics from [9].',\n",
       " 'With the increasing number of high quality datasets, the Semantic Web is seeing a renewed interest in estion Answering (QA) over graph data [18].',\n",
       " '[30] and Section 5.',\n",
       " 'Throughout our experiment, we employed the Large-Scale Complex Question Answering DatasetThroughout our experiment, we employed the Large-Scale Complex Question Answering Dataset12 (LC-QuAD) [28] as well as the 5 th edition of Question Answering over Linked Data Challenge13 (QALD-5) dataset [30].',\n",
       " 'Multiple datasets have been proposed for KB-QA, which differ in the underlying KB (DBpedia or Freebase), size (a couple of hundreds to a few thousands), and question phenomena they involve (simple, compositional, and/or questions with conditions, among others) Multiple datasets have been proposed for KB-QA, which differ in the underlying KB (DBpedia or Freebase), size (a couple of hundreds to a few thousands), and question phenomena they involve (simple, compositional, and/or questions with conditions, among others) [1,5,7,9,10,29,31].',\n",
       " 'QALD [29,31] is a series of evaluation campaigns on QA over linked data, and releases datasets every year to evaluate KB-QA systems.',\n",
       " 'QALD-5 questions were compiled from the QALD-4 training and test questions, slightly modified in order to account for changes in the DBpedia dataset [107].',\n",
       " 'The most popular benchmarks for QA over Knowledge Bases are WebQuestions The most popular benchmarks for QA over Knowledge Bases are WebQuestions [1], SimpleQuestions [3] and QALD 3 [8][5] [10][11] [12].',\n",
       " 'The use of CQA for ontology matching opens new perspectives such as ontology matching with natural language to ontology mapping techniques [24] over multiple ontologies.',\n",
       " 'In the area of Question Answering using LD, challenges such as BioASQ In the area of Question Answering using LD, challenges such as BioASQ [43], and the Question Answering over Linked Data (QALD) [45] have aimed to provide benchmarks for retrieving answers to human-generated questions.',\n",
       " 'In this work, we use the QALD version 2 (QALD-2) data set benchmark from The Test Collection for Entity Search (DBpedia-Entity) [1], and; QALD version 4 (QALD-4) [34].',\n",
       " 'QALD-4 has ten queries that follow this criteria, Queries 12,13,21,26,30,32,34,41,42,and 44.',\n",
       " 'The field of QA is vast and can be classified along many different dimensions, including (i) knowledge based (Unger et al. 2014) vs.',\n",
       " '[11] refer to nonneural KGQA approaches as \"traditional\" [7,30,38].',\n",
       " 'As far as quantities are concerned, lookups are supported by many methods, over both knowledge graphs and text documents, and are part of major benchmarks, such as QALD [36], NaturalQuestions [22], ComplexWebQuestions [35], LC-QuAD [12] and others.',\n",
       " 'Bulk of the recent literature on automated QA is focused on either (i) proposing a new kind of dataset Bulk of the recent literature on automated QA is focused on either (i) proposing a new kind of dataset (Unger et al., 2014;Rajpurkar et al.',\n",
       " 'Marginean (2014) proposed a different approach using manually generated rules and showed that it performs better than POMELO for biomedical data (QALD-4) (Unger et al., 2014); however, this approach is brittle and does not work for newer or more complex queries.',\n",
       " 'In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series In addition to the technical contribution of position-based patterns, we also introduce an expansion to benchmark datasets for the challenges in the Question Answering over Linked Data (QALD) series [20][21][22][23][24][25][26][27][28], which is a series of benchmarks for evaluating KGQA systems.',\n",
       " 'For instance, a question on QALD-5 [24] \"Which anti-apartheid activist was born in Mvezo?',\n",
       " 'We used the following benchmarks in particular: QALD-5 We used the following benchmarks in particular: QALD-5 [46]: This benchmark consists of over 340 training questions and focuses on multilingual question answering.',\n",
       " 'This kind of system is evaluated in the QALD open challenge [33].',\n",
       " 'QALD-4 [28] comprises 50 natural language biomedical questions, involving SPARQL queries from SIDER, Drugbank, and Diseasome domains.',\n",
       " 'We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset We begin by qualitatively demonstrating label embeddings trained on label cooccurrence patterns from the BioASQ dataset [1], which is one of the largest datasets for multi-label text classification, and label hierarchies in the 2015 MeSH vocabulary.',\n",
       " 'BioASQ BioASQ [132,1,10,11] is a benchmark challenge which ran until September 2015 and consists of semantic indexing as well as an SQA part on biomedical data.',\n",
       " 'This approach has been shown to be highly successful in the recent BioASQ 2 challenge evaluations [44][45][46] and has also been adopted by many others [47,48].',\n",
       " 'In 2014, the BioASQ challenge task In 2014, the BioASQ challenge task [45] ran for six consecutive periods (batches) of 5 weeks each.',\n",
       " 'In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ In order to promote the development of semantic indexing and automatic question answering systems in the biomedical field, BioASQ [16][17][18], a challenge on large-scale biomedical semantic indexing and question answering, held an international competition from 2013 to 2017.',\n",
       " 'The FP7 BIOASQ project aims to push research towards highly precise biomedical information access systems by establishing a series of challenges in which systems from teams around the world compete The FP7 BIOASQ project aims to push research towards highly precise biomedical information access systems by establishing a series of challenges in which systems from teams around the world compete [2].',\n",
       " 'There are also established challenges on answering factual questions posed by humans [9], natural language knowledge base queries [41] and even university entrance exams [34].',\n",
       " ', 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014;Shibuki et al.',\n",
       " ', 2014) offers comparative evaluation for solving real-world university entrance exam questions; The dataset of CLEF QA Entrance Exams Task (Rodrigo et al., 2015) is extracted from standardized English examinations for university admission in Japan; ARC dataset (Clark et al.',\n",
       " \"Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab Modern systems and SA's state of the art can be seen in NLP Workshops and competitive challenges, such as RepLab [7], SemEval Twitter SA [8], and aspect based SA tasks [9,10].\",\n",
       " 'More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter, see PAN [18] and RepLab [19] overviews for more details.',\n",
       " 'The RepLab 2014 \"Author Ranking\" task was specifically focused on influence The RepLab 2014 \"Author Ranking\" task was specifically focused on influence [19], as explained in more details in Section III.',\n",
       " 'The CLEF RepLab 2014 dataset The CLEF RepLab 2014 dataset [19] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum2 (CLEF).',\n",
       " 'The RepLab framework The RepLab framework [19] uses the Mean Average Precision (MAP) to evaluate the estimated rankings.',\n",
       " 'Actually, in RepLab 2014 [19], the organizers were not able to conclude on significant differences between certain participants due to the number of considered domains.',\n",
       " '3 These tasks were organized as a CLEF evaluation task [8,9] where teams were given a set of entities and for each entity a set of tweets were provided.',\n",
       " 'RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.',\n",
       " 'Keeping these different dimensions in view, the task of reputation dimensions classification was first introduced within RepLab 2014 [9].',\n",
       " '9 In the month of June 2015.',\n",
       " 'More recently, two conference tasks were proposed in order to investigate real-life influencers based on Twitter: PAN [59] and RepLab [3].',\n",
       " 'The RepLab Challenge 2014 dataset The RepLab Challenge 2014 dataset [3] was designed for an influence ranking challenge organized in the context of the Conference and Labs of the Evaluation Forum 1 (CLEF).',\n",
       " 'The RepLab framework The RepLab framework [3] uses the traditional Mean Average Precision (MAP) to evaluate the estimated rankings.',\n",
       " 'In RepLab 2014 [3], the organizers were not able to conclude on significant differences between participants (and features or methods used) due to the small number of considered domains.',\n",
       " \"We use the context of RepLab We use the context of RepLab [2,3] tasks to evaluate our proposal that is to say: to propose an overview of 61 entity's (drawn in 4 domains: Automotive, Banking, Music and University) e-Reputation regarding experts taxonomies using provided set and pertaining of Micro-Blogs concerning each entity.\",\n",
       " 'It leads us to consider probability re-estimation of a document d in a class c using a smoothing as defined in (3).',\n",
       " 'Specifically, we evaluate the models for five applications: (1) predict whether the sentiment of tweet is positive, negative or neutral (SA) [26], (2) predict the entity the tweet belongs to (EI) [27], (3) predict the priority of the topic the tweet belongs to (TP) [ From the results, we find that Paragraph2Vec has poor performance for all the tasks compared to BOW.',\n",
       " 'There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [13], event and rumor detection [3,16], topic popularity and summarization [2,23], information diffusion [11], popularity prediction [18], and reputation monitoring [1].',\n",
       " \"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\",\n",
       " '(1)[293] Other(8) [62,171,179,[197][198][199]214,294] Hotel Reviews(24) [[93][94][95][96][97][98][99][100]102,103,[127][128][129][130][131][132][133][135][136][137][138][139][140][141][142] https://doi.',\n",
       " 'In 2014, RepLab focused on two aspects of reputation analysis, which is Reputation Dimensions Classification and Author Profiling (Amigo et al., 2014).',\n",
       " 'Finally, the Twitter subcorpus was constructed in cooperation with RepLab [3] in order to address also the reputational perspective (e.',\n",
       " ', , 2013(Amig√≥ et al., , 2014) ) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance.',\n",
       " 'There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [24], event and rumor detection [16,28], topic popularity and summarization [2,42], information diffusion [18], popularity prediction [34], and reputation monitoring [1].',\n",
       " \"First, for a text c ‚àà C, let s + c ‚àà [1,5] be the text's positive sentiment score and s - c ‚àà [-5, -1] be the text's negative sentiment score (according to SentiStrength, c.\",\n",
       " 'Authorities receive interactions in particular from hubs whereas hubs connect to a lot of authorities [21].',\n",
       " 'O gerenciamento da reputa√ß√£o digital √© uma importante an√°lise que serve para medir como √© a reputa√ß√£o de uma empresa em rela√ß√£o a certos grupos de interessados [1].',\n",
       " 'Para avaliar experimentalmente a proposta, ela foi experi-mentada usando o conjunto de publica√ß√µes do desafio do RepLab 2014 [1], que consiste de publica√ß√µes no Twitter extra√≠das em 2012 durante o per√≠odo de 1 o de Junho at√© 31 de Dezembro, com cerca de 48 mil tweets rotulados em 8 assuntos.',\n",
       " '[11], vencedores da RepLab2014 [1], prop√µem resolver esta tarefa usando a Wikip√©dia para enriquecer as publica√ß√µes e treinando um classificador SVM (Support Vector Machines) para aprender a classificar as dimens√µes de reputa√ß√£o.',\n",
       " 'Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 [1] e as m√©tricas de avalia√ß√£o.',\n",
       " 'The main idea is then to predict For selecting relevant users we could make use of existing resources such as analytical platforms, like Socialbakers \\uf0d2 , or existing datasets, like RepLab 2014 [34].',\n",
       " '[70] utilising the RepLab 2014 corpus [71] and Azzouza et al.',\n",
       " 'It was noticeable that the majority of studies sought to create their own training data, with only 6 studies using publicly available precollected data sets, and only 4 of these originated from gold-standard data sets produced as part of NLP community challenges like RepLab and SemEval, focused on reputational classification [71] and sentiment [69,63].',\n",
       " 'Other studies aimed at specific user profiles such as campaign promoters, bots, influencers, political position and polarity (Amig√≥ et al., 2014;Li et al.',\n",
       " 'In the RepLab 2014 edition (Amig√≥ et al., 2014), one of the objectives was author profiling in the automotive and banking domains.',\n",
       " 'We also evaluated our results against the RepLab dataset ( We also evaluated our results against the RepLab dataset ( Amig√≥ et al., 2014) with the revision of tags presented in (Nebot et al.',\n",
       " 'It must be noticed that the evaluation results are not comparable to those published in It must be noticed that the evaluation results are not comparable to those published in (Amig√≥ et al., 2014) and (Nebot et al.',\n",
       " 'Regarding reputation, most work has been focused on identifying the influential users in a specific domain Regarding reputation, most work has been focused on identifying the influential users in a specific domain (Amigo ¬¥et al. 2014).',\n",
       " \"These categories are inspired in the RepLab 2014 dataset and designed according to the experts' criteria involved in the project (Amigo ¬¥et al. 2014).\",\n",
       " 'The first one is the RepLab 2014 dataset (Amigo ¬¥et al. 2014), which contains a track for the automotive domain.',\n",
       " 'Although significant advances have been made in RepLab 1 [1,2].',\n",
       " 'Most of the contributions on reputation monitoring to extract sets of tweets requiring a particular attention from a reputation manager have been proposed in the last editions of RepLab [1,2].',\n",
       " \"RepLab'2014 [2] focused on the reputation dimension classification.\",\n",
       " \"We perform a supervised classification over Replab'2013-14 dataset We perform a supervised classification over Replab'2013-14 dataset [1,2].\",\n",
       " 'Language technologies hold the potential for making information easier to understand and access [17].',\n",
       " '1) [1].',\n",
       " 'Such exception, the CLEF eHealth Evaluation-lab and Lab-workshop Series1 has been organized every year since 2012 as part of the Conference and Labs of the Evaluation Forum (CLEF) [4,5,[8][9][10]13,16,17].',\n",
       " 'The work by [35] focused on evaluating the impacts and designing a patient portal for inpatient that would provide patients with a 24 hours free admittance to particular medical facts with internet connection from anywhere.',\n",
       " 'Work by [35] proposed that sufficient consideration should to be accorded to the information requests of various classes of information seekers.',\n",
       " 'The dataset created for the ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task is similar to the previous one The dataset created for the ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task is similar to the previous one [7].',\n",
       " 'Similar to previous ImageCLEF annotation tasks Similar to previous ImageCLEF annotation tasks [9,10,[23][24][25], the 2019 ImageCLEFcoral task will require participants to automatically annotate and localize a collection of images with types of benthic substrate, such as hard coral and sponge.',\n",
       " 'Nevertheless, and understandably, the focus resided on image processing and, so far, the methods used for text similarity for the purpose of image retrieval are fairly mainstream [17].',\n",
       " 'In this work we adopt the ImageCLEF 2012 Photo Annotation dataset [44] as our training set.',\n",
       " 'We consider three different sources for unpaired text data to train the language model: (1) MSCOCO consists of all captions from the MSCOCO train set (2) Text from Image Description Corpora (Caption-Txt) consists of text data from other paired image and video description datasets: Flickr1M [13], Flickr30k [11], Pascal-1k [25] and ImageCLEF-2012 [32] and sentence descriptions of Youtube clips from the MSVD training corpus.',\n",
       " 'Experiments have been carried out using two datasets, namely a subset of the MIRFLICKRExperiments have been carried out using two datasets, namely a subset of the MIRFLICKR1 collection proposed for the ImageCLEF 2012 Photo Flickr Annotation Task [24] and the MICC-Flickr101 dataset [1].',\n",
       " 'Triggered through benchmark collections, for image retrieval [37] and benchmarking tasks [33], a large body of works focuses on how to detect objects in images (e.',\n",
       " 'However, the focus resided on image processing and, so far, the methods used for text similarity for the purpose of multimodal retrieval are fairly mainstream [24].',\n",
       " ', 2011;Thomee and Popescu, 2012], shows that the first model outperforms the state-of-the-art methods on three out of five datasets and the second proposed model outperforms the state-of-the-art methods on the five considered datasets on a tag-based image annotation task.',\n",
       " ', 2010;Thomee and Popescu, 2012].',\n",
       " ', 2011;Thomee and Popescu, 2012] shows that our framework achieves comparable and better results compared to more sophisticated state-of-the-art approaches as summarized in Table 1.',\n",
       " \"‚Ä¢ ImageClef '12 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2012 photo annotation challenge ‚Ä¢ ImageClef '12 is used to refer to the subset of the MIR-Flickr that was used within the ImageCLEF 2012 photo annotation challenge [Thomee and Popescu, 2012].\",\n",
       " 'In 2011, 2012 and 2013 respectively 8, 10 and 12 international research groups did cross the finish line of this large collaborative evaluation by benchmarking their images-based plant identification systems (see [14], [15] and [19] for more details).',\n",
       " 'in [22,5,9] or in the more recent methods evaluated in [15]).',\n",
       " 'Our descriptors have been tested on three leaf datasets: the Flavia dataset Our descriptors have been tested on three leaf datasets: the Flavia dataset [23], the ImageCLEF dataset in 2011 [8] and in 2012 [9].',\n",
       " 'The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see [9] for details).',\n",
       " '2012, Pl@ntNet-Identify: an important milestone was marked in 2012 with the launch of the Pl@ntNet-Identify web application 2012, Pl@ntNet-Identify: an important milestone was marked in 2012 with the launch of the Pl@ntNet-Identify web application 6 and the development of an end-to-end innovative workflow involving the members of the Tela Botanica social network [17].',\n",
       " 'Details about the data, the methodology, the participants and the results can be found in the overview working notes produced each year [13,12,11].',\n",
       " 'The ImageCLEF 2012 The ImageCLEF 2012 [26] image dataset was adopted in this work in order to evaluate the proposed methodology by considering the plant identification species from its leaves.',\n",
       " 'In 2011, 2012 and 2013 respectively 8, 11 and 12 international research groups participated in this large collaborative evaluation by benchmarking their image-based plant identification systems (see [16,17,18] for more details).',\n",
       " 'Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset Our descriptors have been tested on four leaf datasets: the Swedish leaf dataset [35], the Flavia dataset [38], the Im-ageCLEF dataset in 2011 [16] and in 2012 [17].',\n",
       " 'The formula used to rank the runs in the ImageCLEF2012 plant identification task The formula used to rank the runs in the ImageCLEF2012 plant identification task [17] is nearly the same as in 2011 (see [17] for details).',\n",
       " 'The formula used to rank the runs in the ImageCLEF2012 plant identification task The formula used to rank the runs in the ImageCLEF2012 plant identification task [17] is nearly the same as in 2011 (see [17] for details).',\n",
       " 'Evaluation results on scans and scan-like leaf images of the ImageCLEF 2011 and ImageCLEF 2012 plant identification tasks [9,10] are reported and discussed in Section 3.',\n",
       " 'Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 Our descriptors have been tested on two leaf datasets: the scans and the scan-like images of the ImageCLEF datasets in 2011 [9] and in 2012 [10].',\n",
       " 'The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details).',\n",
       " 'The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details).',\n",
       " 'To do so, we use a subset of the training set of ImageCLEF 2012 plant identification task [2] freely available 2 : the training sets of scans and scan-like images are merged into one to form our knowledge database.',\n",
       " ', 2012;Go√´au et al., 2012), the analysis of plant vein structure (Dhondt et al.',\n",
       " 'While plant related datasets exist for leaf or flower recognition (Go√´au et al., 2012;Silva et al.',\n",
       " 'We ran experiments on the following datasets: Flavia We ran experiments on the following datasets: Flavia [21], Foliage [10], Plant-CLEF2012 [6] and PlantCLEF2013 [5].',\n",
       " 'In 2011, 2012 and 2013 respectively 8, 11 and 12 research groups participated in this large collaborative evaluation by benchmarking their image-based plant identification systems (see [17,18,16] for more details).',\n",
       " 'The ImageCLEF 2011 [50] and ImageCLEF 2012 [51] datasets were used for the experiments.',\n",
       " 'In order to measure progress in a sustainable and repeatable way, the LifeCLEF [15] research platform was created in 2014 as a continuation and extension of the plant identification task [37] that had been run within the ImageCLEF lab [12] since 2011 [32][33][34].',\n",
       " 'The Pl@antLeaves II The Pl@antLeaves II [48] dataset is a subset of the ImageCLEF2012 [49] dataset, which contains different types of leaves from trees of the Mediterranean region of France.',\n",
       " 'Unreleased sequences from ViDRILO have been successfully used in the RobotVision at Image-CLEF competition [13] in 2013 [3] and 2014 [2].',\n",
       " 'Finally, the fifth edition [17] included unprocessed 3D information in the form point cloud files (PCD format [18]).',\n",
       " 'This dataset includes images of the environment and point cloud files (in PCD format) [17].',\n",
       " 'The fifth edition The fifth edition [17] encouraged participants for using 3D information (point cloud files) with the inclusion of rooms completely imaged in dark.',\n",
       " 'Some web-accessible retrieval systems such as Goldminer 7 or Yottalook8 allow users to filter the search results by modality [36].',\n",
       " 'ImageCLEFmed1 , an annual evaluation campaign on retrieval of images from the biomedical open access literature [8].',\n",
       " 'For this reason, the ImageCLEF2012 medical data set was used [28].',\n",
       " 'For this reason, the ImageCLEF2012 medical data set was used [28].',\n",
       " 'The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].',\n",
       " 'Similar to retrieval, it has shown that the classification results have better accuracy by combining text and images, in most cases, than the results using either text or image features alone [10][11][12].',\n",
       " '-And finally, a performed systematic retrieval evaluation in a standard benchmark ImageCLEFmed 2012 evaluation [12] dataset of more than 300,000 images with asso-ciated annotations that demonstrated significant improvement in performance comparatively.',\n",
       " 'The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.',\n",
       " 'The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks The importance of medical illustrations in clinical decision-making has motivated the development of large databases of medical images, such as the Public Health Image Library (PHIL) and GoldMiner, as well as active research in image retrieval within the yearly ImageCLEF medical image retrieval tasks [10][11][12] and by individual researchers.',\n",
       " '[27]) for ImageCLEFmed 2012 classification task [12] to a new hierarchy with the same acronyms for classes as shown in Fig.',\n",
       " 'For the purposes of this research, we used the Image-CLEFmed 2012 dataset For the purposes of this research, we used the Image-CLEFmed 2012 dataset [12] which contains over 300,000 images from 75,000 biomedical journal articles in the open access literature subset of the PubMed Central 11 database.',\n",
       " 'For the purposes of this research, we used the Image-CLEFmed 2012 dataset For the purposes of this research, we used the Image-CLEFmed 2012 dataset [12] which contains over 300,000 images from 75,000 biomedical journal articles in the open access literature subset of the PubMed Central 11 database.',\n",
       " '2D images from the same anatomic regions as the selected 3D volumes (head, chest) were chosen from the publicly available ImageCLEF medical database 2D images from the same anatomic regions as the selected 3D volumes (head, chest) were chosen from the publicly available ImageCLEF medical database [18] as query images.',\n",
       " 'Since 2004, the medical task of ImageCLEF (Im-ageCLEFmed) aims at evaluating the performance of medical image retrieval systems [7,8].',\n",
       " 'Already many web-accessible search systems such as OpenI2  [16], Goldminer3 or Yottalook4 allow users to limit the search results to a particular modality [8] as this is a feature often requested by end users [17].',\n",
       " 'Therefore, medical image retrieval has attracted much more attention in recent years [11,12,7,4].',\n",
       " 'We notice that most works target at retrieving similar objects in image content [11,12] or the same imaging modalities [7,4] for a given query image.',\n",
       " 'Already, many web-accessible search systems such as Goldminer or Yottalook allow users to limit the search results to a particular modality (M√ºller, Garc√≠a Seco de Herrera, et al., 2012), as this is a feature often requested by end users (Markonis et al.',\n",
       " 'For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past (M√ºller, Garc√≠a Seco de Herrera, et al., 2012).',\n",
       " 'The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 The original impetus for most work done in the biomedical image modality classification was the ImageCLEF modality classification or detection sub-task running from 2010 to 2013 [1,[11][12][13].',\n",
       " 'The analysis is based on the overview articles of these years (Clough et al, 2005(Clough et al, , 2006;;M√ºller et al, 2008a;M√ºller et al, 2009;Radhouani et al, 2009;M√ºller et al, 2010b;Kalpathy-Cramer et al, 2011;M√ºller et al, 2012;Garc√≠a Seco de Herrera et al, 2013, 2015, 2016;Dicente Cid et al, 2017;Eickhoff et al, 2017;M√ºller et al, 2006) and is summarized in Table 1.',\n",
       " 'For the development of modality classification models, we employed the challenging ImageCLEF-2012 medical image dataset 12,13 .',\n",
       " 'presented an overview of ImageCLEF-2012 image retrieval task for the 9th edition of Image-CLEF Muller et al.',\n",
       " 'After the evolution of ImageCLEF, three additional data sets were added: 230,088 images for the 2011 [50] data set, and 306,539 images for both the 2012 [51] and 2013 [52] data sets.',\n",
       " 'The image dataset, topics and ground truth of ImageCLEF 2012 medical image retrieval task The image dataset, topics and ground truth of ImageCLEF 2012 medical image retrieval task [10] were used in this evaluation.',\n",
       " 'eu/portal/) and other datasets of annotations extracted from it [Petras et al. 2012].',\n",
       " 'The CHiC track The CHiC track (Petras et al. 2012(Petras et al.',\n",
       " 'RepLab 2012 [10], RepLab 2013 [8] and RepLab 2014 [9] where teams were given a set of entities and for each entity a set of tweets were provided.',\n",
       " ', 2013), the RepLab workshop in the CLEF conference (Amig√≥ et al., 2012), and the Sentiment Analysis in Twitter task (Task 2) in the last SemEval workshop (Nakov et al.',\n",
       " ', 2017) and RepLab (Amig√≥ et al., 2012(Amig√≥ et al.',\n",
       " 'Entity retrieval was studied at the TREC entity retrieval track [2], at INEX with the entity ranking [12] and linked data tracks [32], the workshop on Entity Oriented and Semantic Search [1,5], and other venues.',\n",
       " 'We study this in the context of the INEX Social Book Search Track2  [12,14,15].',\n",
       " 'The INEX Social Book Search Track The INEX Social Book Search Track [12,14,15] investigates book search in collections with both professional metadata and social media content.',\n",
       " 'For the retrieval system, we use the Amazon/LibraryThing collection [2] that is also used in the INEX Social Book Search Track [15].',\n",
       " 'INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time INEX 2011 Social Book Search Track announced the task searching books with social information in the INEX evaluations for the first time [43].',\n",
       " '2009) or in the social book search lab (Bogers et al. 2014).',\n",
       " 'The general background of the INEX workshop is best summarized in INEX reports published within INEX workshops The general background of the INEX workshop is best summarized in INEX reports published within INEX workshops [24][25][26][27][28][29][30] and the SIGIR Forum [1,2,22], the biannual publication of the ACM Specific Interest Group on Information Retrieval (SIGIR).',\n",
       " \"All these tasks, as well as the participants' approaches, are described in the corresponding Book Track overviews [23][24][25][26][27][28][29][30].\",\n",
       " 'Other topics of research for which online book discussion has served as input includes support for readers advisory services in the public library (Ridenour and Jeong 2016;Spiteri and Pecoskie 2016) and for social book search from an information retrieval perspective (Koolen et al. 2013).',\n",
       " 'Social Book Search (SBS) Social Book Search (SBS) (Koolen et al. 2014) finally really ended the earlier scanned books tasks, and fully focused on the social book data also used in the iSBS track.',\n",
       " 'As reported in [39], there are no special studies regarding human judgement on text informativeness; however, it is a common evaluation criterion in the INEX Tweet Contextualization task at CLEF [40], [12], [13].',\n",
       " 'This motivated the proposal of a new track of Tweet Contextualization at INEX 1 in 2011 [71] which became a CLEF Lab 2 in 2012 [72] and that we fully depict in this paper.',\n",
       " 'It appeared that the use of paragraphs improves readability but does not allow informativeness optimization [72].',\n",
       " 'Dissimilarity values are very close, however differences are often statistically significant (details are not provided here but can be found in [72] for 2012 and in [8] for 2013).',\n",
       " \"Nous pr√©sentons dans cet article le mod√®le de RI puis l'approche de r√©sum√© automatique qui constituent notre syst√®me de contextualisation, puis nous √©valuons notre approche en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d 'INEX 2012(SanJuan et al., 2012).\",\n",
       " \"Nous utilisons la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs Nous utilisons la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs (SanJuan et al., 2012).\",\n",
       " \"Les organisateurs ont donc propos√© une mesure d'√©valuation qui calcule une divergence entre le contexte produit et les phrases jug√©es pertinentes (SanJuan et al., 2012).\",\n",
       " 'To evaluate our method, we compare the system rankings we obtained using the informativeness measure proposed in [6] with the official system rankings based on document relevance, considering various TREC collections on adhoc tasks.',\n",
       " \"Nous √©valuons les performances de l'approche propos√©e en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d'INEX Nous √©valuons les performances de l'approche propos√©e en utilisant l'ensemble de donn√©es issu de la t√¢che Tweet Contextualization d'INEX [22], qui propose un cadre exp√©rimental permettant d'√©valuer la contextualisation de Tweets r√©alis√©e √† l'aide de phrases issues de Wikip√©dia.\",\n",
       " \"Nous utilisons dans un premier temps la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs Nous utilisons dans un premier temps la collection de test de la t√¢che Tweet Contextualization d'INEX 2012 pour nos exp√©rimentations ainsi que les diff√©rentes donn√©es mises √† disposition par les organisateurs [22].\",\n",
       " \"Les organisateurs ont donc propos√© une mesure d'√©valuation qui calcule une divergence entre le contexte produit et les phrases jug√©es pertinentes [4,22].\",\n",
       " 'Tweet Contextualization (TC) Tweet Contextualization (TC) (SanJuan et al. 2012) was also a new track, but directly derived from the INEX 2011 Question Answering Track, which focused on more NLP-oriented tasks and moved to multidocument summarization.',\n",
       " \"La collection utilis√©e pour l'√©valuation dans campagnes INEX Tweet Contextualization 2011 et 2012 a √©t√© d√©crite dans La collection utilis√©e pour l'√©valuation dans campagnes INEX Tweet Contextualization 2011 et 2012 a √©t√© d√©crite dans (SanJuan et al., 2012).\",\n",
       " 'Although there are corpora available for other BioNLP tasks, such as text classification 16 and question answering 17 , these are not covered in this survey.',\n",
       " \"Question Answering for Machine Reading Evaluation (QA4MRE): Biomedical Text about Alzheimer's Disease QA4MRE2 for biomedical data [25] differs from TREC datasets because the focus of the dataset is on passage comprehension and multiple answers are already provided with each question.\",\n",
       " 'The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease The CLEF initiative began in Europe in 2000, and at the same time that the first CLEF eHealth evaluation lab with 3 shared tasks was launched in 2013, the CLEF Question Answering for Machine Reading lab introduced a pilot task on machine reading on biomedical text about Alzheimer disease [28].',\n",
       " 'The detection of the correct answer is specifically designed to require various types of inference, and the consideration of prior knowledge acquired from a collection of reference documents [6,7].',\n",
       " 'The entrance exam task was first proposed in 2013 as a pilot task The entrance exam task was first proposed in 2013 as a pilot task [12] in the Question Answering for Machine Reading Evaluation (QA4MRE) lab, which has been offered at the CLEF conference1 since 2011 [10,11].',\n",
       " '1975), statAP (Pavlu and Aslam 2007), MTC (Carterette et al.',\n",
       " \"While contributing to performance on certain benchmarks, the over-reliance on specific entity names leads to an overestimation of model's actual ability to read and comprehend the provided passage (Pe√±as et al., 2011).\",\n",
       " 'Pe√±as et al. (2011) take a coarser, epistemic approach, asking whether events are asserted, negated, or speculated, and Saurƒ± et al.',\n",
       " 'An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators (21) in which a ranked list of suspects is automatically created to prioritize the investigation.',\n",
       " 'We followed the logic of the automatic identification of sexual predators We followed the logic of the automatic identification of sexual predators (21).',\n",
       " 'It could also be used to learn the general profile of a group of persons such as sexual predators (Inches, Crestani 2012).',\n",
       " 'k See Inches & Crestani k See Inches & Crestani [51] for further detail related to creation of the Dataset.',\n",
       " 'p See participant run for gomezhidalgo12-2012-06-15-1 in Inches & Crestani p See participant run for gomezhidalgo12-2012-06-15-1 in Inches & Crestani [51].',\n",
       " \"Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN '12) [11], has focused on the automatic identification of predators in chat-room logs.\",\n",
       " '[ [ Inches and Crestani 2012] cover the 2012 International Sexual Predator Identification Competition, detailing a common evaluation framework against which 16 methods for identification of sexual predators could be evaluated in a comparable manner.',\n",
       " 'Additional comparable approaches can be seen in the results reported by [Inches and Crestani 2012], our figure for [Villatoro-Tello et al.',\n",
       " '[32]: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).',\n",
       " \"Therefore, we included texts from the previous year's shared task on sexual predator identification [14].\",\n",
       " 'Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 [Inches and Crestani, 2012].',\n",
       " 'Para a composic ¬∏√£o das conversas n√£o-predat√≥rias, √© apresentado um m√©todo de extrac ¬∏√£o, transformac ¬∏√£o e selec ¬∏√£o de chats em comunidades virtuais, baseado em um dos trabalhos desenvolvidos na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012].',\n",
       " 'Para a composic ¬∏√£o das conversas n√£o-predat√≥rias, √© apresentado um m√©todo de extrac ¬∏√£o, transformac ¬∏√£o e selec ¬∏√£o de chats em comunidades virtuais, baseado em um dos trabalhos desenvolvidos na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012].',\n",
       " 'Nesse cen√°rio, o presente trabalho apresenta tr√™s contribuic ¬∏√µes: (i) criac ¬∏√£o do conjunto de dados PREDADORES-BR de acordo com o m√©todo descrito em [Inches and Crestani 2012], utilizado na competic ¬∏√£o PAN-2012, composto por conversas regulares ocorridas em comunidades virtuais e conversas com a presenc ¬∏a de predadores sexuais; (ii) an√°lise estat√≠stica do conjunto de dados PREDADORES-BR com base no m√©todo proposto em [Sokolova and Bobicev 2018]; (iii) avaliac ¬∏√£o experimental considerando os algoritmos que correspondem ao estado da arte em aprendizado de m√°quina no dom√≠nio da pesquisa.',\n",
       " 'Na competic ¬∏√£o PAN-2012 Na competic ¬∏√£o PAN-2012 [Inches and Crestani 2012], √© proposto o uso de duas sub-categorias para conversas n√£o-predat√≥rias: 1) Conversas regulares, isto √©, conversas pertencentes √† diversas categorias, realizadas, originalmente, em formato textual e que n√£o apresentam termos sexuais; e 2) Conversas pertencentes a categoria adulta.',\n",
       " 'Essa caracter√≠stica do modelo CNN-RMSPROP √© considerada importante, visto que, no mundo real, por conta de todo o tempo desprendido e da mobilizac ¬∏√£o de profissionais da lei para atuar na investigac ¬∏√£o de um caso de suspeita de atividade predat√≥ria, a assertividade na identificac ¬∏√£o de uma conversa predat√≥ria √© priorizada, e essa caracter√≠stica se encontra refletida na medida Precis√£o [Inches and Crestani 2012].',\n",
       " 'A obtenc ¬∏√£o de conversas regulares √© considerada uma tarefa complexa, porque, normalmente, apresentam informac ¬∏√µes sens√≠veis ou restritas √†s pessoas envolvidas nas conversas particulares [Inches and Crestani 2012].',\n",
       " \"Moreover, since the seduction stage often shows similar characteristics with adults' or teenagers' flirting, initial studies trying to detect predatory behaviour directly on the user level typically resulted in numerous false positives when they were applied to non-predatory sexually-oriented chat conversations in the PAN 2012 dataset [27].\",\n",
       " 'Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults [27].',\n",
       " 'Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults Recently, the detection of Internet child sex offenders has been extensively investigated in the framework of the PAN 2012 competition, during which efforts have been made to pair the PJ data with a whole range of non-predatory data, including cybersex conversations between adults [27].',\n",
       " 'NLP research in understanding these chats is crucial because law enforcement agencies have become overwhelmed with online cases; automated systems are needed in order to sift through the available textual data and transcripts to improve case triage through identification of criminal activity (Inches and Crestani, 2012).',\n",
       " 'This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).',\n",
       " 'This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).',\n",
       " 'This competition aimed to provide researchers with an initial benchmark for comparing different methods of detecting cyberpeodophiles or sexual groomers by using PAN-12, a large dataset of chat logs between convicted sex offenders and volunteers posing as children (created by Perverted Justice [67]).',\n",
       " 'In 2012, we saw a spike in the literature, likely due to the PAN12 competition for predator detection [67].',\n",
       " 'The most popular public datasets used in the literature for identifying sexual groomers include Perverted Justice (PJ) dataset [47] (N=22, 30%) and dataset from PAN-2012 competition (N=16, 22%) [67].',\n",
       " 'PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].',\n",
       " 'PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].',\n",
       " 'PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].',\n",
       " 'PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].',\n",
       " 'PAN-12 was the first to benchmark performances of the models by the standard Information Retrieval measure of Precision (P), Recall (R), and F measure (weighted harmonic mean between Precision and Recall) [67].',\n",
       " 'As the data acquisition for underage victims or law enforcement officers posing as children is challenging due to the laws and procedure involved [67], the PJ dataset focused on data from predators and adult volunteers posing as children.',\n",
       " 'We demonstrate the applicability of the proposed approach on the publicly available PAN 2012 dataset [24] where our results indicate a high detection rate of true positives.',\n",
       " 'The PAN 2012 [24] competition on identifying online child groomers collected these conversations from several resources to cover all characteristics of real-world data.',\n",
       " 'All the mentioned conversations were in two different datasets as training and testing set All the mentioned conversations were in two different datasets as training and testing set [24].',\n",
       " '5 as the scale for measuring the performance of the grooming detection approaches [2,24].',\n",
       " '5 -score is essential in the case of grooming detection [24].',\n",
       " 'Another strain of literature has focused on identifying predators from a mixed corpus of illicit and everyday conversations Another strain of literature has focused on identifying predators from a mixed corpus of illicit and everyday conversations [20,26,27,29,41,45].',\n",
       " 'Juola [42] Authorship Profiling (detecting age and gender): e.',\n",
       " ') PAN12: from the state-of-the-art corpus, especially created for the use in authorship identification for the PAN 2012 workshop5  (Juola, 2012), all closed-classed problems have been chosen -type: misc, authors: 3-16, documents: 6-16, samples per author: 2.',\n",
       " 'It was evaluated over two datasets, a baseline developed by the author and the dataset of the international workshop on plagiarism detection, author identification and author profiling (PAN) 2012 [33] .',\n",
       " 'In 2012, emphasis was put on smaller candidate sets and fiction in English [15].',\n",
       " '[32]: 10s (13)(14)(15)(16)(17), 20s (23)(24)(25)(26)(27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).',\n",
       " '‚Ä¢ PAN12 [42] unlike the short email lengths per author in PAN11, this dataset consisted of dense volumes per author.',\n",
       " 'Table 2 presents the structure of each PAN benchmark [40].',\n",
       " 'This is a basic design principle used in user profiling to check whether questionable content is written by the same user who publishes a wide range of similar content [21] .',\n",
       " \"The BioASQ challenge is a notable example of recent use of deep learning to bibliometrics The BioASQ challenge is a notable example of recent use of deep learning to bibliometrics [22] related task, but BioASQ's researcher and bibliometrician have so far explored the topic in parallel.\",\n",
       " 'Several were based on BERT and BioBERT [22].',\n",
       " 'org/ resul ts/ 8b/ phaseB/ ing task in BioASQ 6b and 7b challenges; in 8b challenge, they experimented with different pre-trained language models, such as BERT [5], BioBERT [7], XLNet [13] and SpanBERT [33], combining with transfer learning and voting method [34], to better solve biomedical factoid QA task.',\n",
       " 'To execute this task, we used the BioASQ-8b dataset (Nentidis et al., 2020) for different question types, i.',\n",
       " 'For each task, we gather the best performance, and specifically, they are BioASQ-8b (Nentidis et al., 2020), Chemprot (Peng et al.',\n",
       " ', , 2020;;Shaar et al., 2020;Nakov et al.',\n",
       " 'Exploiting sources of evidence for Arabic rumor verification in Twitter is still under-studied; existing studies exclusively focused on the tweet text for verification [18,20,28,2,31,5].',\n",
       " ', 2018;Barron-Cedeno et al., 2020).',\n",
       " ', 2020;Shaar et al., 2020;Hasanain et al.',\n",
       " ', , 2020;;Shaar et al., 2020;Nakov et al.',\n",
       " 'Study in [79]focused on the task of claim verification, which involves a collection of claims and corresponding evidence in the form of text snippets sourced from web pages [93].',\n",
       " ', 2020;Shaar et al., 2020;Nakov et al.',\n",
       " 'In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media [1,27,36].',\n",
       " ', , 2020;;Shaar et al., 2020;Nakov et al.',\n",
       " 'Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems (Shaar et al., 2020).',\n",
       " 'In light of the wave of misinformation associated with COVID-19 pandemic researchers have been collecting relevant datasets of scientific publications, news articles and their headlines, social media posts and claims about COVID-19 In light of the wave of misinformation associated with COVID-19 pandemic researchers have been collecting relevant datasets of scientific publications, news articles and their headlines, social media posts and claims about COVID-19 (Shaar et al., 2020;Dharawat et al.',\n",
       " ', 2020;Shaar et al., 2020;Hasanain et al.',\n",
       " ', , 2020;;Shaar et al., 2020;Nakov et al.',\n",
       " 'Automated fact-checking systems are pivotal not only for combatting false information on digital media but also for reducing the workload of fact-checkers Automated fact-checking systems are pivotal not only for combatting false information on digital media but also for reducing the workload of fact-checkers [1,2].',\n",
       " 'A key functionality of these systems is the retrieval of already debunked narratives for misinformation claims, which essentially means retrieving previously fact-checked similar claims [2][3][4].',\n",
       " 'Previous methods for training debunked-narrative retrieval models heavily rely on annotated pairs of misinformation claims and debunks Previous methods for training debunked-narrative retrieval models heavily rely on annotated pairs of misinformation claims and debunks [2,4,5].',\n",
       " 'Lab shared task 2020, 2021 and 2022 [2,3,14,23] focus on debunked-narrative retrieval task and release different datasets for training and testing.',\n",
       " 'Lab task datasets which include CLEF 22 2A [3], CLEF 21 2A [14] and CLEF 20 2A [2].',\n",
       " 'We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers We also report previous State-Of-The-Art (SOTA) performance achieved by the winners of the shared tasks on the test set, as published in their respective papers [2,3,14,16].',\n",
       " 'com/view/clef2021-checkthat (4) CLEF 20 2A-EN [2] https://sites.',\n",
       " 'In that year, the team utilizing RoBERTa secured the first position in the English category [9].',\n",
       " 'Feature representation methods, including word embeddings, Bag of Words, Named Entity Recognition, and Part of Speech tagging [6,7,9] have been widely used to enhance model understanding of the task.',\n",
       " 'Clinical coding standardizes medical records for health information management systems so as to perform research studies, monitor health trends or facilitate medical billing and reimbursement [31].',\n",
       " 'CodiEsp dataset CodiEsp dataset [40] is a public dataset released by the CLEF eHealth 2020 conference 5 .',\n",
       " 'It prompted the search for tools that assist manual standardization [6].',\n",
       " '‚Ä¢ An updated list of term variants documented in real domain texts, namely the Spanish versions of MedlinePlus [18], and state-of-the-art annotated medical corpora: datasets used in recent shared tasks (CODIESP [19], CANTEMIST [20], Phar-maCoNER [21]), the Chilean Waiting List Corpus (CLWC) [22] and the CT-EBM-SP corpus of texts about clinical trials [23].',\n",
       " 'covd- 19), tokenization mistakes or words with hashtags (e.',\n",
       " 'Our query revealed a number of medical NER challenges for the Spanish language (Table 3), including CLEF eHealth (2020-21) [92][93][94][95][96][97][98][99][100][101], Iber-LEF (2020-22) [17,70,[102][103][104][105][106][107][108][109][110][111][112][113][114][115], and CLEF BioASQ (2022) [116][117][118][119].',\n",
       " 'Being orders of magnitude bigger than previously annotated corpora in historical French Being orders of magnitude bigger than previously annotated corpora in historical French (Ehrmann et al. 2020), contemporary French (Sagot, Richard, and Stern 2012) and even English (Pradhan et al.',\n",
       " \"[60] evaluated systems' performances on various entity noise levels, defined as the length-normalised Levenshtein distance between the OCR surface form of an entity and its manual transcription.\",\n",
       " \"[60] evaluated systems' performances on various entity noise levels, defined as the length-normalised Levenshtein distance between the OCR surface form of an entity and its manual transcription.\",\n",
       " '[55], second on the occasion of the HIPE-2020 shared task [60].',\n",
       " 'The recently organised HIPE shared tasks on named entity recognition and linking in multilingual historical documents are essential first step towards alleviating this situation: both editions, HIPE-2020 [57,60] and HIPE-2022 [58,63,65], have produced significant datasets for the evaluation of NE processing systems on historical material.',\n",
       " 'In the context of the CLEF-HIPE-2020 shared task In the context of the CLEF-HIPE-2020 shared task [60], Dekhili and Sadat [46] proposed different variations of a BiLSTM-CRF network, with and without the in-domain HIPE flair embeddings and/or an attention layer.',\n",
       " 'In this regard, the first CLEF-HIPE-2020 editionIn this regard, the first CLEF-HIPE-2020 edition5  [5] proposed the tasks of NE recognition and classification (NER) and entity linking (EL) in ca.',\n",
       " ', 2019) and HIPE (Ehrmann et al., 2020) annotated historical NER datasets using prototypical NER systems built on the previous and current generation of models, trained on contemporary annotations from ACE 2005 (Walker et al.',\n",
       " 'HIPE (Ehrmann et al., 2020) is a collection of digitized documents covering three different languages: English, French, and German.',\n",
       " 'AI methods and models have provided significant improvements for all of the above applications, for instance, for recognizing text in historical prints (  (Brantl/Schweter 2022), or in the area of named entity recognition and linking (Ehrmann et al. 2020;2022).',\n",
       " \"Ainsi, pour la reconnaissance et la classification d'entit√©s nomm√©es, mais aussi de mani√®re g√©n√©rale, les efforts sont consacr√©s √† la mani√®re de transf√©rer efficacement les connaissances pour l'adaptation au domaine en d√©veloppant des syst√®mes robustes inter-domaines et en explorant l'apprentissage zero-shot ou few-shot pour traiter la coh√©rence et l'inad√©quation des domaines et des annotations dans des contextes inter-domaines (Ehrmann et al., 2020c(Ehrmann et al.\",\n",
       " 'Research into lifelogging has been gaining in popularity with many collaborative benchmarking workshops taking place recently -the NTCIR Lifelog task [12], the Lifelog Search Challenge (LSC) [13] and the ImageCLEFlifelog [23].',\n",
       " 'The baseline for the comparison was the modified version of the Mysc√©al system [29] that had been used in the ImageCLEFlifelog2020 benchmarking workshop and achieved third place out of six participants [23].',\n",
       " 'In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF In recent years, many research challenges have been organized to introduce new problems in the lifelog domain to the research community; ImageCLEF [5][6][7]29], NTCIR [9][10][11], and the Lifelog Search Challenge (LSC) [13].',\n",
       " 'Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges Since then, workshops and tasks have been organized to advance research on some of the key challenges: ImageCLEFlifelog challenges [82][83][84]; Lifelog Search Challenge [85][86][87], which aims to encourage the development of efficient interactive lifelog retrieval systems; and NTCIR Lifelog Tasks [77].',\n",
       " \"Out of the five R's, retrieving lifelog data, typically lifelog photos, has been the subject of the majority of lifelog research, as seen in various workshops [11,12,21].\",\n",
       " 'In the last years, shared lifelogging retrieval tasks were set up on different continents In the last years, shared lifelogging retrieval tasks were set up on different continents [9,14,16,28] in order to bring the attention of a wider audience to lifelogging and to promote research into some of its key challenges.',\n",
       " 'Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].',\n",
       " 'In the last editions of ImageCLEFlifelog In the last editions of ImageCLEFlifelog [4,28], we proposed some preliminary work presenting a baseline approach to solve the task of lifelog moment retrieval (LMRT) [29,30].',\n",
       " 'Although multiple data sources are recorded in multimodal personal datasets [12], only the combination of visual and related metadata including semantic locations, daily-life activities, date and time are employed extensively in research [10,19] while others has not yet been exploited.',\n",
       " 'The Lifelog Search Challenge (LSC) is a comparative benchmarking workshop founded in 2018 [23] to foster advances in multimodal information retrieval similar to previous activities like NTCIR-Lifelog tasks [24][25][26]55] and the ImageCLEF Lifelog tasks [12][13][14]46].',\n",
       " 'To benchmark for different search engines in indexing and retrieving multimodal lifelog data, there have been a number of interactive lifelog retrieval challenges such as NTCIR Lifelog [7,8], ImageCLEF Lifelog [4,5,20], and Lifelog Search Challenge (LSC) [9,10].',\n",
       " 'There are various challenges and workshops have been organized in the field of lifelog image retrieval There are various challenges and workshops have been organized in the field of lifelog image retrieval [5,6,22].',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'Many research challenges, with related tasks and research studies, have been conducted to explore the potential applications of using lifelog data, such as Activities of Daily Living Understanding [5], Solve My Life Puzzle [6], Sports Performance Lifelog [21], and Lifelog Moment Retrieval [5,6,21] at the ImageCLEF conference.',\n",
       " 'Datasets: Lifelogging tasks, such as the Lifelog Search Challenge Datasets: Lifelogging tasks, such as the Lifelog Search Challenge [4] and ImageCLEF lifelog [11] task, have been developed and maintained to facilitate research in the lifelog retrieval domain.',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b8afa0f-c038-4496-b838-4ef5dd3364d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [s for s in flat_list if isinstance(s, str)]\n",
    "filtered_list = list(set(filtered_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "320bacae-2caf-42fe-a7be-0e6ca14243db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dd1ae8b-0dca-4a2a-8f3c-b8abb47d7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 300 random snippets\n",
    "random_selection = random.sample(filtered_list, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ebea9cc-3267-484f-abd8-71da6689626c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in [1].',\n",
       " 'They also implemented training on artificially constructed datasets and reported superior performances on ImageCLEF dataset (Garc√≠a Seco de Herrera et al., 2016).',\n",
       " 'One of the means to improve the access to the Linked Data Web lies in the development of natural-language interfaces that can transform human languages or even controlled languages into a representation suitable for querying large knowledge bases [10].',\n",
       " 'Organizers of this challenge provided a largescale question answering competition, in which the systems are required to cope with all stages of a question answering task, including the retrieval of relevant articles and snippets as well as the provision of natural language answers [30,31].',\n",
       " 'Previous research has shown that di‚Üµerent users tend to issue di‚Üµerent queries for the same information need and that the use of query variations for evaluation of IR systems leads to as much variability as system variations [1,2,23].',\n",
       " ', 2019;Barr√≥n-Cedeno et al., 2020;Hidey et al.',\n",
       " ', 2007), (M√ºller et al., 2008), (M√ºller et al.',\n",
       " ', 2017;de Herrera et al., 2018) and stopped in 2019.',\n",
       " 'Some of the most common tasks, often co-located with international conferences, are those about fake news (Rangel et al., 2020), hate speech (Bosco et al.',\n",
       " 'To effectively leverage the massive explosion of multimedia content, a large number of approaches have been proposed in the areas such as information retrieval, multimedia retrieval and computer vision [2,3,5,11,16].',\n",
       " '‚Ä¢ ImageCLEF ‚Ä¢ ImageCLEF [71]: it consists of more than 250k images belonging to 95 concepts and is split into training, dev and test data; we only consider the dev set, which includes 1,000 images equally split between training and testing, as the ground-truth is released on this dev set only.',\n",
       " 'In recent years there has been an increasing interest in the problem of plant species classification in images In recent years there has been an increasing interest in the problem of plant species classification in images [3,7,12,18].',\n",
       " 'PlantCLEF PlantCLEF [32]: The PlantCLEF dataset is a large-scale dataset for plant identification, comprising millions of images covering thousands of plant species, including trees, flowers, fruits, and leaves.',\n",
       " 'Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task [27] that proposes a dataset consisting of 498,920 news photographs (images and caption text) classified into sub-topics (e.',\n",
       " ', 2021bNakov et al., , 2022)).',\n",
       " ', 2013;Goeuriot et al., 2014) but may also be provided from works of researchers, such as POS-tag (Tsuruoka et al.',\n",
       " 'Moreover, we opted for a ViT model that was pretrained on the plant-relevant dataset PlantCLEF2022 [21].',\n",
       " 'However, several runs for Task 3 inserted some context or definition for difficult terms in additional to language simplification [25,11].',\n",
       " 'Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream.',\n",
       " 'The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5].',\n",
       " 'The third WePS shared task (Artiles et al., 2010) introduced a novel subtask which sought to mine attributes for persons, i.',\n",
       " 'Throughout our experiment, we employed the Large-Scale Complex Question Answering DatasetThroughout our experiment, we employed the Large-Scale Complex Question Answering Dataset12 (LC-QuAD) [28] as well as the 5 th edition of Question Answering over Linked Data Challenge13 (QALD-5) dataset [30].',\n",
       " 'Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task.',\n",
       " 'Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 Researchers worldwide have contributed to achieve a significant improvement on the clinical handover task because of a shared computational task organized in 2016 [51].',\n",
       " 'This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( [5,6]) and is closely related to the Interactive Track of the CLEF Social Book Search Lab of 2015 and 2016 [7,8].',\n",
       " '‚Ä¢ Controversial arguments: Touch√© ‚Ä¢ Controversial arguments: Touch√© (Bondarenko et al., 2020) and ArguAna (Wachsmuth et al.',\n",
       " 'Recent development on GIR systems Recent development on GIR systems [6] evidence that: i) traditional IR systems are able to retrieve the majority of the relevant documents for most queries, but that, ii) they have severe difficulties to generate a pertinent ranking of them.',\n",
       " 'The best results were obtained on blogs for English with an accuracy above 75% for gender and below 60% for age identification [25].',\n",
       " 'In 2012, INEX introduced the Linked Data track In 2012, INEX introduced the Linked Data track [74] with the aim to investigate retrieval techniques over a combination of textual and highly structured data.',\n",
       " 'With the increasing number of high quality datasets, the Semantic Web is seeing a renewed interest in estion Answering (QA) over graph data [18].',\n",
       " '5% accuracy on a far more complex task encompassing 10,000 plant species, characterized by imbalanced, heterogeneous, and noisy visual data [9].',\n",
       " 'Changes in writing styles is important for many problems: diagnosis of neurological diseases Changes in writing styles is important for many problems: diagnosis of neurological diseases [4], authorship attribution [5,6], author profiling [7,8], author identification [9] and fake news detection [10,11].',\n",
       " 'Two evaluation metrics are employed: the image-centered and the observation score Two evaluation metrics are employed: the image-centered and the observation score [16].',\n",
       " 'In this work, 80% of the image randomly selected as training data, whereas the rest (20%) were used for validatio previous studies of deep learning-based computer vision applications [27,28].',\n",
       " 'The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments [7].',\n",
       " 'For the Spanish paper, we use paper abstracts open sourced by the Mesinesp (Gasco et al., 2021).',\n",
       " 'The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6].',\n",
       " 'ROC-AUC has been a standard machine learning metric [16].',\n",
       " 'This is a problem closer to authorship identification;2 see Stamatatos (2009a) and Argamon and Juola (2011) for an overview of state of the art authorship identification approaches that could be exploited when detecting plagiarism from this point of view.',\n",
       " ', 2018), and Webis-Touch√© (Bondarenko et al., 2021).',\n",
       " 'The LifeCLEF Bird task proposes to evaluate one of these challenges The LifeCLEF Bird task proposes to evaluate one of these challenges [12] based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios.',\n",
       " 'The ImageCLEF 2011 [50] and ImageCLEF 2012 [51] datasets were used for the experiments.',\n",
       " 'As users visit selected news publishers, ORP randomly forwards recommendation requests to registered participants [8].',\n",
       " 'Additional comparable approaches can be seen in the results reported by [Inches and Crestani 2012], our figure for [Villatoro-Tello et al.',\n",
       " 'For details of the creation of CLEF18 dataset, we refer to[3,104].',\n",
       " 'Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition [66] .',\n",
       " 'The tasks are: multilingual Information extraction; technologically assisted reviews in empirical medicine; and, patient-centred information retrieval [15].',\n",
       " 'The 2018 ImageCLEF-Med challenge [6] provides a good overview about the approaches and their results.',\n",
       " ', check-worthiness estimation has been severely understudied as a problem [1,5,7].',\n",
       " '(2018), and in public competitions (Kahl et al., 2019).',\n",
       " 'The CLEF-IP tracks included a patent classification task [46], [47], which provided a dataset of more than 1 million patents as the training set to classify 3,000 patents into their IPC subclasses.',\n",
       " '‚Ä¢ PAN12 [42] unlike the short email lengths per author in PAN11, this dataset consisted of dense volumes per author.',\n",
       " 'Al√©m deste trabalho, [Potthast et al. 2016] avaliaram a efici√™ncia de tr√™s m√©todos de ofuscac ¬∏√£o [Keswani et al.',\n",
       " 'Dissimilarity values are very close, however differences are often statistically significant (details are not provided here but can be found in [72] for 2012 and in [8] for 2013).',\n",
       " 'Let us now introduce the context of the plant identification task of ImageCLEF 2011 Let us now introduce the context of the plant identification task of ImageCLEF 2011 [9].',\n",
       " 'While plant related datasets exist for leaf or flower recognition (Go√´au et al., 2012;Silva et al.',\n",
       " 'The ImageCLEF 2012 The ImageCLEF 2012 [26] image dataset was adopted in this work in order to evaluate the proposed methodology by considering the plant identification species from its leaves.',\n",
       " 'Les principales caract√©ristiques de la collection qui a √©t√© utilis√©e dans le cadre de la comp√©tition ImageCLEF 2008 et 2009 (Tsikrika et al.,, 2008 ;Tsikrika et al.',\n",
       " '[20] Natural Images 50,000 10,000 Tiny ImageNet [21] Natural Images (ImageNet subset) 100,000 10,000 Stanford dogs [22] Natural Images (Dog breeds) 12,000 8,580 Flowers-102 [23] Natural Images (Flower species) 2,040 6,149 CUB-200-2011 [24] Natural Images (Bird species) 5,994 5,794 Stanford Cars [25] Natural Images (Car models) 8,144 8,041 Food-101 [26] Natural Images (Food categories) 75,750 25,250 DTD [27] Texture Images 1,880 1,880 47 NEU Surface Defects [28] Surface Defect Images 1,440 360 6 UC Merced Land Use [29] Remote Sensing Images 1,680 420 21 EuroSAT [30] Remote Sensing Images 18,900 8,100 10 PlantVillage [31] Plant Images 44,343 11,105 39 PlantCLEF [32] Plant Images 10,455 1135 20 Galaxy10 DECals [33] Astronomy Images (Galaxy Morphology) Stanford Dogs [22]: Stanford Dogs dataset is a comprehensive dataset for fine-grained image classification, containing 20,580 images of 120 different dog breeds.',\n",
       " 'In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus (Pe√±as et al., 2007).',\n",
       " 'The CLEF Technology Assisted Reviews (TAR) track The CLEF Technology Assisted Reviews (TAR) track [25,26] considers both screening prioritisation and stopping prediction tasks.',\n",
       " 'BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006).',\n",
       " 'Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.',\n",
       " 'Assuming that the transportation network T belongs to a family of models with infinite capacity, which means it has the ability to approximate any continuous function with arbitrary precision, then the optimization problem in ( 4) is equivalent to the optimization problem in (3).',\n",
       " 'There is also a plethora of works on exploiting social media for a variety of tasks, like opinion summarization [24], event and rumor detection [16,28], topic popularity and summarization [2,42], information diffusion [18], popularity prediction [34], and reputation monitoring [1].',\n",
       " 'The main objective of question answering over linked data [17,26] is to facilitate, in part, multilingual access to the information originally produced in different culture and language.',\n",
       " 'Content Generation Content Generation [124] generating new medical descriptions or knowledge based on a given input  [127] 2012 ‚àº4K English RE Link ShARe13 [128] 2013 ‚àº29K English NER Link GENIA13 [129] 2013 ‚àº5K English EE Link NCBI [21] 2014 ‚àº7K English NER Link ShARe14 [130] 2014 ‚àº35K English NER Link CADEC [20] 2015 ‚àº7.',\n",
       " ', 2018;Losada et al., 2019), p is set such that the penalty is 0.',\n",
       " 'Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 [1] e as m√©tricas de avalia√ß√£o.',\n",
       " 'The MSPRL (Kordjamshidi et al., 2017)   10a) for relation types.',\n",
       " '‚Ä¢ In the 2018 PAN authorship attribution task [14], features continued to focus on characters and word n-grams, with various weighting and normalization methods.',\n",
       " 'Notable features that are repeatedly included are reading scores [8,78], part of speech tagging [80], occasions of drug word usage [81] and sentiment or valence measures [78].',\n",
       " 'The training set used for the challenge will be a version of the 2019 training set The training set used for the challenge will be a version of the 2019 training set [40] enriched by new contributions from the Xeno-canto network and a geographic extension.',\n",
       " ', 2010;Nowak and Huiskes, 2010;Nowak et al., 2011;Thomee and Popescu, 2012], shows that the first model outperforms the state-of-the-art methods on three out of five datasets and the second proposed model outperforms the state-of-the-art methods on the five considered datasets on a tag-based image annotation task.',\n",
       " \"Les organisateurs ont donc propos√© une mesure d'√©valuation qui calcule une divergence entre le contexte produit et les phrases jug√©es pertinentes [4,22].\",\n",
       " 'IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks IRMA dataset -The Image Retrieval in Medical Applications (IRMA) dataset, created by the Aachen University of Technology, has been used to evaluate retrieval methods for medical CBIR tasks [25].',\n",
       " 'Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification.',\n",
       " 'The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing The IRMA database with a collection of 14,410 x-ray images taken arbitrary form medical routine is used for training and testing [25], [26], [27].',\n",
       " 'The task and data used in this study are based on the CLEF Lab eRisk task 2 The task and data used in this study are based on the CLEF Lab eRisk task 2 [16].',\n",
       " 'The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors.',\n",
       " 'According to According to [7], author obfuscation performance is measured based on three parameters, namely safety (the ability to disguise the original author from a given text), soundness (the level of similarity between the text modified from the obfuscation process with the original text), and sensibleness (the quality of grammar and legibility of the resulting text).',\n",
       " 'In the LifeCLEF Bird (Audio) Identification Task 2016/2017 algorithm benchmarking competition, the top algorithms were a variation of fully supervised deep learning CNN architecture [35], [36].',\n",
       " 'The 4th edition of the task will follow a similar format to previous editions [2][3][4] where participants automatically segment and label a collection of images that can be used in combination to create three-dimensional models of an underwater environment.',\n",
       " 'We are currently exploring the use of our document correction method for the CLEF speech retrieval task based on oral testimonies [6].',\n",
       " 'Retrieval-based systems were also the top performing submissions of the ImageCLEF Caption Prediction subtask, a task that ran for two consecutive years [20,24].',\n",
       " 'In 2013 In 2013 [10], 2014 [11] and 2015 [12] PAN competition age and gender profiling was done on the English and Spanish datasets with the traditional supervised machine learning approaches: Logistic Regression, Random Forest, SVMs, etc.',\n",
       " 'These are mainly built for the purposes of challenges (Kelly et al., 2013;Goeuriot et al.',\n",
       " 'The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks.',\n",
       " 'Aachen University of Technology provided both of the manually IRMA coded datasets [35].',\n",
       " 'For the style breach detection sub-task, we chose as a baseline model the approach of Khan For the style breach detection sub-task, we chose as a baseline model the approach of Khan [8] which is state-ofthe-art on PAN 2017 Competition [9].',\n",
       " 'This follows a previous task initiated in [7] about cultural microblog contextualization.',\n",
       " 'The GeoCLEF search task examined geographic search in text corpus [18].',\n",
       " 'Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media (Nakov et al., 2018;Elsayed et al.',\n",
       " '2022 competition [25].',\n",
       " ', grouping documents by authorship) [16,23].',\n",
       " 'In their study on a sample of 2000 tweets, Pear Analytics In their study on a sample of 2000 tweets, Pear Analytics [1] classified 40% as containing \"pointless babble\", with another 37.',\n",
       " 'In 2017, the Second Workshop on Lifelogging Tools and Applications was organized simultaneously with the lifelog evaluation tasks, NTCIR-13 Lifelog-2 Task [76] and ImageCLEFlifelog 2017 Task [81].',\n",
       " 'The idea of creating all the possible combinations of components has been proposed by Ferro and Harman The idea of creating all the possible combinations of components has been proposed by Ferro and Harman [22], who noted that a systematic series of experiments on standard collections would have created a GoP, where (ideally) all the combinations of retrieval methods and components are represented, allowing us to gain more insights about the effectiveness of the different components and their interaction; this would have called also for the identification of suitable baselines with respect to which all the comparisons have to be made.',\n",
       " 'The Idiap research team [4] coupled LBP and modSIFT [5].',\n",
       " 'The PAN 2015, 2016, 2017 and 2018 datasets The PAN 2015, 2016, 2017 and 2018 datasets [19,20] consisting of tweets in different languages.',\n",
       " 'It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation (Suominen et al., 2013;Pradhan et al.',\n",
       " 'Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17].',\n",
       " '2005;Hashemi and Kamps 2014;Macdonald et al. 2015).',\n",
       " ', 2020;Shaar et al., 2020;Hasanain et al.',\n",
       " 'In the clinical domain, the ShARe/CLEF 2013 eHealth Eval-51 uation Lab and the i2b2/VA challenge methodologies have been applied in shared tasks 52 [11][12][13].',\n",
       " 'The experiments were conducted with the dataset initially provided by The experiments were conducted with the dataset initially provided by [Losada and Crestani 2016] and published as part of the CLEF eRisk 2017 Task [Losada et al. 2017].',\n",
       " \"There is also a workshop on web people searching, called WePS (standing for Web People Search) [1][2][3], that is concerned with organizing web results for a person's given name.\",\n",
       " 'Table Table 1 shows the results of plagiarism detection in terms of Precision, Recall and Plagdet [11] scores.',\n",
       " 'The Pl@antLeaves II The Pl@antLeaves II [48] dataset is a subset of the ImageCLEF2012 [49] dataset, which contains different types of leaves from trees of the Mediterranean region of France.',\n",
       " 'As far as quantities are concerned, lookups are supported by many methods, over both knowledge graphs and text documents, and are part of major benchmarks, such as QALD [36], NaturalQuestions [22], ComplexWebQuestions [35], LC-QuAD [12] and others.',\n",
       " 'In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations [48].',\n",
       " ', the search for health information by people without special medical expertise (Suominen et al., 2021).',\n",
       " 'This chapter is based on two papers by Balog, Kelly, and Schuth This chapter is based on two papers by Balog, Kelly, and Schuth [15] and Schuth, Balog, and Kelly [164].',\n",
       " 'proposed some automatic and manual methods to evaluate and validate submitted corpora on the first shared task on plagiarism detection data submission (Potthast, Goering, Rosso, & Stein, 2015).',\n",
       " '‚Ä¢ Liga√ß√£o autoral: √© uma vers√£o mais flex√≠vel do agrupamento autoral, em que pares de documentos s√£o ordenados pela confian√ßa de pertencerem ao mesmo autor ‚Ä¢ Liga√ß√£o autoral: √© uma vers√£o mais flex√≠vel do agrupamento autoral, em que pares de documentos s√£o ordenados pela confian√ßa de pertencerem ao mesmo autor (TSCHUGGNALL et al., 2017).',\n",
       " 'In 2015, 1,568 were distributed for the ImageCLEFmed multi-label task [10] In 2016, ImageCLEFmed proposed 5 tasks: compound figure detection; compound figure separation; multi-label classification; subfigure classification and caption prediction.',\n",
       " 'Large online image libraries can be rapidly developed; for example, several hundred thousand images of 10,000 Amazonian plant species [50] have been collected.',\n",
       " 'To reduce the time and effort needed for information extraction from chemical literature and patents, datasets and text mining approaches have been developed on a wide range of information extraction tasks, including named entity recognition and relation extraction To reduce the time and effort needed for information extraction from chemical literature and patents, datasets and text mining approaches have been developed on a wide range of information extraction tasks, including named entity recognition and relation extraction [7,8,9,10,11,12].',\n",
       " 'image search diversification [23][24][25][26][27].',\n",
       " 'The ShARe/CLEF 2014 [6] and SemEval 2015 [7] organized open challenges on detecting disorder mentions (subtask 1) and identifying various attributes (subtask 2) for a given disorder, including negation, severity, body location etc.',\n",
       " 'We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30].',\n",
       " 'The collection is a subset of a larger collection of 77,000 images made available by the medical image retrieval track in 2010 [9] of ImageCLEF1 evaluation.',\n",
       " 'Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28.',\n",
       " '‚Ä¢ RepLab polarity dataset ‚Ä¢ RepLab polarity dataset [1]: A dataset of 84,745 tweets mentioning companies, annotated for polarity as positive, negative or neutral.',\n",
       " 'The data comes from the CLEF-IP 2010 task [ 5], where 134 French patent topics are used to search a collection of 1.',\n",
       " '[104] concluded that \"all submissions, despite their increased level of sophistication in most of the cases, were outperformed by a naive baseline based on character n-grams and cosine similarity\" thus noticing a situation of stall in the progress on AV.',\n",
       " 'The third edition of the Robot Vision challenge The third edition of the Robot Vision challenge [32] was attended by seven groups, with three of them participating to both tasks: mandatory and optional.',\n",
       " '97%) in this competition using stylistic and second-order features into SVM [20].',\n",
       " 'The dataset was the same as the one used for BirdCLEF 2017 The dataset was the same as the one used for BirdCLEF 2017 [4], mostly based on the contributions of the Xeno-Canto network.',\n",
       " 'Given that we restricted the number of images per species to a maximum of 8, while successful deep learning-based plant species identification usually requires thousands of images 12,22 , it seems very unlikely that the models inferred traits from species-specific plant features visible in the imagery.',\n",
       " ', 2019) eHealth dataset is a curated collection of non-technical summaries (NTS) of animal experiments from Germany, which was used to organize the Multilingual Information Extraction Task (Task 1) in the CLEF eHealth Challenge 2019 (D√∂rendahl et al., 2019).',\n",
       " 'Some of these were selected from the Xeno-canto1 database [24,33,34], others were taken from the Birds of Argentina & Uruguay: A Field Guide Total Edition corpus [12,27,30], and finally, several recordings were taken from The Internet Bird Collection2  [1].',\n",
       " '2017aNguyen et al. , 2018) ) which focused on a series of image-retrieval and summarisation focused benchmarking initiatives since 2017, and the Lifelog Search Challenge (LSC) (Gurrin et al.',\n",
       " 'In our research, we used training dataset from PAN conference 2017 In our research, we used training dataset from PAN conference 2017 [8].',\n",
       " 'The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in (Mola-Velasco 2010;Potthast, Stein, and Holfeld 2010).',\n",
       " ', 2014) and previous challenge 2013 (Pradhan et al., 2013), we had focused on the task of named entity recognition for disorder mentions in clinical texs, along with normalization to UMLS CUIs.',\n",
       " 'He is an initiator of the CLEF shared task series Touch√© on argument retrieval (Bondarenko et al., 2022), and co-chaired SemEval tasks on argument reasoning comprehension (Habernal et al.',\n",
       " 'com/sshaar/That-is-a-Known-Lie (2) CLEF 22 2A-EN and 2B-EN [3] https://sites.',\n",
       " 'Future studies could make use of databases such as the CLEF TAR database [54] or the systematic review dataset repository [55].',\n",
       " 'Specifically, we used the 299 clinical records annotated with disorder mentions normalised to concept unique identifiers (CUIs) in SNOMED-CT, released in the context of the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al. 2013), and the 1,500 PubMed abstracts annotated with disease mentions normalised to MeSH IDs, created for the BioCreative V Chemical Disease Relation (CDR) Task (Li et al.',\n",
       " 'Different participating teams experimented with term distribution analysis in a language modeling setting employing the document structure of the patent documents in various ways (Piroi and Tait 2010).',\n",
       " 'There is a growing body of work investigating finegrained image classification of birds There is a growing body of work investigating finegrained image classification of birds [8,24,27], insects [15,18], flowers [6,19] and leaves [1,6,14].',\n",
       " 'The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition.',\n",
       " 'Moreover, methods based on CNNs perform well in multiple fine-grained species identification tasks, including plant species classification [41][42][43][44], dog classification [45], snake classification [46], bird classification [18,47,48] and in general species classification [17,49,50].',\n",
       " 'The used document collection is provided by the Radiological Society of North America (RNSA) and constitutes an important body of medical knowledge issued from peer-reviewed scientific literature (Muller et al., 2008).',\n",
       " '12, which is within the range of visual retrieval results reported for the ImageCLEFmed 2008 medical image retrieval task, and is consistent with the observation that visual retrieval techniques can degrade the overall performance [12].',\n",
       " 'For QALD-  shows the results achieved in QALD-3 for DBpedia test set [105].',\n",
       " 'ImageClef, the CLEF Cross Language Image Retrieval Track54 , is a benchmark for the evaluation of cross-language annotation and retrieval of images (Caputo et al, 2014).',\n",
       " 'An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators An empirical support for the benefits of our methodology is by testing it in a similar context to the one used for the automatic identification of sexual predators (21) in which a ranked list of suspects is automatically created to prioritize the investigation.',\n",
       " 'More detailed descriptions are available in the 2013 and 2014 task overview papers in the CLEF proceedings [14,18].',\n",
       " 'The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP The subwords were learned in a fully unsupervised manner using the morphological segmentation algorithm Morfessor Categories-MAP [34], which has become a standard for unsupervised morphological segmentation [46].',\n",
       " 'The living lab is described in detail in [12].',\n",
       " 'The first one is the RepLab 2014 dataset (Amigo ¬¥et al. 2014), which contains a track for the automotive domain.',\n",
       " 'system developed by The Health on the Net Foundation supports English, French and Italian, while DIOGENE system supports only two languages: English and Italian, but in contrast to cross-lingual systems the question are asked in either Italian or English, retrieval of information is carried out in Italian or English, and the answer is given in the language of the query (Magnini et al., 2003).',\n",
       " 'ImageCLEF 2010 PhotoAnnotation: The Image-CLEF2010 PhotoAnnotation data set[28] consists of 8000 labeled training images taken from flickr and a test set with recently disclosed labels.',\n",
       " 'A similar problem was later reintroduced as the ImageCLEF subfigure classification sub-task in 2015 and 2016 which focused on modality classification of individual subfigures of compound figures, in effect removing the \"COMP\" or compound figure modality [14,15].',\n",
       " 'Only for Arabic it was possible to improve accuracy (albeit less than 2%) [21].',\n",
       " 'We decided to work on the 2008 IRMA database version [5], just considering the third axis of the code: it describes the anatomy, namely which part of the body is depicted, independently to the used acquisition technique or direction.',\n",
       " 'AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks [31,6,7,53,8] offer a very good overview of the most recent trends in the field.',\n",
       " '–û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –û—Å–Ω–æ–≤–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —è–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ [7].',\n",
       " 'The increasing uptake of lifelogging as a personal and practitioner technology have also lead to related activities in ImageCLEF [2], the Lifelog Search Challenge [11] and a related task at MediaEval 2019.',\n",
       " 'Further, research on performing large-scale experiments on automatic approaches for identifying species from imagery data collected from citizen science portals revealed that automated species classification performs similarly and sometimes even better than manual annotations performed by citizen scientists [24,25,29].',\n",
       " 'The same holds for the problem of fine-grained visual categorization (FGVC), where datasets and challenges like PlantCLEF [14][15][16], iNaturalist [17], CUB [18], and Oxford Flowers [19] have triggered the development and evaluation of novel approaches to fine-grained domain adaptation [20], domain specific transfer learning [21], image retrieval [22][23][24], unsupervised visual representation [25,26], few-shot learning [27], transfer learning [21] and prior-shift [28].',\n",
       " 'To detect cities, it combines a heuristic method inspired by the schema introduced in [34] with a rank-reciprocity algorithm that helps it detect misspellings (e.',\n",
       " 'We refer to [14] for details.',\n",
       " 'The organizers shared a dataset of 2,309 unannotated drug 4 Although CLEF eHealth is organized every year [13,14], its main focus is multi-linguality and information retrieval rather than clinical NLP Table 1 Clinical NLP Challenges, the tasks they posed, and the number of participating teams, since 2015, ordered by data sensitivity.',\n",
       " '[5]), sentences in medical newswire are long and complex, often positioning the primary claim(s) within a larger context of other information [96], we obtain 6,000 news articles from the \"Health\" category of Google News during April 2018 and augment this with the top 25 RSS feeds in the \"Health and Healthy Living\" category5 from November 2018 through April 2019 to get over 34,000 news articles.',\n",
       " 'To address the issue of imbalance classes, we included some examples from the Zenodo dataset, which is a COVID-19 false news dataset [41].',\n",
       " 'The Conference and Labs of Evaluation Forum for Early Risk XSL ‚Ä¢ FO RenderX Prediction (CLEF eRISK) is a public competition about different areas such as health and safety [26].',\n",
       " 'In case the user opens the moment-detailed box (3) for further browsing, the user can use the temporal browsing panel (5) to view the previous/after moments of the selected one by horizontal scrolling the panel as well as adjusting the time delta to view the temporal nearby or further apart moments.',\n",
       " 'The dataset is provided by ImageCLEF Tuberculosis 2019 The dataset is provided by ImageCLEF Tuberculosis 2019 [4]  [17], intended for the task of severity scoring (SVR).',\n",
       " 'Extending the prior work inclusion criterion from text to other data modalities, the ImageCLEF lab included annual shared tasks on biomedical image processing from 2005 to 2013 [29][30][31].',\n",
       " 'The thesis is based on in total 8 publications [15,[161][162][163][164][165][166][167].',\n",
       " '2018;Nakov et al. 2018].',\n",
       " 'In image retrieval such community building has started around the ImageCLEFmed [12,36] medical image retrieval benchmark.',\n",
       " 'Various methods have been applied to the task, ranging from SVM based approaches, such as (Kestemont et al., 2018), to transformer based models, like (Bauersfeld et al.',\n",
       " 'Twitter datasets were annotated for the task of reputation monitoring Twitter datasets were annotated for the task of reputation monitoring [1,9].',\n",
       " 'We evaluate HAWK against the QALD We evaluate HAWK against the QALD [21] benchmark.',\n",
       " 'A base de imagens utilizada foi a da ImageCLEF Photographic Retrieval Task [Arni et al. 2009], composta por 20.',\n",
       " 'Motivated by this, we extend our proposed model Motivated by this, we extend our proposed model [9] for the ImageCLEFmedical 2021 [10], [11] by adding an explainability module.',\n",
       " \", 2010] and ImageClef'11 [Nowak et al., 2011] datasets were collected from Flickr but they differ significantly.\",\n",
       " 'With appearance of publicly available audio datasets, such as freefield1010 [2], Warblr [3] or Chernobyl dataset from TREE project 1 and challenges for bird recognition, such as the LifeCLEF Bird Identification Task [4] and the recent Bird Audio Detection (BAD) Challenge [3], the problem has received considerable attention from audio research community.',\n",
       " 'The text simplification task (Ermakova et al. 2021) is a promising research direction for providing simplified explanations.',\n",
       " 'C√©PIDC C√©PIDC [34] (the latter in the context of the participation of SIFR Annotator in the CLEF eHealth 2017 challenge [45]) 13 .',\n",
       " 'A newer version of this dataset is published by the CLEF 2017 NewsREEL [126] task, and a competition is held based on this data to train and evaluate news recommender systems.',\n",
       " 'This problem has been also addressed in PAN Workshop and Competition: Uncovering Plagiarism, Authorship and Social Software Misuse (Argamon, Juola 2011, Juola, Stamatos 2013).',\n",
       " 'A obtenc ¬∏√£o de conversas regulares √© considerada uma tarefa complexa, porque, normalmente, apresentam informac ¬∏√µes sens√≠veis ou restritas √†s pessoas envolvidas nas conversas particulares [Inches and Crestani 2012].',\n",
       " 'For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central.',\n",
       " '73% on the LCF-15 [24], [25] dataset.',\n",
       " ', 2021;Joly et al., 2021;Reedha et al.',\n",
       " 'Furthermore, there is an evidence that the combination or fusion of information from textual and visual sources can improve the overall retrieval quality [17,27].',\n",
       " 'In the ImageClef 2007 medical image classification competition In the ImageClef 2007 medical image classification competition [56], a database of 12,000 categorized radiograph images is used.',\n",
       " ', 2004;Carneiro and Vasconcelos, 2005;Nowak and Huiskes, 2010;Semenovich and Sowmya, 2010;Zhang et al.',\n",
       " 'GRAM2VEC was developed on the publicly available PAN22 corpus (Bevendorff et al., 2022), which we refer to throughout the paper.',\n",
       " 'The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see [9] for details).',\n",
       " 'We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab (H√ºrriyetoglu et al., 2019).',\n",
       " 'Recently, the TREC Dynamic Domain Track (2015-2017) [45], TREC Tasks track (2015-2017) [22] and the CLEF Dynamic Search Lab (2017-2018) [21] have also brought significant benefits to the research progress in this area.',\n",
       " 'In the last years, shared lifelogging retrieval tasks were set up on different continents In the last years, shared lifelogging retrieval tasks were set up on different continents [9,14,16,28] in order to bring the attention of a wider audience to lifelogging and to promote research into some of its key challenges.',\n",
       " 'lab at CLEF2019 (Hasanain et al., 2019).',\n",
       " 'As summed up in As summed up in [10], the systems that took part in the rst edition of this shared task opted for dierent methods, namely: dictionary-based pattern matching, machine learning (including topic modeling) and information retrieval methods.',\n",
       " 'This category involves several species, from the smallest such as insects or birds emitting trains of voiced pulses [2,3] to the largest mammals.',\n",
       " '2016 presidential election (following the same setting as in the official CLEF 2018 competition on check-worthiness detection [17] but using even more data).',\n",
       " 'Although X-rays are frequently used, they have side effects such as exposure to ionizing radiation harmful to the human body and relatively low information when compared to other imaging methods; ‚Ä¢ Computerized Tomography (CT): is a more advanced imaging test that can be used to detect disorders such as cancer that an X-ray could miss [36][37][38][39].',\n",
       " 'Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution Data from fanfiction has been used in NLP research for a variety of tasks, including authorship attribution (Kestemont et al., 2018), action prediction (Vilares and G√≥mez-Rodr√≠guez, 2019), finegrained entity typing (Chu et al.',\n",
       " 'In ImageCLEF 2016 In ImageCLEF 2016 [6], they expand their training set to 20,997 figures and reduce the size of the test set to 3456.',\n",
       " 'Therefore, a future step in the evaluation of our search approach would be to benchmark our methods against these existing IR techniques specifically developed for the prior art search, for example, using the CLEF-IP datasets [35,36].',\n",
       " 'Another witness of this trend are geographic information retrieval systems [5,12,17,19] and in particular local search services, such as Google Maps1 , Yahoo!',\n",
       " 'The last edition of CLEF (2006) The last edition of CLEF (2006) [15] has confirmed that most of the implementations of current CL-QA systems [5,13,18,20,21]are based on the use of on-line translation services.',\n",
       " 'Lab Task 2 (verified claim retrieval ) challenges of the CLEF2020 [25], CLEF2021 [6] and CLEF2022 [7] initiatives in English language.',\n",
       " 'Since 2010, academic competitions have been held to enhance and compare automatic vandalism detection techniques in Wikipedia, using an annotated corpus of changes as the ground truth (Potthast and Holfeld, 2011).',\n",
       " 'As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 (Gonzalo et al., 2010), we have used several measures related to question reformulations: (i) correctness (the fraction of questions for which there is at least one right reformulation offered by our system), obtaining a 75.',\n",
       " 'In the past ten years research projects such as Assert [7], IRMA 1 (Image Retrieval in Medical Applications [8]), and MedGIFT2 (Medical GNU Image Finding Tool, [9]) have advanced the field through a fairly large number of publications and explorations of several sub-domains of medical image re-trieval such as varying feature spaces, interaction with the user [10], evaluation of real user needs [11], and evaluations of image retrieval systems [12].',\n",
       " 'We used data from CLEF-2007 and CLEF-2008 [12,18].',\n",
       " 'The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge [31].',\n",
       " 'Compared to previous results reported for gender detection on Twitter in Italian, as obtained at the 2015 PAN Lab challenge on author profiling (Rangel et al., 2015), and on the TwiSty dataset (Verhoeven et al.',\n",
       " 'The first edition was organized with the aim of investigating age and gender identification in a social media realistic scenario [11].',\n",
       " 'For text-oriented QA, the TREC [2,32] and CLEF [21] conference series offer a wealth of benchmark questions, but there is no design consideration on harnessing structured data at all.',\n",
       " 'For example, the inter-annotator agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-Task dataset (Agirre et al., 2007) was approximately 72.',\n",
       " ', , 2019) ) and verification (Barr√≥n-Cede√±o et al., 2018;Hasanain et al.',\n",
       " 'Interestingly, the participants of the QALD-3 challenge [14] cover a wide range of the formality continuum, even if most of them (4/6) fall in the \"spontaneous natural language\" category.',\n",
       " 'Currently, the used of only visual information still achieves low retrieval performance in this task and the combination of text and visual search is improving and seems promising Currently, the used of only visual information still achieves low retrieval performance in this task and the combination of text and visual search is improving and seems promising [17].',\n",
       " 'Feature representation methods, including word embeddings, Bag of Words, Named Entity Recognition, and Part of Speech tagging [6,7,9] have been widely used to enhance model understanding of the task.',\n",
       " \"Out of the five R's, retrieving lifelog data, typically lifelog photos, has been the subject of the majority of lifelog research, as seen in various workshops [11,12,21].\",\n",
       " 'It was created for evaluation on the image track of the Cross Language Evaluation Forum [5].',\n",
       " '[22] due to the continual, ongoing evolution of misinformation requiring the continual retraining of models.',\n",
       " 'We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset [35].',\n",
       " 'In ImageCLEF2016 [34], they expand the training set to 6776 figures and the test set to 4166 figures.',\n",
       " 'Text stylometry was initially popularized in the area of forensic linguistics, specifically to the problems of author profiling and author attribution (Juola, 2006;Rangel et al., 2013).',\n",
       " 'The transcripts produced with Automatic Speech Recognition (ASR) systems tend to contain many recognition errors, leading to low Information Retrieval (IR) performance [1] unlike the retrieval from broadcast speech, where the lower word error rate did not harm the retrieval [2].',\n",
       " 'This has been most commonly used in shared tasks (Nakov et al., 2021).',\n",
       " ', Lasseck, 2013;Murcia and Paniagua, 2013;Go√´au et al., 2014;Stowell et al.',\n",
       " 'Potthast and Holfeld (2011);Potthast et al. (2010c) for an overview on automatic detection of this kind of \"contribution\".',\n",
       " 'For age detection, we followed what was previously done in [37] and three classes where considered: 10s (13-17), 20s (23-27) and 30s (33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47).',\n",
       " '[143] leverage a recently proposed EL toolkit REL [181] for mining historical newspapers for people, places, and other entities in the CLEF HIPE 2020 evaluation campaign [37].',\n",
       " \"An overview of the approaches of last year's participants of NEWSREEL 2014 is provided in [10].\",\n",
       " 'k See Inches & Crestani k See Inches & Crestani [51] for further detail related to creation of the Dataset.',\n",
       " 'Seit 2015 ist DBIS im Organisationsteam des international angesehenen PAN-Workshops vertreten, der j√§hrliche Tasks rund um das Thema Textanalyse veranstaltet [17,22].',\n",
       " ', 2010;Nowak and Huiskes, 2010;Nowak et al.',\n",
       " 'The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12].',\n",
       " 'The results of this pilot investigation were first presented at the CLEF 2002 Workshop and are reported in Jones and Federico (2003).',\n",
       " 'Effective \"querying agents\" can then simulate users towards developing dynamic search systems [10].',\n",
       " \"Thus, AI recommendation services offer a self-relevant recommendation to consumers [8,23,34], whereas the non-AI recommendation system may provide irrelevant information to consumers' tastes.\",\n",
       " 'We performed the same experiment joining the Answer Validation Exercise4 (AVE) 2006 English data set (Pe√±as et al., 2006) and the Microsoft Research Paraphrase Corpus5 (MSRPC) (Dolan et al.',\n",
       " 'Other applications include open government document requests Other applications include open government document requests [3] and systematic review in medicine [14][15][16]32].',\n",
       " 'While best retrieval accuracy was achieved using a monolingual evaluation task where the queries were English, our results for the cross language task were the best among those making formal submissions to the CLEF 2007 CL-SR task, showing the lowest decrease relative to monolingual performance when queries were translated from their source language to English [12].',\n",
       " 'The last generation of DLAs offer much promise for passing the bottleneck of image or video analysis through automated species identification The last generation of DLAs offer much promise for passing the bottleneck of image or video analysis through automated species identification [20][21][22][23] .',\n",
       " '(GBIF) Train and test occurrences datasets from the previous year edition [5] were merged to feed the current challenge.',\n",
       " ', , 2020Losada et al., , 2021)).',\n",
       " 'Thus, here we are using data provided by Wikipedia, gathered using Mechanical Turk [13].',\n",
       " 'The PlantCLEF 2016 dataset The PlantCLEF 2016 dataset [10] consists of observations of plant specimen and provides annotations in terms of organ, species, genus, and family.',\n",
       " 'O gerenciamento da reputa√ß√£o digital √© uma importante an√°lise que serve para medir como √© a reputa√ß√£o de uma empresa em rela√ß√£o a certos grupos de interessados [1].',\n",
       " '(1)[293] Other(8) [62,171,179,[197][198][199]214,294] Hotel Reviews(24) [[93][94][95][96][97][98][99][100]102,103,[127][128][129][130][131][132][133][135][136][137][138][139][140][141][142] https://doi.',\n",
       " 'In the last few years, the research community has been looking at automatic check-worthiness predictions In the last few years, the research community has been looking at automatic check-worthiness predictions [15,49], at truthfulness detection/credibility assessments [5,12,24,33,34,39], and at developing fact-checking URL recommender systems and text generation models to mitigate the impact of fake news in social media [51,52,56].',\n",
       " 'However, to compare our methods to the ImageCLEF 2010 challenge results, we   4 shows both top performing results of the participants in the ImageCLEF 2010 challenge (see [5] for an overview of the participants, different methods and results) and the performances of our methods.',\n",
       " \"To the best of our knowledge, no existing figure-caption datasets explicitly contain the figures' accompanying documents (Pelka et al., 2021;Hsu et al.\",\n",
       " 'The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details).',\n",
       " 'Retrieval from an archive of oral history has also been well-studied [15], but lacks the multispeaker, multi-genre aspect of podcasts.',\n",
       " 'This motivated the proposal of a new track of Tweet Contextualization at INEX 1 in 2011 [71] which became a CLEF Lab 2 in 2012 [72] and that we fully depict in this paper.',\n",
       " 'Table Table II provides an overview of reported results in literature [25].',\n",
       " ', [1,14,24]), it is often still necessary to involve humans in the fact-checking process.',\n",
       " ', the 2012 Informatics for Integrating Biology and the Bedside (i2b2) challenge [14], the 2013/ 2014 CLEF/ShARe challenges [4], and the 2015/2016/ 2017 Clinical TempEval challenges [5][6][7]).',\n",
       " 'Indeed, our method is among the most successful authorship recognition approaches according to the literature [74].',\n",
       " 'WePS-3 conducted a competitive evaluation on person attribute extraction on web pages WePS-3 conducted a competitive evaluation on person attribute extraction on web pages [2].',\n",
       " 'Otherwise, most papers on Wikipedia vandalism propose automatic vandalism detection tools: Potthast, Stein, and Gerling (2008) first developed machine learning technology for this purpose, and many of the approaches in existence today have been developed or derived from the results of two shared tasks at PAN 2010and PAN 2011(Potthast, Stein, and Holfeld 2010;Potthast and Holfeld 2011).',\n",
       " 'The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 [1] and 2015 [6] was to investigate how professional metadata (title, authors, .',\n",
       " 'Finally, the fifth edition [17] included unprocessed 3D information in the form point cloud files (PCD format [18]).',\n",
       " 'In this respect, the Early Risk Prediction on the Internet (eRisk) [14,15], as well as the Computational Linguistics and Clinical Psychology (CLPsych) [9] workshops were the first to propose benchmarks to bring together many researchers to address the automatic detection of mental disorders in online social media.',\n",
       " '[36,73,52,55,56,69]), the problem is not solved yet.',\n",
       " '2011), flowchart/structure recognition (Piroi et al. 2012(Piroi et al.',\n",
       " 'For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past (M√ºller, Garc√≠a Seco de Herrera, et al., 2012).',\n",
       " '28 in GeoCLEF 2007 [41] and from 0.',\n",
       " \"All these tasks, as well as the participants' approaches, are described in the corresponding Book Track overviews [23][24][25][26][27][28][29][30].\",\n",
       " 'Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014.',\n",
       " 'The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach The IAP problem in text-based image retrieval task is typically addressed by relevance feedback (RF) approach [1].',\n",
       " ', whether the semantics of the original text are preserved) (Hagen et al., 2017).',\n",
       " 'Using the ImageCLEF 2004 CasImage medical database and retrieval task, we have demonstrated the effectiveness of our framework that is very promising when compared to the current automatic runs [17].',\n",
       " 'Our training data is a truly parallel corpus of directly simplified sentences (648 sentences for now) coming from scientific abstracts from the DBLP Citation Network Dataset for Computer Science and Google Scholar and PubMed articles on Health and Medicine [7,8,11,10].',\n",
       " 'Being orders of magnitude bigger than previously annotated corpora in historical French Being orders of magnitude bigger than previously annotated corpora in historical French (Ehrmann et al. 2020), contemporary French (Sagot, Richard, and Stern 2012) and even English (Pradhan et al.',\n",
       " 'The image processing part of the model architecture was identical for both models: a modification of an existing 3D CNN 59 previously applied to assess tuberculosis severity 60 .',\n",
       " \"The TREC Filtering Track [31] and CLEF INFILE [6] focused on the multiple filtering tasks including adaptive filtering, where systems aim to select relevant documents from a stream of incoming documents based on a user's profile.\",\n",
       " 'Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 [Inches and Crestani, 2012].',\n",
       " 'An example living lab is CLEF NEWSREEL 1 An example living lab is CLEF NEWSREEL 1 [13], a campaign-style evaluation lab on news recommendation in realtime which is organized as part of CLEF 2014.',\n",
       " 'For an overview of the challenge, including the participants, different methods and results, see (Nowak and Huiskes, 2010).',\n",
       " 'Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10].',\n",
       " 'The prize winners of the 15th evaluation lab on digital text forensics PAN 2015, which was held in a bid to find the most accurate ways of identifying the gender, age, and psychological traits in accordance with the Five Factor Theory (extroversion, emotional stability/neuroticism, agreeableness, conscientiousness, openness to experience) of Twitter users (Rangel et al., 2015) applied two types of features.',\n",
       " 'In this study, we use the datasets provided by eRisk 2019 and 2020 for the task \"Measuring the Severity of the Signs of Depression\" In this study, we use the datasets provided by eRisk 2019 and 2020 for the task \"Measuring the Severity of the Signs of Depression\" [5,6].',\n",
       " 'PlantCLEF2022 PlantCLEF2022 (Go√´au et al., 2022) is an extensive dataset comprising over 4 million images and includes a wide range of 80,000 plant species.',\n",
       " 'This is best exemplified by the test collection methodology employed by large-scale international efforts, such as TREC 10) , CLEF 11) , NTCIR 12) and in the multimedia field, efforts such as ImageCLEF 13) or MediaEval 14) .',\n",
       " 'A number of solutions have arisen to address the amount of time spent screening documents, including: screening prioritisation (which seeks to re-rank the set of retrieved documents to show more relevant documents first, thus starting the full-text screening earlier), and stopping estimation (which seeks to predict at what point continuing to screen will no longer contribute gain) A number of solutions have arisen to address the amount of time spent screening documents, including: screening prioritisation (which seeks to re-rank the set of retrieved documents to show more relevant documents first, thus starting the full-text screening earlier), and stopping estimation (which seeks to predict at what point continuing to screen will no longer contribute gain) [25,26,40].',\n",
       " 'Erisk@CLEF Erisk@CLEF (Losada et al., 2017) served as a root for this task, which aims to detect depression from the social media data.',\n",
       " 'The CLEF LL4IR track [11] featured product search and web search as use cases.',\n",
       " 'Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently [58].',\n",
       " 'Online reputation management: Reputation management in social media Online reputation management: Reputation management in social media [76] has been proposed in RepLab competitive evaluation campaign for Online Reputation Management Systems [12,11].',\n",
       " 'The same features have been used in our system that won the challenge [5]; for more details see [28].',\n",
       " 'Para avaliar experimentalmente a proposta, ela foi experi-mentada usando o conjunto de publica√ß√µes do desafio do RepLab 2014 [1], que consiste de publica√ß√µes no Twitter extra√≠das em 2012 durante o per√≠odo de 1 o de Junho at√© 31 de Dezembro, com cerca de 48 mil tweets rotulados em 8 assuntos.',\n",
       " \"Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN Previous research on detecting cyberpaedophilia online, including the efforts of the first international sexual predator identification competition (PAN '12) [11], has focused on the automatic identification of predators in chat-room logs.\",\n",
       " 'In all experiments, we used the training dataset from the CLEF CL-SR 2007 task [18].',\n",
       " 'For instance in the LifeCLEF competition, which attempts to classify plants using images of different parts such as the leaves, the fruit, or the stem, the best performing methods all employed deep learning [17], [18], [19], [20].',\n",
       " 'A summary of recent work on algorithms for plant identification and as well as detailed evaluations are described in the CLEF 2011 plant images classification task [4].',\n",
       " 'Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17].']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a6762ed-72f4-4b99-955c-b9fb84ec60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the localhost url for the Llama3 instance\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "606fc95c-597c-4973-bb37-ed8c11af7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the classification with Llama3\n",
    "\n",
    "prompt = \"\"\"\n",
    "\tCan you tell me from the context of the sentence at the end of the prompt if the underlying paper used a dataset related to CLEF? \n",
    "    If there is one or more datasets from CLEF used, please return all names separated by a semicolon. If there are none, please just respond with None. \n",
    "    Please provide no other output sentences except for the list or None.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b82d297d-9eeb-483f-afcc-20e27125880d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n",
      "1 / 300\n"
     ]
    }
   ],
   "source": [
    "list_of_used_datasets = []\n",
    "\n",
    "# Classify the sentence with Llama3 and count how many of the citations are referring to the usage of the underlying dataset \n",
    "counter = 1\n",
    "for i in random_selection:\n",
    "    print(counter, \"/\", len(random_selection)) \n",
    "    prompt = \"\"\"\n",
    "            Can you tell me from the context of the sentence on the end of the prompt, if the underlying paper used a dataset related to CLEF? If there is one or more datasets from CLEF used please return all names separated by a semicolon, if there are none please just responde with None. Please no other \n",
    "            output sentences except for the list or the None.\n",
    "            \"\"\"\n",
    "    prompt = prompt + \"\\nSentence:\\n\" + i \n",
    "    data = {\"model\":\"llama3\", \"prompt\" : f\"{prompt}\"}\n",
    "    response = httpx.post(url, data=json.dumps(data), headers={\"Content-Type\": \"application/json\"}, timeout=15)\n",
    "    response_lines = [line for line in response.text.strip().split(\"\\n\") if line]\n",
    "    response_dicts = [json.loads(line) for line in response_lines]\n",
    "    answer = \"\".join(response_dict.get(\"response\", \"\") for response_dict in response_dicts)\n",
    "    \n",
    "    list_of_used_datasets.append([i, answer])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55ebe90d-46fe-4273-9fe4-5fbfe48fe965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In recent years there has been an increasing interest in the problem of plant species classification in images In recent years there has been an increasing interest in the problem of plant species classification in images [3,7,12,18].',\n",
       " 'None']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_used_datasets[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5d2b926-7668-4fbd-9d45-3ac6270ecabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in A detailed description of the protocol used to build the GeoLifeCLEF 2018 dataset is provided in [1]. \n",
      "\n",
      "They also implemented training on artificially constructed datasets and reported superior performances on ImageCLEF dataset (Garc√≠a Seco de Herrera et al., 2016). \n",
      "\n",
      "‚Ä¢ ImageCLEF ‚Ä¢ ImageCLEF [71]: it consists of more than 250k images belonging to 95 concepts and is split into training, dev and test data; we only consider the dev set, which includes 1,000 images equally split between training and testing, as the ground-truth is released on this dev set only. \n",
      "\n",
      "PlantCLEF PlantCLEF [32]: The PlantCLEF dataset is a large-scale dataset for plant identification, comprising millions of images covering thousands of plant species, including trees, flowers, fruits, and leaves. \n",
      "\n",
      "Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task Closely related to our initiative is the ImageCLEF benchmarking and in particular the 2009 Photo Retrieval task [27] that proposes a dataset consisting of 498,920 news photographs (images and caption text) classified into sub-topics (e. \n",
      "\n",
      "Moreover, we opted for a ViT model that was pretrained on the plant-relevant dataset PlantCLEF2022 [21]. \n",
      "\n",
      "Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 Using the lattices for searching is an important step away from the oversimplifying approach to speech retrieval on the same archive that was adopted by all teams participating in the CLEF campaign CL-SR tracks in 2006 [24] and 2007 [25], where the problem of searching speech was reduced to a classic documentoriented retrieval by using only the one-best ASR output and artificially creating \"documents\" by sliding a fixedlength window across the resulting text stream. \n",
      "\n",
      "The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 The ImageCLEF evaluation campaign was started as part of the CLEF (Cross Language Evaluation Forum) in 2003 [4,5]. \n",
      "\n",
      "Throughout our experiment, we employed the Large-Scale Complex Question Answering DatasetThroughout our experiment, we employed the Large-Scale Complex Question Answering Dataset12 (LC-QuAD) [28] as well as the 5 th edition of Question Answering over Linked Data Challenge13 (QALD-5) dataset [30]. \n",
      "\n",
      "Interesting insights can also be gained from the outcomes of the Image-CLEF image retrieval evaluations [32,33] in which different systems are compared on the same task. \n",
      "\n",
      "This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( This workshop is a follow-up to the first SCST workshop at ECIR 2015 ( [5,6]) and is closely related to the Interactive Track of the CLEF Social Book Search Lab of 2015 and 2016 [7,8]. \n",
      "\n",
      "‚Ä¢ Controversial arguments: Touch√© ‚Ä¢ Controversial arguments: Touch√© (Bondarenko et al., 2020) and ArguAna (Wachsmuth et al. \n",
      "\n",
      "The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments The cross-language search task in CLEF-IP 2010 is adapted for our patent retrieval experiments [7]. \n",
      "\n",
      "The subtask of compound figure detection was first introduced in ImageCLEF2015 [5] and continued in ImageCLEF2016 [6]. \n",
      "\n",
      "The LifeCLEF Bird task proposes to evaluate one of these challenges The LifeCLEF Bird task proposes to evaluate one of these challenges [12] based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios. \n",
      "\n",
      "The ImageCLEF 2011 [50] and ImageCLEF 2012 [51] datasets were used for the experiments. \n",
      "\n",
      "Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition [66] . \n",
      "\n",
      "The 2018 ImageCLEF-Med challenge [6] provides a good overview about the approaches and their results. \n",
      "\n",
      "The CLEF-IP tracks included a patent classification task [46], [47], which provided a dataset of more than 1 million patents as the training set to classify 3,000 patents into their IPC subclasses. \n",
      "\n",
      "‚Ä¢ PAN12 [42] unlike the short email lengths per author in PAN11, this dataset consisted of dense volumes per author. \n",
      "\n",
      "Let us now introduce the context of the plant identification task of ImageCLEF 2011 Let us now introduce the context of the plant identification task of ImageCLEF 2011 [9]. \n",
      "\n",
      "The ImageCLEF 2012 The ImageCLEF 2012 [26] image dataset was adopted in this work in order to evaluate the proposed methodology by considering the plant identification species from its leaves. \n",
      "\n",
      "Les principales caract√©ristiques de la collection qui a √©t√© utilis√©e dans le cadre de la comp√©tition ImageCLEF 2008 et 2009 (Tsikrika et al.,, 2008 ;Tsikrika et al. \n",
      "\n",
      "[20] Natural Images 50,000 10,000 Tiny ImageNet [21] Natural Images (ImageNet subset) 100,000 10,000 Stanford dogs [22] Natural Images (Dog breeds) 12,000 8,580 Flowers-102 [23] Natural Images (Flower species) 2,040 6,149 CUB-200-2011 [24] Natural Images (Bird species) 5,994 5,794 Stanford Cars [25] Natural Images (Car models) 8,144 8,041 Food-101 [26] Natural Images (Food categories) 75,750 25,250 DTD [27] Texture Images 1,880 1,880 47 NEU Surface Defects [28] Surface Defect Images 1,440 360 6 UC Merced Land Use [29] Remote Sensing Images 1,680 420 21 EuroSAT [30] Remote Sensing Images 18,900 8,100 10 PlantVillage [31] Plant Images 44,343 11,105 39 PlantCLEF [32] Plant Images 10,455 1135 20 Galaxy10 DECals [33] Astronomy Images (Galaxy Morphology) Stanford Dogs [22]: Stanford Dogs dataset is a comprehensive dataset for fine-grained image classification, containing 20,580 images of 120 different dog breeds. \n",
      "\n",
      "In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus In addition to the RTE2 corpus, I experimented with the English section of the CLEF 2006 Answer Validation Exercise (CLEF-AVE) corpus (Pe√±as et al., 2007). \n",
      "\n",
      "BRILIW was presented at CLEF 2006 being ranked first in the bilingual English-Spanish QA task (Magnini et al, 2006;Ferr√°ndez et al, 2006). \n",
      "\n",
      "Four of our five runs are ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28. \n",
      "\n",
      "Content Generation Content Generation [124] generating new medical descriptions or knowledge based on a given input  [127] 2012 ‚àº4K English RE Link ShARe13 [128] 2013 ‚àº29K English NER Link GENIA13 [129] 2013 ‚àº5K English EE Link NCBI [21] 2014 ‚àº7K English NER Link ShARe14 [130] 2014 ‚àº35K English NER Link CADEC [20] 2015 ‚àº7. \n",
      "\n",
      "Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 Esta subse√ß√£o descreve as caracter√≠sticas do conjunto de dados utilizado, o algoritmo baseline, que foi o vencedor da competi√ß√£o RepLab2014 [1] e as m√©tricas de avalia√ß√£o. \n",
      "\n",
      "Similarly, GeoLifeCLEF competition series [22,11,17,40,41,10] provide a list of benchmark datasets for location-based species classification. \n",
      "\n",
      "The task and data used in this study are based on the CLEF Lab eRisk task 2 The task and data used in this study are based on the CLEF Lab eRisk task 2 [16]. \n",
      "\n",
      "The LifeCLEF 2015 bird dataset The LifeCLEF 2015 bird dataset [3] is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 240k audio records covering 9,350 bird species observed all around the world thanks to the active work of more than 2,510 contributors. \n",
      "\n",
      "In the LifeCLEF Bird (Audio) Identification Task 2016/2017 algorithm benchmarking competition, the top algorithms were a variation of fully supervised deep learning CNN architecture [35], [36]. \n",
      "\n",
      "We are currently exploring the use of our document correction method for the CLEF speech retrieval task based on oral testimonies [6]. \n",
      "\n",
      "Retrieval-based systems were also the top performing submissions of the ImageCLEF Caption Prediction subtask, a task that ran for two consecutive years [20,24]. \n",
      "\n",
      "The proposed model is tested on the ImageCLEF2007 data collection The proposed model is tested on the ImageCLEF2007 data collection [12], the purpose of which is to investigate the effectiveness of combining image and text for retrieval tasks. \n",
      "\n",
      "The GeoCLEF search task examined geographic search in text corpus [18]. \n",
      "\n",
      "Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media Over the past three years, the CLEF check-that lab introduced tasks for detecting check-worthy political claims from debates and social media (Nakov et al., 2018;Elsayed et al. \n",
      "\n",
      "In 2017, the Second Workshop on Lifelogging Tools and Applications was organized simultaneously with the lifelog evaluation tasks, NTCIR-13 Lifelog-2 Task [76] and ImageCLEFlifelog 2017 Task [81]. \n",
      "\n",
      "The PAN 2015, 2016, 2017 and 2018 datasets The PAN 2015, 2016, 2017 and 2018 datasets [19,20] consisting of tweets in different languages. \n",
      "\n",
      "It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation It was designed as a follow up to the shared tasks organized during the ShARe/CLEF eHealth 2013 evaluation (Suominen et al., 2013;Pradhan et al. \n",
      "\n",
      "Ever since, workshops and tasks have been pursued to advance research onto some of its key challenges: The Second Workshop on Lifelogging Tools and Applications (LTA 2017) [9], NTCIR Lifelog Tasks [13,14], Image-CLEFlifelog [3][4][5]28], and the Lifelogging Search Challenges (LSC) [15][16][17]. \n",
      "\n",
      "In the clinical domain, the ShARe/CLEF 2013 eHealth Eval-51 uation Lab and the i2b2/VA challenge methodologies have been applied in shared tasks 52 [11][12][13]. \n",
      "\n",
      "The experiments were conducted with the dataset initially provided by The experiments were conducted with the dataset initially provided by [Losada and Crestani 2016] and published as part of the CLEF eRisk 2017 Task [Losada et al. 2017]. \n",
      "\n",
      "The Pl@antLeaves II The Pl@antLeaves II [48] dataset is a subset of the ImageCLEF2012 [49] dataset, which contains different types of leaves from trees of the Mediterranean region of France. \n",
      "\n",
      "In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations In the 2014 ShARe/CLEF eHealth task 2, in an effort to leverage this annotated dataset, several approaches were taken to normalize semantic modifiers such as body site and severity which included optimizing cTAKES modules, developing rules based on resources from UMLS, and employing grammatical relations [48]. \n",
      "\n",
      "In 2015, 1,568 were distributed for the ImageCLEFmed multi-label task [10] In 2016, ImageCLEFmed proposed 5 tasks: compound figure detection; compound figure separation; multi-label classification; subfigure classification and caption prediction. \n",
      "\n",
      "The ShARe/CLEF 2014 [6] and SemEval 2015 [7] organized open challenges on detecting disorder mentions (subtask 1) and identifying various attributes (subtask 2) for a given disorder, including negation, severity, body location etc. \n",
      "\n",
      "We did not evaluate our model on standard medical ad hoc IR collection such as CLEF eHealth 2013 [26] or CLEF eHealth 2014 [7] because they contain about 50 annotated queries each which is not enough to train NLTR models [9,30]. \n",
      "\n",
      "The collection is a subset of a larger collection of 77,000 images made available by the medical image retrieval track in 2010 [9] of ImageCLEF1 evaluation. \n",
      "\n",
      "Four of the five runs were ranked as the first four runs in the GeoCLEF 2007 evaluation task (consult Mandl et al. (2007) for more details) both considering Mean Average Precision (ranging from 28. \n",
      "\n",
      "The data comes from the CLEF-IP 2010 task [ 5], where 134 French patent topics are used to search a collection of 1. \n",
      "\n",
      "The dataset was the same as the one used for BirdCLEF 2017 The dataset was the same as the one used for BirdCLEF 2017 [4], mostly based on the contributions of the Xeno-Canto network. \n",
      "\n",
      ", 2019) eHealth dataset is a curated collection of non-technical summaries (NTS) of animal experiments from Germany, which was used to organize the Multilingual Information Extraction Task (Task 1) in the CLEF eHealth Challenge 2019 (D√∂rendahl et al., 2019). \n",
      "\n",
      "2017aNguyen et al. , 2018) ) which focused on a series of image-retrieval and summarisation focused benchmarking initiatives since 2017, and the Lifelog Search Challenge (LSC) (Gurrin et al. \n",
      "\n",
      "The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in The first and main contribution of this research has been a simple feature set for Wikipedia vandalism detection that won the 1st International Competition on Wikipedia Vandalism Detection, as published in (Mola-Velasco 2010;Potthast, Stein, and Holfeld 2010). \n",
      "\n",
      "Future studies could make use of databases such as the CLEF TAR database [54] or the systematic review dataset repository [55]. \n",
      "\n",
      "Specifically, we used the 299 clinical records annotated with disorder mentions normalised to concept unique identifiers (CUIs) in SNOMED-CT, released in the context of the ShARe/CLEF eHealth Evaluation Lab 2013 (Suominen et al. 2013), and the 1,500 PubMed abstracts annotated with disease mentions normalised to MeSH IDs, created for the BioCreative V Chemical Disease Relation (CDR) Task (Li et al. \n",
      "\n",
      "The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task The CLEF 2018 eHealth Evaluation Lab Consumer Health Search (CHS) task [20] investigated the problem of retrieving Web pages to support information needs of health consumers that are confronted with a health problem or a medical condition. \n",
      "\n",
      "12, which is within the range of visual retrieval results reported for the ImageCLEFmed 2008 medical image retrieval task, and is consistent with the observation that visual retrieval techniques can degrade the overall performance [12]. \n",
      "\n",
      "ImageClef, the CLEF Cross Language Image Retrieval Track54 , is a benchmark for the evaluation of cross-language annotation and retrieval of images (Caputo et al, 2014). \n",
      "\n",
      "ImageCLEF 2010 PhotoAnnotation: The Image-CLEF2010 PhotoAnnotation data set[28] consists of 8000 labeled training images taken from flickr and a test set with recently disclosed labels. \n",
      "\n",
      "A similar problem was later reintroduced as the ImageCLEF subfigure classification sub-task in 2015 and 2016 which focused on modality classification of individual subfigures of compound figures, in effect removing the \"COMP\" or compound figure modality [14,15]. \n",
      "\n",
      "AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks AId is usually tackled by means of machine-learned classifiers or distance-based methods; the annual PAN shared tasks [31,6,7,53,8] offer a very good overview of the most recent trends in the field. \n",
      "\n",
      "The increasing uptake of lifelogging as a personal and practitioner technology have also lead to related activities in ImageCLEF [2], the Lifelog Search Challenge [11] and a related task at MediaEval 2019. \n",
      "\n",
      "The same holds for the problem of fine-grained visual categorization (FGVC), where datasets and challenges like PlantCLEF [14][15][16], iNaturalist [17], CUB [18], and Oxford Flowers [19] have triggered the development and evaluation of novel approaches to fine-grained domain adaptation [20], domain specific transfer learning [21], image retrieval [22][23][24], unsupervised visual representation [25,26], few-shot learning [27], transfer learning [21] and prior-shift [28]. \n",
      "\n",
      "The Conference and Labs of Evaluation Forum for Early Risk XSL ‚Ä¢ FO RenderX Prediction (CLEF eRISK) is a public competition about different areas such as health and safety [26]. \n",
      "\n",
      "The dataset is provided by ImageCLEF Tuberculosis 2019 The dataset is provided by ImageCLEF Tuberculosis 2019 [4]  [17], intended for the task of severity scoring (SVR). \n",
      "\n",
      "Extending the prior work inclusion criterion from text to other data modalities, the ImageCLEF lab included annual shared tasks on biomedical image processing from 2005 to 2013 [29][30][31]. \n",
      "\n",
      "In image retrieval such community building has started around the ImageCLEFmed [12,36] medical image retrieval benchmark. \n",
      "\n",
      "A base de imagens utilizada foi a da ImageCLEF Photographic Retrieval Task [Arni et al. 2009], composta por 20. \n",
      "\n",
      "Motivated by this, we extend our proposed model Motivated by this, we extend our proposed model [9] for the ImageCLEFmedical 2021 [10], [11] by adding an explainability module. \n",
      "\n",
      ", 2010] and ImageClef'11 [Nowak et al., 2011] datasets were collected from Flickr but they differ significantly. \n",
      "\n",
      "With appearance of publicly available audio datasets, such as freefield1010 [2], Warblr [3] or Chernobyl dataset from TREE project 1 and challenges for bird recognition, such as the LifeCLEF Bird Identification Task [4] and the recent Bird Audio Detection (BAD) Challenge [3], the problem has received considerable attention from audio research community. \n",
      "\n",
      "C√©PIDC C√©PIDC [34] (the latter in the context of the participation of SIFR Annotator in the CLEF eHealth 2017 challenge [45]) 13 . \n",
      "\n",
      "A newer version of this dataset is published by the CLEF 2017 NewsREEL [126] task, and a competition is held based on this data to train and evaluate news recommender systems. \n",
      "\n",
      "For our experiments, we utilize ImageCLEF2015 and ImageCLEF2016 subfigure classification datasets [33,34] created from a subset of PubMed Central. \n",
      "\n",
      "73% on the LCF-15 [24], [25] dataset. \n",
      "\n",
      "In the ImageClef 2007 medical image classification competition In the ImageClef 2007 medical image classification competition [56], a database of 12,000 categorized radiograph images is used. \n",
      "\n",
      "The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see The formula used to rank the runs in the ImageCLEF 2012 plant identification task is nearly the same as in 2011 (see [9] for details). \n",
      "\n",
      "We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab We focus on the protest event detection task following the 2019 CLEF ProtestNews Lab (H√ºrriyetoglu et al., 2019). \n",
      "\n",
      "Recently, the TREC Dynamic Domain Track (2015-2017) [45], TREC Tasks track (2015-2017) [22] and the CLEF Dynamic Search Lab (2017-2018) [21] have also brought significant benefits to the research progress in this area. \n",
      "\n",
      "lab at CLEF2019 (Hasanain et al., 2019). \n",
      "\n",
      "2016 presidential election (following the same setting as in the official CLEF 2018 competition on check-worthiness detection [17] but using even more data). \n",
      "\n",
      "In ImageCLEF 2016 In ImageCLEF 2016 [6], they expand their training set to 20,997 figures and reduce the size of the test set to 3456. \n",
      "\n",
      "Therefore, a future step in the evaluation of our search approach would be to benchmark our methods against these existing IR techniques specifically developed for the prior art search, for example, using the CLEF-IP datasets [35,36]. \n",
      "\n",
      "Lab Task 2 (verified claim retrieval ) challenges of the CLEF2020 [25], CLEF2021 [6] and CLEF2022 [7] initiatives in English language. \n",
      "\n",
      "As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 As proposed by the Swedish Institute of Computer Science, SICS, in iCLEF 2009 (Gonzalo et al., 2010), we have used several measures related to question reformulations: (i) correctness (the fraction of questions for which there is at least one right reformulation offered by our system), obtaining a 75. \n",
      "\n",
      "We used data from CLEF-2007 and CLEF-2008 [12,18]. \n",
      "\n",
      "The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge The BirdCLEF2019 dataset contains about 350 h of soundscapes, which was built for the 2019BirdCLEF challenge [31]. \n",
      "\n",
      "For text-oriented QA, the TREC [2,32] and CLEF [21] conference series offer a wealth of benchmark questions, but there is no design consideration on harnessing structured data at all. \n",
      "\n",
      "It was created for evaluation on the image track of the Cross Language Evaluation Forum [5]. \n",
      "\n",
      "We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset We train our models on a subset of Wikipedia articles provided in the Wikipedia ImageCLEF dataset [35]. \n",
      "\n",
      "In ImageCLEF2016 [34], they expand the training set to 6776 figures and the test set to 4166 figures. \n",
      "\n",
      "[143] leverage a recently proposed EL toolkit REL [181] for mining historical newspapers for people, places, and other entities in the CLEF HIPE 2020 evaluation campaign [37]. \n",
      "\n",
      "The results from medical retrieval tracks of previous ImageCLEF 2 campaigns also suggest that the combination of CBIR and text-based image searches provides better results than using the two different approaches individually [10][11][12]. \n",
      "\n",
      "While best retrieval accuracy was achieved using a monolingual evaluation task where the queries were English, our results for the cross language task were the best among those making formal submissions to the CLEF 2007 CL-SR task, showing the lowest decrease relative to monolingual performance when queries were translated from their source language to English [12]. \n",
      "\n",
      "The PlantCLEF 2016 dataset The PlantCLEF 2016 dataset [10] consists of observations of plant specimen and provides annotations in terms of organ, species, genus, and family. \n",
      "\n",
      "However, to compare our methods to the ImageCLEF 2010 challenge results, we   4 shows both top performing results of the participants in the ImageCLEF 2010 challenge (see [5] for an overview of the participants, different methods and results) and the performances of our methods. \n",
      "\n",
      "The formula used to rank the runs in the ImageCLEF 2012 plant identification task The formula used to rank the runs in the ImageCLEF 2012 plant identification task [10] is nearly the same as in 2011 (see [10] for details). \n",
      "\n",
      ", the 2012 Informatics for Integrating Biology and the Bedside (i2b2) challenge [14], the 2013/ 2014 CLEF/ShARe challenges [4], and the 2015/2016/ 2017 Clinical TempEval challenges [5][6][7]). \n",
      "\n",
      "The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 The overall goal of the Social Book Search lab at CLEF (Conference and Labs of the Evaluation Forum) in 2014 [1] and 2015 [6] was to investigate how professional metadata (title, authors, . \n",
      "\n",
      "For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past For the visual content of the images multiple features are used, as this was a successfully used technique in ImageCLEFmed in the past (M√ºller, Garc√≠a Seco de Herrera, et al., 2012). \n",
      "\n",
      "28 in GeoCLEF 2007 [41] and from 0. \n",
      "\n",
      "Such problems are posed by the Plant Identification task of the LifeCLEF workshop [14,36,37], known as the PlantCLEF challenge, since 2014. \n",
      "\n",
      "Using the ImageCLEF 2004 CasImage medical database and retrieval task, we have demonstrated the effectiveness of our framework that is very promising when compared to the current automatic runs [17]. \n",
      "\n",
      "The TREC Filtering Track [31] and CLEF INFILE [6] focused on the multiple filtering tasks including adaptive filtering, where systems aim to select relevant documents from a stream of incoming documents based on a user's profile. \n",
      "\n",
      "Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 Conforme observado na Figura 1, os dados est√£o apresentados de forma similar ao formato usado na competic ¬∏√£o PAN-2012 [Inches and Crestani, 2012]. \n",
      "\n",
      "An example living lab is CLEF NEWSREEL 1 An example living lab is CLEF NEWSREEL 1 [13], a campaign-style evaluation lab on news recommendation in realtime which is organized as part of CLEF 2014. \n",
      "\n",
      "Recently, more researches have been dedicated to plant identification from images of multi-organs especially with the release of a large dataset of multi-organs images of PlantCLEF since 2013 [6][7][8][9][10]. \n",
      "\n",
      "This is best exemplified by the test collection methodology employed by large-scale international efforts, such as TREC 10) , CLEF 11) , NTCIR 12) and in the multimedia field, efforts such as ImageCLEF 13) or MediaEval 14) . \n",
      "\n",
      "Erisk@CLEF Erisk@CLEF (Losada et al., 2017) served as a root for this task, which aims to detect depression from the social media data. \n",
      "\n",
      "Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently Two large-scale (ImageCLEF) studies complementing image features with textual information extracted from articles have been proposed recently [58]. \n",
      "\n",
      "Online reputation management: Reputation management in social media Online reputation management: Reputation management in social media [76] has been proposed in RepLab competitive evaluation campaign for Online Reputation Management Systems [12,11]. \n",
      "\n",
      "In all experiments, we used the training dataset from the CLEF CL-SR 2007 task [18]. \n",
      "\n",
      "For instance in the LifeCLEF competition, which attempts to classify plants using images of different parts such as the leaves, the fruit, or the stem, the best performing methods all employed deep learning [17], [18], [19], [20]. \n",
      "\n",
      "A summary of recent work on algorithms for plant identification and as well as detailed evaluations are described in the CLEF 2011 plant images classification task [4]. \n",
      "\n",
      "Applications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methodsApplications of the annotated IAPR-TC12 BenchmarkThe IAPR-TC12 collection is an established image retrieval benchmark that has been used for the evaluation of a wide variety of CBIR and TBIR methods[16,17]. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the amount of random samples that were considered as referecing the reusage of the underlying dataset\n",
    "counter = 0\n",
    "for i in list_of_used_datasets:\n",
    "    if i[1] != \"None\":\n",
    "        print(i[0] ,\"\\n\")\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ae09aeb-bc09-48f9-a28d-21e6969a197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of positive labels is 118 out of 300 for the 300 random samples\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "974de9d8-5165-4592-b133-a061bd1829ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing snippets and labels into text file to manually evaluate the correctness of the labels\n",
    "with open('testing_quality_of_model.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for sublist in list_of_used_datasets:\n",
    "        for item in sublist:\n",
    "            f.write(item + '\\n')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MasterThesisEnv)",
   "language": "python",
   "name": "masterthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
