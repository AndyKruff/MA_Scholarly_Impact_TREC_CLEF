{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85f1f6d9-fb48-4661-aff6-70e38bfe85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed96f01b-6fae-406b-9717-9b94b67829ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trec = pd.read_parquet(\"../../../data/metadata_TREC.parquet\")\n",
    "df_ceur = pd.read_parquet(\"../../../data/metadata_CEUR.parquet\")\n",
    "df_lncs.loc[df_lncs['ID'] == \"lncs_649\", 'Section'] = \"CLEF at SemEval 2007\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31495b54-9ca9-4151-98f9-f43a9e1a89eb",
   "metadata": {},
   "source": [
    "## Assign corresponding Labs to the documents of CEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebfda33d-b3f1-4bbf-90ff-83ea87014148",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data/abbreviations_CLEF_CEUR.json', 'r', encoding=\"utf-8\") as file:\n",
    "    matching_labs = json.load(file)\n",
    "\n",
    "labs = []\n",
    "for i,j in df_ceur.iterrows():\n",
    "    assign_labs = []\n",
    "    for k in matching_labs:\n",
    "        for l in matching_labs[k]:\n",
    "            if l in j[\"Section\"]:\n",
    "                assign_labs.append(k)\n",
    "    labs.append(list(set(assign_labs)))\n",
    "    \n",
    "# Apply concordance dict to unify the corresponding Lab names\n",
    "df_ceur[\"Labs\"] = labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c4730-264d-4531-9694-cf9f1f7ace19",
   "metadata": {},
   "source": [
    "## Assign corresponding Tracks to the document of TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c9e4ea5-0962-4014-ba4a-4c9f41dd0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_assignments = pd.read_parquet(\"../../../data/Assign_tracks_by_ID_for_TREC.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b3b3261-6b40-4525-b694-234dc7d477f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trec = pd.merge(df_trec, track_assignments, how = \"left\", left_on=\"ID\", right_on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146eb87-384f-45cb-b837-231e0fc003c8",
   "metadata": {},
   "source": [
    "## Define the length of the Tracks and Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496977f7-5fc1-4a85-a2cb-d9373a916ebd",
   "metadata": {},
   "source": [
    "### Starting with TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d07477f-56ad-4bc4-9a42-f0abc5fe7cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_trec_explode = df_trec.explode(\"Tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c25f41b1-a996-49b2-aae9-b6724a90fb3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubYear</th>\n",
       "      <th>url</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Title</th>\n",
       "      <th>Section</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>ID</th>\n",
       "      <th>Tracks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>http://trec.nist.gov/pubs/trec9/papers/overvie...</td>\n",
       "      <td>[Ellen M. Voorhees, Donna Harman]</td>\n",
       "      <td>Overview of the Ninth Text REtrieval Conferenc...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>overview_9.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1</td>\n",
       "      <td>Overview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>http://trec.nist.gov/pubs/trec9/papers/trec9-c...</td>\n",
       "      <td>[Fredric C. Gey, Aitao Chen]</td>\n",
       "      <td>TREC-9 Cross-Language Information Retrieval (E...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>trec9-clir-overview.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_2</td>\n",
       "      <td>Cross-Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>http://trec.nist.gov/pubs/trec9/papers/filteri...</td>\n",
       "      <td>[Stephen E. Robertson, David A. Hull]</td>\n",
       "      <td>The TREC-9 Filtering Track Final Report.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>filtering_new.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_3</td>\n",
       "      <td>Filtering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>http://trec.nist.gov/pubs/trec9/papers/t9irep.pdf</td>\n",
       "      <td>[William R. Hersh, Paul Over]</td>\n",
       "      <td>The TREC-9 Interactive Track Report.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>t9irep.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_4</td>\n",
       "      <td>Interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>http://trec.nist.gov/pubs/trec9/papers/liggett...</td>\n",
       "      <td>[Walter Liggett, Chris Buckley]</td>\n",
       "      <td>Query Expansion Seen Through Return Order of R...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>liggett.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_5</td>\n",
       "      <td>Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2019</td>\n",
       "      <td>https://trec.nist.gov/pubs/trec28/papers/OVERV...</td>\n",
       "      <td>[Laura Dietz, John Foley]</td>\n",
       "      <td>TREC CAR Y3: Complex Answer Retrieval Overview</td>\n",
       "      <td>Overview</td>\n",
       "      <td>OVERVIEW.CAR.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1968</td>\n",
       "      <td>Complex Answer Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2007</td>\n",
       "      <td>https://trec.nist.gov/pubs/trec16/papers/umelb...</td>\n",
       "      <td>[William Webber, Vo Ngoc Anh, Alistair Moffat]</td>\n",
       "      <td>The University of Melbourne in the Million Que...</td>\n",
       "      <td>Participant</td>\n",
       "      <td>umelbourne.ngoc-ahn.MQ.final.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1969</td>\n",
       "      <td>Million Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>2020</td>\n",
       "      <td>https://trec.nist.gov/pubs/trec29/papers/OVERV...</td>\n",
       "      <td>[Asia J. Biega, Fernando Diaz, Michael D. Ekst...</td>\n",
       "      <td>Overview of the TREC 2020 Fair Ranking Track∗</td>\n",
       "      <td>Overview</td>\n",
       "      <td>OVERVIEW.FR.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1970</td>\n",
       "      <td>Fair Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>2017</td>\n",
       "      <td>https://trec.nist.gov/pubs/trec26/papers/NOVAS...</td>\n",
       "      <td>[Gonçalo Araújo, André Mourão, João Magalhães]</td>\n",
       "      <td>NOVASearch at Precision Medicine 2017</td>\n",
       "      <td>Participant</td>\n",
       "      <td>NOVASearch-PM.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1971</td>\n",
       "      <td>Precision Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>2012</td>\n",
       "      <td>https://trec.nist.gov/pubs/trec21/papers/IBM.m...</td>\n",
       "      <td>[Myle Ott, Vittorio Castelli, Hema Raghavan, R...</td>\n",
       "      <td>IBM at TREC 2012: Microblog Track</td>\n",
       "      <td>Participant</td>\n",
       "      <td>IBM.microblog.final.pdf</td>\n",
       "      <td>D:/Studium/Masterarbeit/PythonProject/data/PDF...</td>\n",
       "      <td>trec_1972</td>\n",
       "      <td>Microblog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2178 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PubYear                                                url  \\\n",
       "0        2000  http://trec.nist.gov/pubs/trec9/papers/overvie...   \n",
       "1        2000  http://trec.nist.gov/pubs/trec9/papers/trec9-c...   \n",
       "2        2000  http://trec.nist.gov/pubs/trec9/papers/filteri...   \n",
       "3        2000  http://trec.nist.gov/pubs/trec9/papers/t9irep.pdf   \n",
       "4        2000  http://trec.nist.gov/pubs/trec9/papers/liggett...   \n",
       "...       ...                                                ...   \n",
       "1967     2019  https://trec.nist.gov/pubs/trec28/papers/OVERV...   \n",
       "1968     2007  https://trec.nist.gov/pubs/trec16/papers/umelb...   \n",
       "1969     2020  https://trec.nist.gov/pubs/trec29/papers/OVERV...   \n",
       "1970     2017  https://trec.nist.gov/pubs/trec26/papers/NOVAS...   \n",
       "1971     2012  https://trec.nist.gov/pubs/trec21/papers/IBM.m...   \n",
       "\n",
       "                                                Authors  \\\n",
       "0                     [Ellen M. Voorhees, Donna Harman]   \n",
       "1                          [Fredric C. Gey, Aitao Chen]   \n",
       "2                 [Stephen E. Robertson, David A. Hull]   \n",
       "3                         [William R. Hersh, Paul Over]   \n",
       "4                       [Walter Liggett, Chris Buckley]   \n",
       "...                                                 ...   \n",
       "1967                          [Laura Dietz, John Foley]   \n",
       "1968     [William Webber, Vo Ngoc Anh, Alistair Moffat]   \n",
       "1969  [Asia J. Biega, Fernando Diaz, Michael D. Ekst...   \n",
       "1970     [Gonçalo Araújo, André Mourão, João Magalhães]   \n",
       "1971  [Myle Ott, Vittorio Castelli, Hema Raghavan, R...   \n",
       "\n",
       "                                                  Title        Section  \\\n",
       "0     Overview of the Ninth Text REtrieval Conferenc...  Uncategorized   \n",
       "1     TREC-9 Cross-Language Information Retrieval (E...  Uncategorized   \n",
       "2              The TREC-9 Filtering Track Final Report.  Uncategorized   \n",
       "3                  The TREC-9 Interactive Track Report.  Uncategorized   \n",
       "4     Query Expansion Seen Through Return Order of R...  Uncategorized   \n",
       "...                                                 ...            ...   \n",
       "1967     TREC CAR Y3: Complex Answer Retrieval Overview       Overview   \n",
       "1968  The University of Melbourne in the Million Que...    Participant   \n",
       "1969      Overview of the TREC 2020 Fair Ranking Track∗       Overview   \n",
       "1970              NOVASearch at Precision Medicine 2017    Participant   \n",
       "1971                  IBM at TREC 2012: Microblog Track    Participant   \n",
       "\n",
       "                              filename  \\\n",
       "0                       overview_9.pdf   \n",
       "1              trec9-clir-overview.pdf   \n",
       "2                    filtering_new.pdf   \n",
       "3                           t9irep.pdf   \n",
       "4                          liggett.pdf   \n",
       "...                                ...   \n",
       "1967                  OVERVIEW.CAR.pdf   \n",
       "1968  umelbourne.ngoc-ahn.MQ.final.pdf   \n",
       "1969                   OVERVIEW.FR.pdf   \n",
       "1970                 NOVASearch-PM.pdf   \n",
       "1971           IBM.microblog.final.pdf   \n",
       "\n",
       "                                               filepath         ID  \\\n",
       "0     D:/Studium/Masterarbeit/PythonProject/data/PDF...     trec_1   \n",
       "1     D:/Studium/Masterarbeit/PythonProject/data/PDF...     trec_2   \n",
       "2     D:/Studium/Masterarbeit/PythonProject/data/PDF...     trec_3   \n",
       "3     D:/Studium/Masterarbeit/PythonProject/data/PDF...     trec_4   \n",
       "4     D:/Studium/Masterarbeit/PythonProject/data/PDF...     trec_5   \n",
       "...                                                 ...        ...   \n",
       "1967  D:/Studium/Masterarbeit/PythonProject/data/PDF...  trec_1968   \n",
       "1968  D:/Studium/Masterarbeit/PythonProject/data/PDF...  trec_1969   \n",
       "1969  D:/Studium/Masterarbeit/PythonProject/data/PDF...  trec_1970   \n",
       "1970  D:/Studium/Masterarbeit/PythonProject/data/PDF...  trec_1971   \n",
       "1971  D:/Studium/Masterarbeit/PythonProject/data/PDF...  trec_1972   \n",
       "\n",
       "                        Tracks  \n",
       "0                     Overview  \n",
       "1               Cross-Language  \n",
       "2                    Filtering  \n",
       "3                  Interactive  \n",
       "4                   Evaluation  \n",
       "...                        ...  \n",
       "1967  Complex Answer Retrieval  \n",
       "1968             Million Query  \n",
       "1969              Fair Ranking  \n",
       "1970        Precision Medicine  \n",
       "1971                 Microblog  \n",
       "\n",
       "[2178 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trec_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb86c26f-0e4c-4e8f-b5b0-8722cd737a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the amount of unique Publication Years for every Track in order to get the runtime of this Track\n",
    "track_years = df_trec_explode.groupby('Tracks')['PubYear'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8a5ead-d569-40aa-8a77-74dc0629e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"Overview Track\" and \"Others\" because classification does not refer to a specifc Track\n",
    "track_years = track_years[track_years['Tracks'] != 'Overview']\n",
    "track_years = track_years[track_years['Tracks'] != 'Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "531aecf5-09d5-4ec6-8bca-3cceefaa3f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tracks</th>\n",
       "      <th>PubYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blog</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CENTRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical Decision Support</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical Trials</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Common Core</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Complex Answer Retrieval</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Contextual Suggestion</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conversational Assistance</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CrisisFACTs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cross-Language</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Crowdsourcing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Deep Learning</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dynamic Domain</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Enterprise</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Entity</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Evaluation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fair Ranking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Federated Web Search</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Filtering</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Genomics</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HARD</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Health Misinformation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Incident Streams</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Interactive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Knowledge Base Acceleration</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Legal</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LiveQA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Medical</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Microblog</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Million Query</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NeuCLIR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>News</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Novelty</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>OpenSearch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Podcast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Precision Medicine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Query</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Question Answering</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Real-time Summarization</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Relevance Feedback</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Robust</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Session</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Spam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Spoken Document Retrieval</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Tasks</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Temporal Summarization</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Terabyte</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Total Recall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Video</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Web</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Tracks  PubYear\n",
       "0                          Blog        5\n",
       "1                        CENTRE        1\n",
       "2                      Chemical        3\n",
       "3     Clinical Decision Support        3\n",
       "4               Clinical Trials        2\n",
       "5                   Common Core        2\n",
       "6      Complex Answer Retrieval        3\n",
       "7         Contextual Suggestion        5\n",
       "8     Conversational Assistance        4\n",
       "9                   CrisisFACTs        1\n",
       "10               Cross-Language        3\n",
       "11                Crowdsourcing        3\n",
       "12                     Decision        1\n",
       "13                Deep Learning        4\n",
       "14               Dynamic Domain        3\n",
       "15                   Enterprise        4\n",
       "16                       Entity        3\n",
       "17                   Evaluation        1\n",
       "18                 Fair Ranking        4\n",
       "19         Federated Web Search        2\n",
       "20                    Filtering        3\n",
       "21                     Genomics        5\n",
       "22                         HARD        3\n",
       "23        Health Misinformation        3\n",
       "24             Incident Streams        4\n",
       "25                  Interactive        3\n",
       "26  Knowledge Base Acceleration        3\n",
       "27                        Legal        6\n",
       "28                       LiveQA        3\n",
       "29                      Medical        2\n",
       "30                    Microblog        5\n",
       "31                Million Query        3\n",
       "32                      NeuCLIR        1\n",
       "33                         News        4\n",
       "34                      Novelty        3\n",
       "35                   OpenSearch        2\n",
       "38                      Podcast        2\n",
       "39           Precision Medicine        4\n",
       "40                        Query        1\n",
       "41           Question Answering        8\n",
       "42      Real-time Summarization        3\n",
       "43           Relevance Feedback        3\n",
       "44                       Robust        3\n",
       "45                      Session        5\n",
       "46                         Spam        3\n",
       "47    Spoken Document Retrieval        1\n",
       "48                        Tasks        3\n",
       "49       Temporal Summarization        3\n",
       "50                     Terabyte        3\n",
       "51                 Total Recall        2\n",
       "52                        Video        2\n",
       "53                          Web       11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1796b2b1-93f0-4cbe-9636-28c7e0694688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the publication year is: 3.2115384615384617\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average runtime in the time period between 2000 and 2022 for TREC Tracks\n",
    "average_pub_year_trec = track_years['PubYear'].mean()\n",
    "\n",
    "print(f\"The average of the publication year is: {average_pub_year_trec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "875c26f3-cae5-4187-bd29-07d41da4764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pub_year_trec = track_years['PubYear'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b04944e-d27f-469a-af2a-49e5d9a05176",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = average_pub_year_trec + 2 * std_pub_year_trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb3c2f1c-5a41-45e8-b8c7-8306734109c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = track_years[track_years['PubYear'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "debd0cba-29b9-4418-bac8-33ce2bb92793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tracks</th>\n",
       "      <th>PubYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Question Answering</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Web</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tracks  PubYear\n",
       "41  Question Answering        8\n",
       "53                 Web       11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dba97f-fc3e-412c-bb0a-7b942c5f9b28",
   "metadata": {},
   "source": [
    "### Analysis of CEUR Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866c39e7-1a27-40f3-9149-acc40fafa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceur_explode = df_ceur.explode(\"Labs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a67899-1c82-4732-b7a8-24cfb1449705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the amount of unique Publication Years for every Lab in order to get the runtime of this Lab\n",
    "lab_years = df_ceur_explode.groupby('Labs')['PubYear'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05e47260-35bb-41c3-8a14-c92c0f752d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labs</th>\n",
       "      <th>PubYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARQMath</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adhoc IR Track</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BioASQ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CENTRE@CLEF</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHiC</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CL-SDR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CL-SR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CLEF-ER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CLEF-IP</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CLIR</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ChEMU</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CheckThat!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CriES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Domain Specific</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DynSE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GeoCLEF</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Grid@CLEF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HIPE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>INEX</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>INFILE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ImageCLEF</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Issues</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JokeR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LL4IR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LL4IR/NewsREEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LeQua</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LiLAS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LifeCLEF</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogCLEF</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MC2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Miscellanea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Morpho</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MusiCLEF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NewsREEL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PAN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>PIR-CLEF</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ProtestNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Question Answering</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RepLab</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SBS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SemEval</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SimpleText</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Touché</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>VideoCLEF</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>WEPS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>WebCLEF</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>eHealth</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>eRisk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>iCLEF</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>iDPP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>mSpRL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Labs  PubYear\n",
       "0              ARQMath        3\n",
       "1       Adhoc IR Track        4\n",
       "2               BioASQ        3\n",
       "3          CENTRE@CLEF        2\n",
       "4                 CHiC        3\n",
       "5               CL-SDR        2\n",
       "6                CL-SR        3\n",
       "7              CLEF-ER        1\n",
       "8              CLEF-IP        5\n",
       "9                 CLIR        6\n",
       "10               ChEMU        3\n",
       "11          CheckThat!        5\n",
       "12               CriES        1\n",
       "13                  DS        3\n",
       "14     Domain Specific        7\n",
       "15               DynSE        1\n",
       "16             GeoCLEF        4\n",
       "17           Grid@CLEF        1\n",
       "18                HIPE        2\n",
       "19                INEX        3\n",
       "20              INFILE        2\n",
       "21           ImageCLEF       20\n",
       "22              Issues        4\n",
       "23               JokeR        1\n",
       "24               LL4IR        1\n",
       "25      LL4IR/NewsREEL        1\n",
       "26               LeQua        1\n",
       "27               LiLAS        2\n",
       "28            LifeCLEF        9\n",
       "29             LogCLEF        3\n",
       "30                 MC2        3\n",
       "31         Miscellanea        1\n",
       "32              Morpho        3\n",
       "33            MusiCLEF        1\n",
       "34            NewsREEL        4\n",
       "35                 PAN       13\n",
       "36            PIR-CLEF        2\n",
       "37         ProtestNews        1\n",
       "38  Question Answering       13\n",
       "39              RepLab        3\n",
       "40                 SBS        2\n",
       "41             SemEval        1\n",
       "42          SimpleText        2\n",
       "43              Touché        3\n",
       "44           VideoCLEF        2\n",
       "45                WEPS        1\n",
       "46             WebCLEF        4\n",
       "47             eHealth       10\n",
       "48               eRisk        6\n",
       "49               iCLEF        8\n",
       "50                iDPP        1\n",
       "51               mSpRL        1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c57cc83-0b00-4c9e-928e-457133b879ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the publication year is: 3.673076923076923\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average runtime in the time period between 2000 and 2022 for CEUR labs\n",
    "average_pub_year_ceur = lab_years['PubYear'].mean()\n",
    "\n",
    "print(f\"The average of the publication year is: {average_pub_year_ceur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "422b4f7c-265a-4a6b-80d8-5b8f9efc61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pub_year_ceur = lab_years['PubYear'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "300eaa7b-773f-4469-9e90-066836a1923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = average_pub_year_ceur + 2 * std_pub_year_ceur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b07fae8c-f8c2-4eb5-ab66-fe6a6e3c4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = lab_years[lab_years['PubYear'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "161beef3-2f6a-4cef-afe5-d5e5beea4958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labs</th>\n",
       "      <th>PubYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ImageCLEF</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PAN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Question Answering</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Labs  PubYear\n",
       "21           ImageCLEF       20\n",
       "35                 PAN       13\n",
       "38  Question Answering       13"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716085e8-e3e8-45f8-b6db-6dfb65d612f6",
   "metadata": {},
   "source": [
    "## Testing for significant differences between the run time of tracks/labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be639b08-457c-4a75-9df1-1eb8365a6079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test for OpenAlex LNCS: Statistic=0.804054055720903, P-Value=7.421018270049134e-07\n",
      "Shapiro-Wilk Test for OpenAlex CEUR: Statistic=0.6986854546656287, P-Value=4.9283481080108245e-09\n",
      "Levene Test: Statistic=5.765306808253242, P-Value=0.018156720354996127\n",
      "Data is not normally distributed. Consider using the Welch test or the Mann-Whitney U test.\n",
      "Variances are unequal. Consider using the Welch test instead of the standard T-Test.\n",
      "T-Statistic: 1487.5\n",
      "P-Value: 0.36773136910860604\n",
      "Median Citations for OpenAlex LNCS: 3.0\n",
      "Median Citations for OpenAlex CEUR: 3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Extract citations for the two proceedings\n",
    "runtime_trec = track_years['PubYear']\n",
    "runtime_ceur = lab_years['PubYear']\n",
    "\n",
    "# Shapiro-Wilk Test for normality\n",
    "shapiro_trec_stat, shapiro_trec_p = stats.shapiro(runtime_trec)\n",
    "shapiro_ceur_stat, shapiro_ceur_p = stats.shapiro(runtime_ceur)\n",
    "\n",
    "print(f'Shapiro-Wilk Test for OpenAlex LNCS: Statistic={shapiro_trec_stat}, P-Value={shapiro_trec_p}')\n",
    "print(f'Shapiro-Wilk Test for OpenAlex CEUR: Statistic={shapiro_ceur_stat}, P-Value={shapiro_ceur_p}')\n",
    "\n",
    "# Levene Test for equal variances\n",
    "levene_stat, levene_p = stats.levene(runtime_trec, runtime_ceur)\n",
    "\n",
    "print(f'Levene Test: Statistic={levene_stat}, P-Value={levene_p}')\n",
    "\n",
    "# Perform the T-Test, based on the test results\n",
    "# Decide based on the tests whether to perform the T-Test or a different test\n",
    "if shapiro_trec_p > 0.05 and shapiro_ceur_p > 0.05:\n",
    "    print(\"Data is normally distributed.\")\n",
    "else:\n",
    "    print(\"Data is not normally distributed. Consider using the Welch test or the Mann-Whitney U test.\")\n",
    "\n",
    "if levene_p > 0.05:\n",
    "    print(\"Variances are equal.\")\n",
    "else:\n",
    "    print(\"Variances are unequal. Consider using the Welch test instead of the standard T-Test.\")\n",
    "\n",
    "# Perform T-Test\n",
    "mann_whitney_stat, mann_whitney_p = stats.mannwhitneyu(runtime_trec, runtime_ceur, alternative='two-sided')\n",
    "\n",
    "print(f'T-Statistic: {mann_whitney_stat}')\n",
    "print(f'P-Value: {mann_whitney_p}')\n",
    "\n",
    "median_A = np.median(runtime_trec)\n",
    "median_B = np.median(runtime_ceur)\n",
    "\n",
    "print(f'Median Citations for OpenAlex LNCS: {median_A}')\n",
    "print(f'Median Citations for OpenAlex CEUR: {median_B}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MasterThesisEnv)",
   "language": "python",
   "name": "masterthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
